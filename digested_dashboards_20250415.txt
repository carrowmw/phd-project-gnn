Directory structure:
└── dashboards/
    ├── __init__.py
    ├── digester.sh
    ├── data/
    ├── dataloader_explorer/
    │   ├── README.md
    │   ├── __init__.py
    │   ├── __main__.py
    │   ├── __pycache__/
    │   ├── components/
    │   │   ├── __init__.py
    │   │   ├── adjacency_plot.py
    │   │   ├── batch_explorer.py
    │   │   ├── correlation_plot.py
    │   │   ├── data_stats.py
    │   │   ├── node_explorer.py
    │   │   ├── window_explorer.py
    │   │   └── __pycache__/
    │   ├── static/
    │   │   ├── css/
    │   │   │   └── styles.css
    │   │   └── js/
    │   │       └── custom.js
    │   ├── templates/
    │   │   ├── __init__.py
    │   │   └── layout.html
    │   └── utils/
    │       ├── __init__.py
    │       ├── data_utils.py
    │       └── __pycache__/
    └── eda/
        ├── __init__.py
        ├── __main__.py
        ├── __pycache__/
        ├── components/
        │   ├── __init__.py
        │   ├── calendar_heatmap.py
        │   ├── completeness_trend.py
        │   ├── counts_bar.py
        │   ├── daily_patterns.py
        │   ├── heatmap.py
        │   ├── monthly_data_coverage.py
        │   ├── sensor_clustering.py
        │   ├── sensor_map.py
        │   ├── traffic_comparison.py
        │   ├── traffic_profile.py
        │   ├── window_segments.py
        │   └── __pycache__/
        ├── templates/
        │   ├── __init__.py
        │   └── dashboard_template.html
        └── utils/
            ├── __init__.py
            ├── data_utils.py
            ├── template_utils.py
            └── __pycache__/

================================================
File: __init__.py
================================================



================================================
File: digester.sh
================================================
#!/bin/bash

# digester.sh - Script to ingest codebase while excluding large files and data files
# Dependencies: gitingest, nbstripout

set -e  # Exit on error

# Configuration
MAX_FILE_SIZE_KB=500  # Set maximum file size to 500 KB
MAX_FILE_SIZE_BYTES=$((MAX_FILE_SIZE_KB * 1024))
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
OUTPUT_FILE="$PROJECT_ROOT/digested_dashboards_$(date +%Y%m%d).txt"

# Check if gitingest is installed
if ! command -v gitingest &> /dev/null; then
    echo "Error: gitingest is not installed. Please install it first."
    echo "Install with: pip install gitingest"
    exit 1
fi

# Check if nbstripout is installed
if ! command -v nbstripout &> /dev/null; then
    echo "Warning: nbstripout is not installed. Notebooks will not be processed."
    echo "Consider installing with: pip install nbstripout"
    PROCESS_NOTEBOOKS=false
else
    PROCESS_NOTEBOOKS=true
fi

# Process notebooks if nbstripout is available
if [ "$PROCESS_NOTEBOOKS" = true ]; then
    echo "Processing notebooks with nbstripout..."
    find "$SCRIPT_DIR" -name "*.ipynb" -exec nbstripout {} \;
fi

echo "Starting codebase ingestion from gnn_package directory..."
echo "- Max file size: ${MAX_FILE_SIZE_KB}KB"
echo "- Output will be saved to: ${OUTPUT_FILE}"

# Run gitingest on the gnn_package directory
gitingest "$SCRIPT_DIR" \
    -s "${MAX_FILE_SIZE_BYTES}" \
    --exclude-pattern="*.pkl" \
    --exclude-pattern="*.npy" \
    --exclude-pattern="*.csv" \
    --exclude-pattern="*.parquet" \
    --exclude-pattern="*.json" \
    --exclude-pattern="*.gz" \
    --exclude-pattern="*.zip" \
    --exclude-pattern="*.tar" \
    --exclude-pattern="*.h5" \
    --exclude-pattern="*.hdf5" \
    --exclude-pattern="*.pyc" \
    --exclude-pattern="__pycache__/" \
    --exclude-pattern=".ipynb_checkpoints/" \
    --exclude-pattern="cache/" \
    --exclude-pattern="*/cache/*" \
    --exclude-pattern="*.so" \
    --exclude-pattern="*.o" \
    --exclude-pattern="*.a" \
    --exclude-pattern="*.dll" \
    --exclude-pattern="*.geojson" \
    --exclude-pattern="*.shp" \
    --exclude-pattern="*.shx" \
    --exclude-pattern="*.dbf" \
    --exclude-pattern="*.prj" \
    --exclude-pattern="*.cpg" \
    --exclude-pattern="*.pth" \
    --exclude-pattern="*.pt" \
    --exclude-pattern="*.ckpt" \
    --exclude-pattern="*.bin" \
    --exclude-pattern="*.png" \
    --exclude-pattern="*.jpg" \
    --exclude-pattern="*.jpeg" \
    --exclude-pattern="*.gif" \
    --exclude-pattern="*.svg" \
    --exclude-pattern="*.ico" \
    --exclude-pattern="*.pdf" \
    --exclude-pattern="*index.html" \
    --output="$OUTPUT_FILE"

echo "Nom nom, digestion complete! Output saved to $OUTPUT_FILE"



================================================
File: dataloader_explorer/README.md
================================================
# DataLoader Explorer

An interactive dashboard for exploring DataLoader structures for GNN-based time series forecasting.

## Features

- **Batch Explorer**: Visualize data availability across nodes and time steps in a batch
- **Node Explorer**: Examine individual node data with input/target visualization
- **Window Explorer**: View multiple windows for a specific node in a heatmap
- **Correlation Analysis**: Analyze correlations between different nodes
- **Adjacency Matrix**: Visualize and explore the graph structure
- **Statistics**: View comprehensive statistics about your data

## Installation

This module requires the following dependencies:

```bash
pip install dash dash-bootstrap-components plotly pandas numpy torch networkx
```

## Usage

### Running the Dashboard

To run the DataLoader Explorer dashboard, use the following command:

```bash
python -m dashboards.dataloader_explorer --data path/to/data_loaders.pkl
```

Optional arguments:
- `--port PORT`: Specify the port to run the dashboard on (default: 8050)
- `--debug`: Run the dashboard in debug mode

### Programmatic Usage

You can also use the components programmatically in your own code:

```python
from dashboards.dataloader_explorer import load_data_loaders, create_node_explorer

# Load your data
data_loaders = load_data_loaders("path/to/data_loaders.pkl")

# Create a visualization
node_fig = create_node_explorer(data_loaders, loader_key='train_loader', batch_idx=0, node_idx=0)

# Display the figure
node_fig.show()
```

## Dashboard Components

1. **Batch Explorer**
   - Overview of data availability in a batch
   - Highlights missing data across nodes and time steps

2. **Node Explorer**
   - Detailed view of individual node data
   - Shows input (historical) and target (future) values
   - Clearly marks missing data points

3. **Window Explorer**
   - Visualizes multiple windows for a node
   - Shows patterns across windows using a heatmap

4. **Correlation Analysis**
   - Calculates and visualizes correlations between nodes
   - Helps identify related nodes in the network

5. **Adjacency Matrix**
   - Visualizes the graph structure
   - Toggle between heatmap and network views

6. **Statistics**
   - Comprehensive statistics about your data
   - Distribution, missing values, adjacency matrix properties

## Integration with Existing Dashboards

This module follows the same structure as other dashboard modules in the project, making it easy to integrate with existing visualization workflows.

## Development

To contribute to DataLoader Explorer, follow the standard development practices:

1. Fork the repository
2. Install development dependencies
3. Implement your changes
4. Test thoroughly
5. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.


================================================
File: dataloader_explorer/__init__.py
================================================
# dashboards/dataloader_explorer/__init__.py

from .utils.data_utils import load_data_loaders, get_batch_from_loader
from .components.batch_explorer import create_batch_explorer
from .components.node_explorer import create_node_explorer
from .components.window_explorer import create_window_explorer
from .components.correlation_plot import create_correlation_plot
from .components.adjacency_plot import create_adjacency_plot
from .components.data_stats import create_stats_panel

__all__ = [
    "load_data_loaders",
    "get_batch_from_loader",
    "create_batch_explorer",
    "create_node_explorer",
    "create_window_explorer",
    "create_correlation_plot",
    "create_adjacency_plot",
    "create_stats_panel",
]



================================================
File: dataloader_explorer/__main__.py
================================================
# dashboards/dataloader_explorer/__main__.py
# Main entry point for the dataloader explorer dashboard

import os
import argparse
import webbrowser
from pathlib import Path
import dash
from dash import dcc, html, callback, Input, Output, State
import dash_bootstrap_components as dbc
from .utils.data_utils import load_data_loaders
from .components.batch_explorer import create_batch_explorer
from .components.node_explorer import create_node_explorer
from .components.window_explorer import create_window_explorer
from .components.correlation_plot import create_correlation_plot
from .components.adjacency_plot import create_adjacency_plot
from .components.data_stats import create_stats_panel


# Define the layout for the Dash app
def create_layout(data_loaders, available_loaders):
    """Create the dashboard layout"""
    return dbc.Container(
        [
            dbc.Row(
                [
                    dbc.Col(
                        html.H1(
                            "DataLoader Explorer Dashboard",
                            className="text-center my-4",
                        ),
                        width=12,
                    )
                ]
            ),
            # Controls and settings section
            dbc.Row(
                [
                    dbc.Col(
                        [
                            dbc.Card(
                                [
                                    dbc.CardHeader("Settings"),
                                    dbc.CardBody(
                                        [
                                            html.Label("DataLoader:"),
                                            dcc.Dropdown(
                                                id="loader-selector",
                                                options=[
                                                    {"label": loader, "value": loader}
                                                    for loader in available_loaders
                                                ],
                                                value=(
                                                    available_loaders[0]
                                                    if available_loaders
                                                    else None
                                                ),
                                                className="mb-3",
                                            ),
                                            html.Label("Batch Index:"),
                                            dcc.Slider(
                                                id="batch-slider",
                                                min=0,
                                                max=10,  # Will be updated dynamically
                                                step=1,
                                                value=0,
                                                marks=None,
                                                tooltip={
                                                    "placement": "bottom",
                                                    "always_visible": True,
                                                },
                                                className="mb-3",
                                            ),
                                            html.Label("Node Index:"),
                                            dcc.Slider(
                                                id="node-slider",
                                                min=0,
                                                max=10,  # Will be updated dynamically
                                                step=1,
                                                value=0,
                                                marks=None,
                                                tooltip={
                                                    "placement": "bottom",
                                                    "always_visible": True,
                                                },
                                                className="mb-3",
                                            ),
                                            html.Label("Number of Windows:"),
                                            dcc.Slider(
                                                id="window-slider",
                                                min=5,
                                                max=50,
                                                step=5,
                                                value=20,
                                                marks={
                                                    i: str(i) for i in range(5, 51, 5)
                                                },
                                                tooltip={
                                                    "placement": "bottom",
                                                    "always_visible": True,
                                                },
                                                className="mb-3",
                                            ),
                                        ]
                                    ),
                                ],
                                className="mb-4",
                            )
                        ],
                        width=12,
                        md=4,
                    ),
                    dbc.Col(
                        [
                            dbc.Card(
                                [
                                    dbc.CardHeader("Dataset Statistics"),
                                    dbc.CardBody([html.Div(id="dataset-stats")]),
                                ],
                                className="mb-4",
                            )
                        ],
                        width=12,
                        md=8,
                    ),
                ]
            ),
            # Tabs for different views
            dbc.Tabs(
                [
                    dbc.Tab(
                        [
                            dbc.Row(
                                [
                                    dbc.Col(
                                        dcc.Loading(
                                            id="loading-batch-explorer",
                                            type="circle",
                                            children=dcc.Graph(
                                                id="batch-explorer-fig",
                                                style={"height": "600px"},
                                            ),
                                        ),
                                        width=12,
                                    )
                                ],
                                className="mb-4",
                            ),
                        ],
                        label="Batch Explorer",
                        tab_id="tab-batch",
                    ),
                    dbc.Tab(
                        [
                            dbc.Row(
                                [
                                    dbc.Col(
                                        dcc.Loading(
                                            id="loading-node-explorer",
                                            type="circle",
                                            children=dcc.Graph(
                                                id="node-explorer-fig",
                                                style={"height": "600px"},
                                            ),
                                        ),
                                        width=12,
                                    )
                                ],
                                className="mb-4",
                            ),
                        ],
                        label="Node Explorer",
                        tab_id="tab-node",
                    ),
                    dbc.Tab(
                        [
                            dbc.Row(
                                [
                                    dbc.Col(
                                        dcc.Loading(
                                            id="loading-window-explorer",
                                            type="circle",
                                            children=dcc.Graph(
                                                id="window-explorer-fig",
                                                style={"height": "700px"},
                                            ),
                                        ),
                                        width=12,
                                    )
                                ],
                                className="mb-4",
                            ),
                        ],
                        label="Window Explorer",
                        tab_id="tab-window",
                    ),
                    dbc.Tab(
                        [
                            dbc.Row(
                                [
                                    dbc.Col(
                                        dcc.Loading(
                                            id="loading-correlation-plot",
                                            type="circle",
                                            children=dcc.Graph(
                                                id="correlation-plot-fig",
                                                style={"height": "700px"},
                                            ),
                                        ),
                                        width=12,
                                    )
                                ],
                                className="mb-4",
                            ),
                        ],
                        label="Correlation Analysis",
                        tab_id="tab-correlation",
                    ),
                    dbc.Tab(
                        [
                            dbc.Row(
                                [
                                    dbc.Col(
                                        dcc.Loading(
                                            id="loading-adjacency-plot",
                                            type="circle",
                                            children=dcc.Graph(
                                                id="adjacency-plot-fig",
                                                style={"height": "700px"},
                                            ),
                                        ),
                                        width=12,
                                    )
                                ],
                                className="mb-4",
                            ),
                        ],
                        label="Adjacency Matrix",
                        tab_id="tab-adjacency",
                    ),
                    dbc.Tab(
                        [
                            dbc.Row(
                                [
                                    dbc.Col(
                                        dcc.Loading(
                                            id="loading-stats-panel",
                                            type="circle",
                                            children=dcc.Graph(
                                                id="stats-panel-fig",
                                                style={"height": "700px"},
                                            ),
                                        ),
                                        width=12,
                                    )
                                ],
                                className="mb-4",
                            ),
                        ],
                        label="Statistics",
                        tab_id="tab-stats",
                    ),
                ],
                id="tabs",
                active_tab="tab-batch",
            ),
            # Footer
            dbc.Row(
                [
                    dbc.Col(
                        html.Footer(
                            [
                                html.P(
                                    [
                                        "DataLoader Explorer Dashboard | Created with Dash and Plotly | ",
                                        html.A(
                                            "GNN Package", href="#", target="_blank"
                                        ),
                                    ],
                                    className="text-center text-muted",
                                )
                            ],
                            className="py-3",
                        ),
                        width=12,
                    )
                ],
                className="mt-5",
            ),
            # Store for sharing data between callbacks
            dcc.Store(id="data-store"),
        ],
        fluid=True,
        className="px-4 py-3",
    )


def create_dash_app(data_loaders_path):
    """Create and configure the Dash app"""
    # Load data loaders
    try:
        data_loaders = load_data_loaders(data_loaders_path)
        # Find available loaders
        available_loaders = [
            key for key in data_loaders.keys() if key.endswith("_loader")
        ]

        if not available_loaders:
            raise ValueError("No valid data loaders found in the provided file.")

    except Exception as e:
        print(f"Error loading data loaders: {e}")
        # Create mock data for UI testing
        data_loaders = {}
        available_loaders = []

    # Create the Dash app
    app = dash.Dash(
        __name__,
        external_stylesheets=[dbc.themes.BOOTSTRAP],
        title="DataLoader Explorer",
    )

    # Define the app layout
    app.layout = create_layout(data_loaders, available_loaders)

    # Define callbacks
    @app.callback(
        [
            Output("batch-slider", "max"),
            Output("node-slider", "max"),
            Output("data-store", "data"),
        ],
        [Input("loader-selector", "value")],
    )
    def update_sliders(loader_key):
        """Update the sliders based on the selected data loader"""
        if not loader_key or loader_key not in data_loaders:
            return 0, 0, {"batch_max": 0, "node_max": 0}

        try:
            # Get a batch to extract dimensions
            batch = next(iter(data_loaders[loader_key]))
            batch_size = batch["x"].shape[0]
            num_nodes = batch["x"].shape[1]

            return (
                batch_size - 1,
                num_nodes - 1,
                {"batch_max": batch_size - 1, "node_max": num_nodes - 1},
            )

        except Exception as e:
            print(f"Error updating sliders: {e}")
            return 0, 0, {"batch_max": 0, "node_max": 0}

    @app.callback(
        Output("batch-explorer-fig", "figure"),
        [Input("loader-selector", "value"), Input("batch-slider", "value")],
    )
    def update_batch_explorer(loader_key, batch_idx):
        """Update the batch explorer visualization"""
        if not loader_key or loader_key not in data_loaders:
            # Return empty figure
            fig = dash.no_update
            return fig

        try:
            return create_batch_explorer(data_loaders, loader_key, batch_idx)
        except Exception as e:
            print(f"Error updating batch explorer: {e}")
            # Return empty figure with error message
            fig = dash.no_update
            return fig

    @app.callback(
        Output("node-explorer-fig", "figure"),
        [
            Input("loader-selector", "value"),
            Input("batch-slider", "value"),
            Input("node-slider", "value"),
        ],
    )
    def update_node_explorer(loader_key, batch_idx, node_idx):
        """Update the node explorer visualization"""
        if not loader_key or loader_key not in data_loaders:
            return dash.no_update

        try:
            return create_node_explorer(data_loaders, loader_key, batch_idx, node_idx)
        except Exception as e:
            print(f"Error updating node explorer: {e}")
            return dash.no_update

    @app.callback(
        Output("window-explorer-fig", "figure"),
        [
            Input("loader-selector", "value"),
            Input("node-slider", "value"),
            Input("window-slider", "value"),
        ],
    )
    def update_window_explorer(loader_key, node_idx, num_windows):
        """Update the window explorer visualization"""
        if not loader_key or loader_key not in data_loaders:
            return dash.no_update

        try:
            return create_window_explorer(
                data_loaders, loader_key, num_windows, node_idx
            )
        except Exception as e:
            print(f"Error updating window explorer: {e}")
            return dash.no_update

    @app.callback(
        Output("correlation-plot-fig", "figure"),
        [Input("loader-selector", "value"), Input("batch-slider", "value")],
    )
    def update_correlation_plot(loader_key, batch_idx):
        """Update the correlation plot"""
        if not loader_key or loader_key not in data_loaders:
            return dash.no_update

        try:
            return create_correlation_plot(data_loaders, loader_key, batch_idx)
        except Exception as e:
            print(f"Error updating correlation plot: {e}")
            return dash.no_update

    @app.callback(
        Output("adjacency-plot-fig", "figure"), [Input("loader-selector", "value")]
    )
    def update_adjacency_plot(loader_key):
        """Update the adjacency matrix visualization"""
        if not loader_key or loader_key not in data_loaders:
            return dash.no_update

        try:
            return create_adjacency_plot(data_loaders, loader_key)
        except Exception as e:
            print(f"Error updating adjacency plot: {e}")
            return dash.no_update

    @app.callback(
        Output("stats-panel-fig", "figure"), [Input("loader-selector", "value")]
    )
    def update_stats_panel(loader_key):
        """Update the statistics panel"""
        if not loader_key or loader_key not in data_loaders:
            return dash.no_update

        try:
            return create_stats_panel(data_loaders, loader_key)
        except Exception as e:
            print(f"Error updating stats panel: {e}")
            return dash.no_update

    @app.callback(
        Output("dataset-stats", "children"), [Input("loader-selector", "value")]
    )
    def update_dataset_stats(loader_key):
        """Update the dataset statistics summary"""
        if not loader_key or loader_key not in data_loaders:
            return html.Div("No data loader selected")

        try:
            # Create a simple HTML table with key stats
            batch = next(iter(data_loaders[loader_key]))
            batch_size, num_nodes, seq_len, feature_dim = batch["x"].shape
            _, _, horizon, _ = batch["y"].shape

            stats_table = html.Table(
                [
                    html.Tr([html.Th("Statistic"), html.Th("Value")]),
                    html.Tr([html.Td("Batch Size"), html.Td(str(batch_size))]),
                    html.Tr([html.Td("Number of Nodes"), html.Td(str(num_nodes))]),
                    html.Tr([html.Td("Sequence Length"), html.Td(str(seq_len))]),
                    html.Tr([html.Td("Prediction Horizon"), html.Td(str(horizon))]),
                    html.Tr([html.Td("Feature Dimension"), html.Td(str(feature_dim))]),
                ],
                className="table table-striped table-sm",
            )

            return stats_table

        except Exception as e:
            print(f"Error updating dataset stats: {e}")
            return html.Div(f"Error: {str(e)}")

    return app, data_loaders


def main():
    """Main entry point for the application"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="DataLoader Explorer Dashboard")
    parser.add_argument(
        "--data",
        "-d",
        type=str,
        required=True,
        help="Path to the pickle file containing data loaders",
    )
    parser.add_argument(
        "--port",
        "-p",
        type=int,
        default=8050,
        help="Port to run the dashboard server on",
    )
    parser.add_argument(
        "--debug", action="store_true", help="Run the app in debug mode"
    )

    args = parser.parse_args()

    # Create the Dash app
    app, data_loaders = create_dash_app(args.data)

    # Print data loader information
    available_loaders = [key for key in data_loaders.keys() if key.endswith("_loader")]
    print(f"Available data loaders: {available_loaders}")

    # Run the app
    print(f"\nStarting DataLoader Explorer Dashboard on port {args.port}")
    print(f"Open your browser and navigate to http://localhost:{args.port}/")

    # Open browser automatically
    webbrowser.open_new(f"http://localhost:{args.port}/")

    # Run server
    app.run(debug=args.debug, port=args.port)


if __name__ == "__main__":
    main()




================================================
File: dataloader_explorer/components/__init__.py
================================================
# dashboards/dataloader_explorer/components/__init__.py

from .batch_explorer import create_batch_explorer
from .node_explorer import create_node_explorer
from .window_explorer import create_window_explorer
from .correlation_plot import create_correlation_plot
from .adjacency_plot import create_adjacency_plot
from .data_stats import create_stats_panel

__all__ = [
    "create_batch_explorer",
    "create_node_explorer",
    "create_window_explorer",
    "create_correlation_plot",
    "create_adjacency_plot",
    "create_stats_panel",
]



================================================
File: dataloader_explorer/components/adjacency_plot.py
================================================
# dashboards/dataloader_explorer/components/adjacency_plot.py

import plotly.graph_objects as go
import numpy as np
import networkx as nx
from ..utils.data_utils import get_batch_from_loader


def create_adjacency_plot(data_loaders, loader_key="train_loader"):
    """
    Create an interactive visualization of the adjacency matrix

    Parameters:
    -----------
    data_loaders : dict
        Dictionary containing data loaders
    loader_key : str
        Key to access the loader from data_loaders

    Returns:
    --------
    plotly.graph_objects.Figure
        Adjacency matrix visualization
    """
    try:
        # Get batch data
        data_loader = data_loaders[loader_key]
        batch = get_batch_from_loader(data_loader)

        # Extract adjacency matrix and node indices
        adj = batch["adj"].cpu().numpy()
        node_indices = batch["node_indices"].cpu().numpy()

        # Create a figure with 2 subplots - heatmap and network graph
        fig = go.Figure()

        # Add adjacency matrix heatmap
        fig.add_trace(
            go.Heatmap(
                z=adj,
                x=[f"Node {id}" for id in node_indices],
                y=[f"Node {id}" for id in node_indices],
                colorscale="Viridis",
                colorbar=dict(title="Edge Weight", titleside="right"),
                hovertemplate="From: %{y}<br>To: %{x}<br>Weight: %{z:.4f}<extra></extra>",
                text=adj.round(3),  # Values to show in hover
            )
        )

        # Create a network layout using NetworkX for node positioning
        G = nx.from_numpy_array(adj)

        # Position nodes using a force-directed layout
        pos = nx.spring_layout(G, seed=42)

        # Get node positions
        node_x = []
        node_y = []
        for p in pos.values():
            node_x.append(p[0])
            node_y.append(p[1])

        # Create node trace
        node_trace = go.Scatter(
            x=node_x,
            y=node_y,
            mode="markers+text",
            text=[f"{id}" for id in node_indices],
            textposition="top center",
            marker=dict(
                size=15, color="lightblue", line=dict(width=1, color="darkblue")
            ),
            name="Nodes",
            hovertemplate="Node ID: %{text}<extra></extra>",
            visible=False,
        )

        # Create edge traces with varying line width based on edge weight
        edge_traces = []

        # Get edge weight range for normalization
        weight_min = np.min(adj[adj > 0]) if np.any(adj > 0) else 0.1
        weight_max = np.max(adj) if np.any(adj > 0) else 1.0

        # Function to normalize edge width
        def normalize_width(weight):
            return 1 + 8 * (weight - weight_min) / (weight_max - weight_min)

        # Create a trace for each edge weight range
        edge_weight_ranges = [
            (0, 0.25, "rgba(220, 220, 220, 0.3)"),  # Very light connections
            (0.25, 0.5, "rgba(144, 238, 144, 0.5)"),  # Light green
            (0.5, 0.75, "rgba(34, 139, 34, 0.7)"),  # Green
            (0.75, 1.01, "rgba(0, 100, 0, 0.9)"),  # Dark green
        ]

        for min_w, max_w, color in edge_weight_ranges:
            # Filter edges in this weight range
            edge_x = []
            edge_y = []
            edge_weights = []

            for i in range(len(node_indices)):
                for j in range(len(node_indices)):
                    weight = adj[i, j]

                    if weight <= 0:
                        continue

                    # Skip if not in range
                    norm_weight = weight / weight_max
                    if norm_weight < min_w or norm_weight >= max_w:
                        continue

                    edge_weights.append(weight)

                    x0, y0 = pos[i]
                    x1, y1 = pos[j]

                    # For curved edges, especially self-loops
                    if i == j:  # Self-loop
                        # Create a loop
                        loop_x = [x0, x0 + 0.1, x0 + 0.1, x0]
                        loop_y = [y0, y0 + 0.1, y0 - 0.1, y0]
                        edge_x.extend(loop_x)
                        edge_y.extend(loop_y)
                        edge_x.append(None)  # Add None to create a break
                        edge_y.append(None)
                    else:
                        edge_x.extend([x0, x1, None])
                        edge_y.extend([y0, y1, None])

            if edge_weights:
                widths = [normalize_width(w) for w in edge_weights]
                avg_width = np.mean(widths)

                edge_trace = go.Scatter(
                    x=edge_x,
                    y=edge_y,
                    mode="lines",
                    line=dict(width=avg_width, color=color),
                    hoverinfo="none",
                    name=f"Weight: {min_w:.2f}-{max_w:.2f}",
                    visible=False,
                )

                edge_traces.append(edge_trace)

        # Add all edge traces to the figure
        for trace in edge_traces:
            fig.add_trace(trace)

        # Add node trace (after edges, so nodes appear on top)
        fig.add_trace(node_trace)

        # Update layout
        fig.update_layout(
            title="Adjacency Matrix Visualization",
            height=800,
            width=800,
            xaxis=dict(title="To Node", tickangle=45),
            yaxis=dict(
                title="From Node",
                autorange="reversed",  # Ensure (0,0) is at the top-left
            ),
            updatemenus=[
                dict(
                    type="buttons",
                    direction="left",
                    buttons=[
                        dict(
                            args=[
                                {"visible": [True] + [False] * (len(edge_traces) + 1)}
                            ],
                            label="Heatmap View",
                            method="update",
                        ),
                        dict(
                            args=[
                                {"visible": [False] + [True] * (len(edge_traces) + 1)}
                            ],
                            label="Network View",
                            method="update",
                        ),
                    ],
                    pad={"r": 10, "t": 10},
                    showactive=True,
                    x=0.11,
                    xanchor="left",
                    y=1.1,
                    yanchor="top",
                ),
            ],
            margin=dict(t=120, b=100, l=100, r=100),
        )

        # Add annotations explaining the visualization
        fig.add_annotation(
            text=(
                "Toggle between heatmap and network views using the buttons above.<br>"
                "Heatmap view shows the full adjacency matrix with cell values representing edge weights.<br>"
                "Network view shows the graph structure with edge thickness representing weight strength."
            ),
            xref="paper",
            yref="paper",
            x=0.5,
            y=1.05,
            showarrow=False,
            font=dict(size=12),
            align="center",
            bordercolor="black",
            borderwidth=1,
            borderpad=4,
            bgcolor="white",
        )

        # Update axes for network view
        fig.update_xaxes(
            showgrid=False, zeroline=False, showticklabels=False, range=[-1.2, 1.2]
        )
        fig.update_yaxes(
            showgrid=False,
            zeroline=False,
            showticklabels=False,
            scaleanchor="x",
            scaleratio=1,  # Equal aspect ratio
            range=[-1.2, 1.2],
        )

        return fig

    except Exception as e:
        # Create an error figure
        fig = go.Figure()
        fig.add_annotation(
            text=f"Error creating adjacency plot: {str(e)}",
            xref="paper",
            yref="paper",
            x=0.5,
            y=0.5,
            showarrow=False,
            font=dict(size=14, color="red"),
        )
        return fig



================================================
File: dataloader_explorer/components/batch_explorer.py
================================================
# dashboards/dataloader_explorer/components/batch_explorer.py

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import pandas as pd
from ..utils.data_utils import get_batch_from_loader


def create_batch_explorer(data_loaders, loader_key="train_loader", batch_idx=0):
    """
    Create an interactive batch explorer visualization

    Parameters:
    -----------
    data_loaders : dict
        Dictionary containing data loaders
    loader_key : str
        Key to access the loader from data_loaders
    batch_idx : int
        Index of the batch to visualize

    Returns:
    --------
    plotly.graph_objects.Figure
        Interactive batch explorer figure
    """
    # Get batch data
    try:
        data_loader = data_loaders[loader_key]
        batch = get_batch_from_loader(data_loader, batch_idx)
    except Exception as e:
        # Create an error figure
        fig = go.Figure()
        fig.add_annotation(
            text=f"Error loading batch: {str(e)}",
            xref="paper",
            yref="paper",
            x=0.5,
            y=0.5,
            showarrow=False,
            font=dict(size=14, color="red"),
        )
        return fig

    # Extract dimensions
    x = batch["x"]
    batch_size, num_nodes, seq_len, features = x.shape

    # Create a figure with subplots to show batch overview
    fig = make_subplots(
        rows=1,
        cols=2,
        subplot_titles=[
            "Batch Data Availability by Node",
            "Batch Missing Data Overview",
        ],
        specs=[[{"type": "heatmap"}, {"type": "heatmap"}]],
        horizontal_spacing=0.1,
    )

    # Extract data for visualization
    node_ids = batch["node_indices"].cpu().numpy()

    # Create data availability heatmap
    # For each node and time step, calculate percentage of valid data points across batch
    availability_matrix = np.zeros((num_nodes, seq_len))
    for n in range(num_nodes):
        for t in range(seq_len):
            mask_values = batch["x_mask"][:, n, t, 0].cpu().numpy()
            availability_matrix[n, t] = np.mean(mask_values) * 100

    # Create heatmap showing data availability
    fig.add_trace(
        go.Heatmap(
            z=availability_matrix,
            x=[f"t{i}" for i in range(seq_len)],
            y=[f"Node {id}" for id in node_ids],
            colorscale="Blues",
            zmin=0,
            zmax=100,
            colorbar=dict(title="Data Available (%)", x=0.46),
            hovertemplate="Node: %{y}<br>Time Step: %{x}<br>Data Available: %{z:.1f}%<extra></extra>",
        ),
        row=1,
        col=1,
    )

    # Create a summary matrix showing missing data by feature and node
    # First for input (x)
    x_missing = np.zeros((num_nodes, 2))
    for n in range(num_nodes):
        # Input data missing
        x_missing[n, 0] = 100 - (
            batch["x_mask"][:, n, :, :].sum().item()
            / batch["x_mask"][:, n, :, :].numel()
            * 100
        )
        # Target data missing
        x_missing[n, 1] = 100 - (
            batch["y_mask"][:, n, :, :].sum().item()
            / batch["y_mask"][:, n, :, :].numel()
            * 100
        )

    # Create heatmap showing missing data by feature
    fig.add_trace(
        go.Heatmap(
            z=x_missing,
            x=["Input", "Target"],
            y=[f"Node {id}" for id in node_ids],
            colorscale="Reds",
            zmin=0,
            zmax=100,
            colorbar=dict(title="Missing Data (%)", x=1.0),
            hovertemplate="Node: %{y}<br>Data Type: %{x}<br>Missing: %{z:.1f}%<extra></extra>",
        ),
        row=1,
        col=2,
    )

    # Update layout with more information
    fig.update_layout(
        title=f"Batch Explorer - {loader_key} (Batch {batch_idx})",
        height=600,
        margin=dict(t=100, b=50, l=50, r=50),
    )

    # Add summary information in an annotation
    summary_text = (
        f"Batch Size: {batch_size}<br>"
        f"Number of Nodes: {num_nodes}<br>"
        f"Sequence Length: {seq_len}<br>"
        f"Missing Input Data: {x_missing[:, 0].mean():.1f}%<br>"
        f"Missing Target Data: {x_missing[:, 1].mean():.1f}%"
    )

    fig.add_annotation(
        text=summary_text,
        xref="paper",
        yref="paper",
        x=0.5,
        y=1.05,
        showarrow=False,
        font=dict(size=12),
        align="center",
        bordercolor="black",
        borderwidth=1,
        borderpad=4,
        bgcolor="white",
    )

    return fig



================================================
File: dataloader_explorer/components/correlation_plot.py
================================================
# dashboards/dataloader_explorer/components/correlation_plot.py

import plotly.graph_objects as go
import numpy as np
from ..utils.data_utils import get_batch_from_loader, compute_node_correlations


def create_correlation_plot(data_loaders, loader_key="train_loader", batch_idx=0):
    """
    Create a correlation matrix visualization between nodes

    Parameters:
    -----------
    data_loaders : dict
        Dictionary containing data loaders
    loader_key : str
        Key to access the loader from data_loaders
    batch_idx : int
        Index of the batch to analyze

    Returns:
    --------
    plotly.graph_objects.Figure
        Correlation matrix figure
    """
    try:
        # Get batch data
        data_loader = data_loaders[loader_key]
        batch = get_batch_from_loader(data_loader, batch_idx)

        # Compute node correlations
        corr_matrix, node_ids = compute_node_correlations(batch, batch_idx)

        # Create correlation heatmap
        fig = go.Figure()

        # Add correlation heatmap
        fig.add_trace(
            go.Heatmap(
                z=corr_matrix.values,
                x=[f"Node {id}" for id in node_ids],
                y=[f"Node {id}" for id in node_ids],
                colorscale="RdBu",
                zmin=-1,
                zmax=1,
                colorbar=dict(
                    title="Correlation",
                    titleside="right",
                    tickvals=[-1, -0.5, 0, 0.5, 1],
                    ticktext=["-1.0", "-0.5", "0.0", "0.5", "1.0"],
                ),
                hovertemplate="Node X: %{x}<br>Node Y: %{y}<br>Correlation: %{z:.4f}<extra></extra>",
                text=corr_matrix.values.round(2),  # Values to show in hover
            )
        )

        # Add a markers for NaN values
        nan_mask = np.isnan(corr_matrix.values)
        if np.any(nan_mask):
            nan_indices = np.where(nan_mask)
            fig.add_trace(
                go.Scatter(
                    x=[f"Node {node_ids[i]}" for i in nan_indices[1]],
                    y=[f"Node {node_ids[i]}" for i in nan_indices[0]],
                    mode="markers",
                    marker=dict(
                        symbol="x",
                        size=8,
                        color="gray",
                        line=dict(width=1, color="darkgray"),
                    ),
                    name="Insufficient Data",
                    hoverinfo="skip",
                )
            )

        # Update layout
        fig.update_layout(
            title=f"Node Correlation Matrix - {loader_key} (Batch {batch_idx})",
            height=700,
            width=800,
            xaxis=dict(title="Node", tickangle=45),
            yaxis=dict(
                title="Node", autorange="reversed"  # Ensure (0,0) is at the top-left
            ),
            margin=dict(t=100, b=100, l=100, r=100),
        )

        # Add a summary annotation
        summary_text = (
            f"Correlation matrix showing the Pearson correlation coefficient<br>"
            f"between time series data from different nodes.<br>"
            f"Values range from -1 (strong negative correlation) to 1 (strong positive correlation).<br>"
            f"0 indicates no linear correlation between nodes.<br>"
            f"X markers indicate insufficient data for correlation calculation."
        )

        fig.add_annotation(
            text=summary_text,
            xref="paper",
            yref="paper",
            x=0.5,
            y=1.05,
            showarrow=False,
            font=dict(size=12),
            align="center",
            bordercolor="black",
            borderwidth=1,
            borderpad=4,
            bgcolor="white",
        )

        return fig

    except Exception as e:
        # Create an error figure
        fig = go.Figure()
        fig.add_annotation(
            text=f"Error creating correlation plot: {str(e)}",
            xref="paper",
            yref="paper",
            x=0.5,
            y=0.5,
            showarrow=False,
            font=dict(size=14, color="red"),
        )
        return fig



================================================
File: dataloader_explorer/components/data_stats.py
================================================
# dashboards/dataloader_explorer/components/data_stats.py

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from ..utils.data_utils import get_dataloader_stats


def create_stats_panel(data_loaders, loader_key="train_loader"):
    """
    Create a statistical summary panel for a data loader

    Parameters:
    -----------
    data_loaders : dict
        Dictionary containing data loaders
    loader_key : str
        Key to access the loader from data_loaders

    Returns:
    --------
    plotly.graph_objects.Figure
        Data statistics visualization
    """
    try:
        # Get data loader statistics
        stats = get_dataloader_stats(data_loaders[loader_key])

        # Check for errors
        if "error" in stats:
            # Create an error figure
            fig = go.Figure()
            fig.add_annotation(
                text=f"Error computing statistics: {stats['error']}",
                xref="paper",
                yref="paper",
                x=0.5,
                y=0.5,
                showarrow=False,
                font=dict(size=14, color="red"),
            )
            return fig

        # Create a figure with multiple subplots
        fig = make_subplots(
            rows=2,
            cols=2,
            subplot_titles=[
                "Data Value Distribution",
                "Missing Data by Type",
                "Data Statistics by Type",
                "Adjacency Matrix Statistics",
            ],
            specs=[
                [{"type": "xy"}, {"type": "domain"}],
                [{"type": "table"}, {"type": "table"}],
            ],
            vertical_spacing=0.15,
            horizontal_spacing=0.08,
        )

        # 1. Create a box plot of data values distribution
        # Extract valid values from input data
        if (
            stats["x_stats"]["mean"] is not None
            and stats["y_stats"]["mean"] is not None
        ):
            # Create box plots
            boxplot_data = [
                go.Box(
                    name="Input Data",
                    y=[
                        stats["x_stats"]["min"],
                        stats["x_stats"]["mean"] - stats["x_stats"]["std"],
                        stats["x_stats"]["mean"],
                        stats["x_stats"]["mean"] + stats["x_stats"]["std"],
                        stats["x_stats"]["max"],
                    ],
                    boxpoints=False,
                    marker_color="blue",
                ),
                go.Box(
                    name="Target Data",
                    y=[
                        stats["y_stats"]["min"],
                        stats["y_stats"]["mean"] - stats["y_stats"]["std"],
                        stats["y_stats"]["mean"],
                        stats["y_stats"]["mean"] + stats["y_stats"]["std"],
                        stats["y_stats"]["max"],
                    ],
                    boxpoints=False,
                    marker_color="green",
                ),
            ]

            for trace in boxplot_data:
                fig.add_trace(trace, row=1, col=1)
        else:
            # Add a message if no valid data
            fig.add_annotation(
                text="Insufficient data for distribution analysis",
                xref="x",
                yref="y",
                x=0.5,
                y=0.5,
                showarrow=False,
                font=dict(size=12, color="gray"),
                row=1,
                col=1,
            )

        # 2. Create a pie chart of missing data
        labels = ["Valid Input", "Missing Input", "Valid Target", "Missing Target"]
        values = [
            100 - stats["input_missing_pct"],
            stats["input_missing_pct"],
            100 - stats["target_missing_pct"],
            stats["target_missing_pct"],
        ]

        colors = [
            "rgba(100, 149, 237, 0.8)",
            "rgba(255, 99, 71, 0.8)",
            "rgba(50, 205, 50, 0.8)",
            "rgba(255, 165, 0, 0.8)",
        ]

        fig.add_trace(
            go.Pie(
                labels=labels,
                values=values,
                textinfo="label+percent",
                insidetextorientation="radial",
                marker=dict(colors=colors),
                hoverinfo="label+percent+value",
                hole=0.3,
            ),
            row=1,
            col=2,
        )

        # 3. Create a table of data statistics
        if (
            stats["x_stats"]["mean"] is not None
            and stats["y_stats"]["mean"] is not None
        ):
            stat_names = ["Mean", "Std Dev", "Min", "Max"]
            input_stats = [
                f"{stats['x_stats']['mean']:.4f}",
                f"{stats['x_stats']['std']:.4f}",
                f"{stats['x_stats']['min']:.4f}",
                f"{stats['x_stats']['max']:.4f}",
            ]
            target_stats = [
                f"{stats['y_stats']['mean']:.4f}",
                f"{stats['y_stats']['std']:.4f}",
                f"{stats['y_stats']['min']:.4f}",
                f"{stats['y_stats']['max']:.4f}",
            ]

            fig.add_trace(
                go.Table(
                    header=dict(
                        values=["Statistic", "Input Data", "Target Data"],
                        fill_color="rgba(0, 0, 100, 0.1)",
                        align="center",
                        font=dict(size=12),
                    ),
                    cells=dict(
                        values=[stat_names, input_stats, target_stats],
                        fill_color=[
                            "rgba(0, 0, 0, 0.01)",
                            "rgba(100, 149, 237, 0.1)",
                            "rgba(50, 205, 50, 0.1)",
                        ],
                        align="center",
                        font=dict(size=11),
                    ),
                ),
                row=2,
                col=1,
            )
        else:
            # Add a message if no valid data
            fig.add_annotation(
                text="Insufficient data for statistical analysis",
                xref="x3",
                yref="y3",
                x=0.5,
                y=0.5,
                showarrow=False,
                font=dict(size=12, color="gray"),
                row=2,
                col=1,
            )

        # 4. Create a table of adjacency matrix statistics
        adj_stat_names = [
            "Min Weight",
            "Max Weight",
            "Mean Weight",
            "Sparsity (% zeros)",
        ]
        adj_stat_values = [
            f"{stats['adj_stats']['min']:.4f}",
            f"{stats['adj_stats']['max']:.4f}",
            f"{stats['adj_stats']['mean']:.4f}",
            f"{stats['adj_stats']['sparsity']:.2f}%",
        ]

        fig.add_trace(
            go.Table(
                header=dict(
                    values=["Statistic", "Value"],
                    fill_color="rgba(0, 0, 100, 0.1)",
                    align="center",
                    font=dict(size=12),
                ),
                cells=dict(
                    values=[adj_stat_names, adj_stat_values],
                    fill_color=["rgba(0, 0, 0, 0.01)", "rgba(144, 238, 144, 0.2)"],
                    align="center",
                    font=dict(size=11),
                ),
            ),
            row=2,
            col=2,
        )

        # Update layout
        fig.update_layout(
            title=f"DataLoader Statistics - {loader_key}",
            height=700,
            showlegend=False,
            margin=dict(t=100, b=50, l=50, r=50),
        )

        # Add a summary annotation
        summary_text = (
            f"Batch Size: {stats['batch_size']}<br>"
            f"Number of Nodes: {stats['num_nodes']}<br>"
            f"Input Length: {stats['seq_len']} time steps<br>"
            f"Target Length: {stats['horizon']} time steps<br>"
            f"Number of Features: {stats['features']}"
        )

        fig.add_annotation(
            text=summary_text,
            xref="paper",
            yref="paper",
            x=0.5,
            y=1.05,
            showarrow=False,
            font=dict(size=12),
            align="center",
            bordercolor="black",
            borderwidth=1,
            borderpad=4,
            bgcolor="white",
        )

        return fig

    except Exception as e:
        # Create an error figure
        fig = go.Figure()
        fig.add_annotation(
            text=f"Error creating statistics panel: {str(e)}",
            xref="paper",
            yref="paper",
            x=0.5,
            y=0.5,
            showarrow=False,
            font=dict(size=14, color="red"),
        )
        return fig



================================================
File: dataloader_explorer/components/node_explorer.py
================================================
# dashboards/dataloader_explorer/components/node_explorer.py

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from ..utils.data_utils import get_batch_from_loader, get_node_data


def create_node_explorer(
    data_loaders, loader_key="train_loader", batch_idx=0, node_idx=0
):
    """
    Create an interactive node explorer visualization

    Parameters:
    -----------
    data_loaders : dict
        Dictionary containing data loaders
    loader_key : str
        Key to access the loader from data_loaders
    batch_idx : int
        Index of the batch to visualize
    node_idx : int
        Index of the node to visualize

    Returns:
    --------
    plotly.graph_objects.Figure
        Interactive node explorer figure
    """
    # Get batch data
    try:
        data_loader = data_loaders[loader_key]
        batch = get_batch_from_loader(data_loader, batch_idx)
        node_data = get_node_data(batch, batch_idx, node_idx)
    except Exception as e:
        # Create an error figure
        fig = go.Figure()
        fig.add_annotation(
            text=f"Error loading node data: {str(e)}",
            xref="paper",
            yref="paper",
            x=0.5,
            y=0.5,
            showarrow=False,
            font=dict(size=14, color="red"),
        )
        return fig

    # Extract dimensions and node ID
    node_id = node_data["node_id"]
    seq_len = node_data["seq_len"]
    horizon = node_data["horizon"]

    # Create a figure with 2 subplots (input and target data)
    fig = make_subplots(
        rows=1,
        cols=2,
        subplot_titles=["Input Data (Historical)", "Target Data (Future)"],
        column_widths=[0.7, 0.3],
        horizontal_spacing=0.08,
    )

    # Create time step arrays
    input_steps = np.arange(seq_len)
    target_steps = np.arange(seq_len, seq_len + horizon)

    # Plot input data
    x_data = node_data["input_data"]
    x_mask = node_data["input_mask"]

    # Create separate traces for valid and missing data to control styling
    valid_mask = x_mask > 0

    # Plot valid data points
    if np.any(valid_mask):
        fig.add_trace(
            go.Scatter(
                x=input_steps[valid_mask],
                y=x_data[valid_mask],
                mode="lines+markers",
                line=dict(color="royalblue", width=2),
                marker=dict(
                    size=8, color="royalblue", line=dict(width=1, color="darkblue")
                ),
                name="Valid Input Data",
                hovertemplate="Step: %{x}<br>Value: %{y:.4f}<extra>Valid Input</extra>",
            ),
            row=1,
            col=1,
        )

    # Plot missing data points
    missing_mask = ~valid_mask
    if np.any(missing_mask):
        fig.add_trace(
            go.Scatter(
                x=input_steps[missing_mask],
                y=x_data[missing_mask],
                mode="markers",
                marker=dict(
                    symbol="x",
                    size=10,
                    color="red",
                    line=dict(width=1, color="darkred"),
                ),
                name="Missing Input Data",
                hovertemplate="Step: %{x}<br>Value: %{y:.4f}<extra>Missing Input</extra>",
            ),
            row=1,
            col=1,
        )

        # Add a semi-transparent rectangle for each missing value
        for step in input_steps[missing_mask]:
            fig.add_shape(
                type="rect",
                xref=f"x",
                yref=f"y",
                x0=step - 0.4,
                y0=-999999,
                x1=step + 0.4,
                y1=999999,
                fillcolor="rgba(255, 0, 0, 0.1)",
                line=dict(width=0),
                layer="below",
                row=1,
                col=1,
            )

    # Plot target data
    y_data = node_data["target_data"]
    y_mask = node_data["target_mask"]

    # Create separate traces for valid and missing target data
    valid_mask = y_mask > 0

    # Plot valid target data points
    if np.any(valid_mask):
        fig.add_trace(
            go.Scatter(
                x=target_steps[valid_mask],
                y=y_data[valid_mask],
                mode="lines+markers",
                line=dict(color="green", width=2, dash="dash"),
                marker=dict(
                    size=8, color="green", line=dict(width=1, color="darkgreen")
                ),
                name="Valid Target Data",
                hovertemplate="Step: %{x}<br>Value: %{y:.4f}<extra>Valid Target</extra>",
            ),
            row=1,
            col=2,
        )

    # Plot missing target data points
    missing_mask = ~valid_mask
    if np.any(missing_mask):
        fig.add_trace(
            go.Scatter(
                x=target_steps[missing_mask],
                y=y_data[missing_mask],
                mode="markers",
                marker=dict(
                    symbol="x",
                    size=10,
                    color="orange",
                    line=dict(width=1, color="darkorange"),
                ),
                name="Missing Target Data",
                hovertemplate="Step: %{x}<br>Value: %{y:.4f}<extra>Missing Target</extra>",
            ),
            row=1,
            col=2,
        )

        # Add a semi-transparent rectangle for each missing value
        for step in target_steps[missing_mask]:
            fig.add_shape(
                type="rect",
                xref=f"x2",
                yref=f"y2",
                x0=step - 0.4,
                y0=-999999,
                x1=step + 0.4,
                y1=999999,
                fillcolor="rgba(255, 165, 0, 0.1)",
                line=dict(width=0),
                layer="below",
                row=1,
                col=2,
            )

    # Add a vertical line at the boundary between input and prediction
    fig.add_vline(
        x=seq_len - 0.5, line=dict(color="black", width=1, dash="dash"), row=1, col=1
    )

    # Add a vertical line at the boundary between input and prediction in the prediction panel
    fig.add_vline(
        x=seq_len - 0.5, line=dict(color="black", width=1, dash="dash"), row=1, col=2
    )

    # Update layout
    fig.update_layout(
        title=f"Node Explorer - Node {node_id} (Batch {batch_idx}, {loader_key})",
        height=500,
        legend=dict(orientation="h", yanchor="bottom", y=-0.2, xanchor="center", x=0.5),
        margin=dict(t=100, b=100, l=50, r=50),
    )

    # Update x and y axes
    fig.update_xaxes(title_text="Time Step", row=1, col=1)
    fig.update_xaxes(title_text="Time Step", row=1, col=2)
    fig.update_yaxes(title_text="Value", row=1, col=1)
    fig.update_yaxes(title_text="Value", row=1, col=2)

    # Ensure the same y-axis range for both panels for better comparison
    all_values = np.concatenate([x_data, y_data])
    if len(all_values) > 0:
        y_min = np.nanmin(all_values) - 0.5
        y_max = np.nanmax(all_values) + 0.5
        fig.update_yaxes(range=[y_min, y_max], row=1, col=1)
        fig.update_yaxes(range=[y_min, y_max], row=1, col=2)

    # Add a summary of node data in an annotation
    summary_text = (
        f"Node ID: {node_id}<br>"
        f"Input Length: {seq_len} steps<br>"
        f"Target Horizon: {horizon} steps<br>"
        f"Valid Input Points: {np.sum(node_data['input_mask'])} / {seq_len} ({np.mean(node_data['input_mask'])*100:.1f}%)<br>"
        f"Valid Target Points: {np.sum(node_data['target_mask'])} / {horizon} ({np.mean(node_data['target_mask'])*100:.1f}%)"
    )

    fig.add_annotation(
        text=summary_text,
        xref="paper",
        yref="paper",
        x=0.5,
        y=1.05,
        showarrow=False,
        font=dict(size=12),
        align="center",
        bordercolor="black",
        borderwidth=1,
        borderpad=4,
        bgcolor="white",
    )

    return fig



================================================
File: dataloader_explorer/components/window_explorer.py
================================================
# dashboards/dataloader_explorer/components/window_explorer.py

import plotly.graph_objects as go
import numpy as np
import pandas as pd
from ..utils.data_utils import get_batch_from_loader


def create_window_explorer(
    data_loaders, loader_key="train_loader", num_windows=10, node_idx=0
):
    """
    Create a visualization of multiple windows for a specific node

    Parameters:
    -----------
    data_loaders : dict
        Dictionary containing data loaders
    loader_key : str
        Key to access the loader from data_loaders
    num_windows : int
        Number of windows to display
    node_idx : int
        Index of the node to visualize

    Returns:
    --------
    plotly.graph_objects.Figure
        Window explorer figure
    """
    try:
        # Get dataloader
        data_loader = data_loaders[loader_key]

        # Create dataloader iterator
        iterator = iter(data_loader)

        # Collect windows
        all_windows = []
        all_masks = []
        node_ids = []
        batch_indices = []
        window_indices = []

        # Process batches to collect windows
        batch_idx = 0
        window_count = 0

        while window_count < num_windows:
            try:
                # Get next batch
                batch = next(iterator)

                # Extract batch components
                x = batch["x"]
                x_mask = batch["x_mask"]
                node_indices = batch["node_indices"]

                # Check if node_idx is valid
                if node_idx >= len(node_indices):
                    print(
                        f"Warning: node_idx {node_idx} is out of range (max {len(node_indices)-1})"
                    )
                    break

                # Get the node ID
                node_id = node_indices[node_idx].item()

                # Store node ID
                if not node_ids:
                    node_ids.append(node_id)
                elif node_id != node_ids[0]:
                    print(f"Warning: Node ID changed from {node_ids[0]} to {node_id}")
                    node_ids.append(node_id)
                    continue

                # Get all windows for this node in this batch
                batch_size = x.shape[0]
                for b in range(batch_size):
                    # Get window data and mask
                    window_data = x[b, node_idx, :, 0].cpu().numpy()
                    window_mask = x_mask[b, node_idx, :, 0].cpu().numpy()

                    # Record window data and batch/window indices
                    all_windows.append(window_data)
                    all_masks.append(window_mask)
                    batch_indices.append(batch_idx)
                    window_indices.append(b)

                    window_count += 1
                    if window_count >= num_windows:
                        break

                batch_idx += 1

            except StopIteration:
                print(f"Reached end of dataloader. Found {len(all_windows)} windows.")
                break

        # Create visualization if we have windows
        if not all_windows:
            # Create empty figure with a message
            fig = go.Figure()
            fig.add_annotation(
                text="No windows found for the selected node.",
                xref="paper",
                yref="paper",
                x=0.5,
                y=0.5,
                showarrow=False,
                font=dict(size=14, color="red"),
            )
            return fig

        # Convert to arrays
        windows_array = np.array(all_windows)
        masks_array = np.array(all_masks)
        seq_len = windows_array.shape[1]

        # Create a heatmap figure for the collected windows
        fig = go.Figure()

        # Determine value range for consistent color scaling
        valid_values = windows_array[masks_array > 0]
        if len(valid_values) > 0:
            vmin = np.min(valid_values)
            vmax = np.max(valid_values)
        else:
            vmin = -1
            vmax = 1

        # Create heatmap of window values
        heatmap_z = np.copy(windows_array)
        # Replace invalid values with NaN for visualization
        heatmap_z[masks_array == 0] = np.nan

        fig.add_trace(
            go.Heatmap(
                z=heatmap_z,
                x=[f"Step {i+1}" for i in range(seq_len)],
                y=[f"Window {i+1}" for i in range(len(all_windows))],
                colorscale="Viridis",
                zmin=vmin,
                zmax=vmax,
                colorbar=dict(title="Value"),
                hovertemplate="Window: %{y}<br>Step: %{x}<br>Value: %{z:.4f}<extra></extra>",
            )
        )

        # Highlight missing values with markers
        missing_y = []
        missing_x = []
        for i, (window, mask) in enumerate(zip(windows_array, masks_array)):
            for j, (val, m) in enumerate(zip(window, mask)):
                if m == 0:
                    missing_y.append(i)
                    missing_x.append(j)

        if missing_y:
            fig.add_trace(
                go.Scatter(
                    x=[f"Step {x+1}" for x in missing_x],
                    y=[f"Window {y+1}" for y in missing_y],
                    mode="markers",
                    marker=dict(
                        symbol="x",
                        size=8,
                        color="red",
                        line=dict(width=1, color="darkred"),
                    ),
                    name="Missing Values",
                    hoverinfo="skip",
                )
            )

        # Update layout
        node_id = node_ids[0] if node_ids else "unknown"
        fig.update_layout(
            title=f"Window Explorer - Node {node_id} ({loader_key})",
            height=max(500, min(len(all_windows) * 30 + 200, 800)),
            xaxis_title="Time Step",
            yaxis_title="Window",
            yaxis=dict(autorange="reversed"),  # Put Window 1 at the top
            margin=dict(t=100, b=50, l=50, r=50),
        )

        # Add a summary annotation
        summary_text = (
            f"Node ID: {node_id}<br>"
            f"Total Windows: {len(all_windows)}<br>"
            f"Window Length: {seq_len}<br>"
            f"Missing Data: {100 - (np.sum(masks_array) / masks_array.size * 100):.1f}%"
        )

        fig.add_annotation(
            text=summary_text,
            xref="paper",
            yref="paper",
            x=0.5,
            y=1.05,
            showarrow=False,
            font=dict(size=12),
            align="center",
            bordercolor="black",
            borderwidth=1,
            borderpad=4,
            bgcolor="white",
        )

        return fig

    except Exception as e:
        # Create an error figure
        fig = go.Figure()
        fig.add_annotation(
            text=f"Error creating window explorer: {str(e)}",
            xref="paper",
            yref="paper",
            x=0.5,
            y=0.5,
            showarrow=False,
            font=dict(size=14, color="red"),
        )
        return fig




================================================
File: dataloader_explorer/static/css/styles.css
================================================



================================================
File: dataloader_explorer/static/js/custom.js
================================================



================================================
File: dataloader_explorer/templates/__init__.py
================================================
# dashboards/dataloader_explorer/templates/__init__.py

from pathlib import Path


def get_template_path(template_name):
    """
    Get the full path to a template file

    Parameters:
    -----------
    template_name : str
        The name of the template file

    Returns:
    --------
    Path
        Full path to the template file
    """
    return Path(__file__).parent / template_name



================================================
File: dataloader_explorer/templates/layout.html
================================================



================================================
File: dataloader_explorer/utils/__init__.py
================================================
# dashboards/dataloader_explorer/utils/__init__.py

from .data_utils import (
    load_data_loaders,
    get_batch_from_loader,
    get_node_data,
    get_dataloader_stats,
    compute_node_correlations,
)

__all__ = [
    "load_data_loaders",
    "get_batch_from_loader",
    "get_node_data",
    "get_dataloader_stats",
    "compute_node_correlations",
]



================================================
File: dataloader_explorer/utils/data_utils.py
================================================
# dashboards/dataloader_explorer/utils/data_utils.py

import os
import pickle
import numpy as np
import pandas as pd
import torch


def load_data_loaders(pickle_path):
    """
    Load data loaders from a pickle file

    Parameters:
    -----------
    pickle_path : str
        Path to the pickle file containing data loaders

    Returns:
    --------
    dict
        Dictionary containing train_loader, val_loader, and other data
    """
    if not os.path.exists(pickle_path):
        raise FileNotFoundError(f"File not found: {pickle_path}")

    with open(pickle_path, "rb") as f:
        data_loaders = pickle.load(f)

    return data_loaders


def get_batch_from_loader(data_loader, batch_index=0):
    """
    Get a specific batch from a data loader

    Parameters:
    -----------
    data_loader : torch.utils.data.DataLoader
        DataLoader to get batch from
    batch_index : int, optional
        Index of the batch to retrieve

    Returns:
    --------
    dict
        Batch data
    """
    # Create an iterator from the data loader
    iterator = iter(data_loader)

    # Skip to the desired batch
    batch = None
    for i in range(batch_index + 1):
        try:
            batch = next(iterator)
        except StopIteration:
            raise IndexError(f"Batch index {batch_index} is out of range")

    return batch


def get_node_data(batch, batch_idx, node_idx):
    """
    Extract data for a specific node from a batch

    Parameters:
    -----------
    batch : dict
        Batch data
    batch_idx : int
        Index within the batch
    node_idx : int
        Index of the node

    Returns:
    --------
    dict
        Node data with keys: input_data, input_mask, target_data, target_mask, node_id
    """
    # Validate indices
    batch_size, num_nodes = batch["x"].shape[0], batch["x"].shape[1]

    if batch_idx >= batch_size:
        raise ValueError(f"batch_idx {batch_idx} is out of range (max {batch_size-1})")

    if node_idx >= num_nodes:
        raise ValueError(f"node_idx {node_idx} is out of range (max {num_nodes-1})")

    # Get the actual node ID
    node_id = batch["node_indices"][node_idx].item()

    # Get data for this node
    x_data = batch["x"][batch_idx, node_idx, :, 0].cpu().numpy()
    x_mask = batch["x_mask"][batch_idx, node_idx, :, 0].cpu().numpy()
    y_data = batch["y"][batch_idx, node_idx, :, 0].cpu().numpy()
    y_mask = batch["y_mask"][batch_idx, node_idx, :, 0].cpu().numpy()

    # Get dimension information
    seq_len = x_data.shape[0]
    horizon = y_data.shape[0]

    return {
        "input_data": x_data,
        "input_mask": x_mask,
        "target_data": y_data,
        "target_mask": y_mask,
        "node_id": node_id,
        "seq_len": seq_len,
        "horizon": horizon,
    }


def get_dataloader_stats(data_loader):
    """
    Compute summary statistics for a data loader

    Parameters:
    -----------
    data_loader : torch.utils.data.DataLoader
        DataLoader to analyze

    Returns:
    --------
    dict
        Statistics about the data loader
    """
    try:
        # Get a sample batch
        batch = get_batch_from_loader(data_loader)

        # Extract shapes and dimensions
        x = batch["x"]
        x_mask = batch["x_mask"]
        y = batch["y"]
        y_mask = batch["y_mask"]
        adj = batch["adj"]
        node_indices = batch["node_indices"]

        # Basic statistics
        batch_size, num_nodes, seq_len, features = x.shape
        _, _, horizon, _ = y.shape

        # Compute missing data percentage
        input_missing_pct = 100 - (x_mask.sum().item() / x_mask.numel() * 100)
        target_missing_pct = 100 - (y_mask.sum().item() / y_mask.numel() * 100)

        # Compute basic data statistics
        x_valid = x[x_mask.bool()]
        y_valid = y[y_mask.bool()]

        x_stats = {
            "mean": x_valid.mean().item() if len(x_valid) > 0 else None,
            "std": x_valid.std().item() if len(x_valid) > 0 else None,
            "min": x_valid.min().item() if len(x_valid) > 0 else None,
            "max": x_valid.max().item() if len(x_valid) > 0 else None,
        }

        y_stats = {
            "mean": y_valid.mean().item() if len(y_valid) > 0 else None,
            "std": y_valid.std().item() if len(y_valid) > 0 else None,
            "min": y_valid.min().item() if len(y_valid) > 0 else None,
            "max": y_valid.max().item() if len(y_valid) > 0 else None,
        }

        # Adjacency matrix statistics
        adj_numpy = adj.cpu().numpy()
        adj_stats = {
            "min": float(np.min(adj_numpy)),
            "max": float(np.max(adj_numpy)),
            "mean": float(np.mean(adj_numpy)),
            "sparsity": float(
                100 - (np.count_nonzero(adj_numpy) / adj_numpy.size * 100)
            ),
        }

        # Collect and return all statistics
        return {
            "batch_size": batch_size,
            "num_nodes": num_nodes,
            "seq_len": seq_len,
            "horizon": horizon,
            "features": features,
            "input_missing_pct": input_missing_pct,
            "target_missing_pct": target_missing_pct,
            "unique_node_ids": node_indices.cpu().numpy().tolist(),
            "x_stats": x_stats,
            "y_stats": y_stats,
            "adj_stats": adj_stats,
        }

    except Exception as e:
        return {"error": str(e)}


def compute_node_correlations(batch, batch_idx=0):
    """
    Compute correlations between nodes in a batch

    Parameters:
    -----------
    batch : dict
        Batch data
    batch_idx : int
        Index within the batch

    Returns:
    --------
    tuple
        (correlation_matrix, node_ids)
    """
    # Extract batch components
    x = batch["x"]  # [batch_size, num_nodes, seq_len, features]
    x_mask = batch["x_mask"]
    node_indices = batch["node_indices"]

    # Get dimensions
    batch_size, num_nodes, seq_len, _ = x.shape

    # Validate batch_idx
    if batch_idx >= batch_size:
        raise ValueError(f"batch_idx {batch_idx} is out of range (max {batch_size-1})")

    # Extract data for all nodes in this batch
    all_node_data = []
    node_ids = []

    for node_idx in range(num_nodes):
        # Get the node ID
        node_id = node_indices[node_idx].item()
        node_ids.append(node_id)

        # Get node data and mask
        node_data = x[batch_idx, node_idx, :, 0].cpu().numpy()
        node_mask = x_mask[batch_idx, node_idx, :, 0].cpu().numpy()

        # Apply mask (replace missing values with NaN)
        masked_data = np.where(node_mask > 0, node_data, np.nan)
        all_node_data.append(masked_data)

    # Convert to DataFrame
    df = pd.DataFrame(
        np.array(all_node_data).T, columns=[f"Node {node_id}" for node_id in node_ids]
    )

    # Compute correlation matrix (using pairwise complete observations)
    corr_matrix = df.corr(method="pearson", min_periods=3)

    return corr_matrix, node_ids




================================================
File: eda/__init__.py
================================================



================================================
File: eda/__main__.py
================================================
# Create a multi-page dashboard using HTML and Plotly
import os
from pathlib import Path
from datetime import datetime
from plotly.io import to_html
import plotly.io as pio

from .components import (
    interactive_data_availability,
    create_calendar_heatmap,
    create_completeness_trend,
    create_sensors_comparison,
    create_sensor_clustering,
    interactive_sensor_windows,
    create_monthly_coverage_matrix,
    visualize_daily_patterns,
    create_sensors_map,
    create_time_of_day_profiles,
    interactive_window_counts,
)

from .utils import load_data, load_template, render_template, get_template_path

from gnn_package.config import ExperimentConfig
from gnn_package import paths


def compute_completeness(time_series_dict):
    """
    Compute data completeness for each sensor.

    Parameters:
    -----------
    time_series_dict : dict
        Dictionary mapping sensor IDs to time series data

    Returns:
    --------
    dict
        Dictionary mapping sensor IDs to completeness percentage (0-1)
    """
    try:
        config = ExperimentConfig(
            "/Users/administrator/Code/python/phd-project-gnn/config.yml"
        )
        start_date = datetime.strptime(config.data.start_date, "%Y-%m-%d %H:%M:%S")
        end_date = datetime.strptime(config.data.end_date, "%Y-%m-%d %H:%M:%S")
    except Exception as e:
        print(f"Warning: Could not load config file, using default dates: {e}")
        # Fallback to using min and max dates from the data
        all_dates = []
        for series in time_series_dict.values():
            if len(series) > 0:
                all_dates.extend(series.index.tolist())
        if all_dates:
            start_date = min(all_dates)
            end_date = max(all_dates)
        else:
            raise ValueError("No data available to compute completeness")

    days_between = (end_date - start_date).total_seconds() / (60 * 60 * 24)
    expected_records = days_between * 24 * 4  # 15-minute intervals = 4 per hour
    print(f"Total days between start and end date: {days_between}")

    completeness_dict = {}
    for sensor_id, series in time_series_dict.items():
        # Remove duplicates to ensure accurate count
        series = series[~series.index.duplicated(keep="first")]
        completeness_dict[sensor_id] = len(series) / expected_records

    return completeness_dict


# Create a comprehensive dashboard combining multiple visualizations
def create_sensor_window_dashboard(data_file, window_size=24):
    """Create a comprehensive dashboard for analyzing sensor time windows"""
    # Load the data
    time_series_dict = load_data(data_file)

    # Identify sensors with most data points for individual analysis
    sensor_data_counts = {
        sensor_id: len(series) for sensor_id, series in time_series_dict.items()
    }
    top_sensor = max(sensor_data_counts, key=sensor_data_counts.get)

    # Set theme
    pio.templates.default = "plotly_white"

    # Compute data completeness
    completeness_dict = compute_completeness(time_series_dict)

    # Create individual visualizations
    # 1. Sensor map with completeness data
    sensor_map_fig = create_sensors_map(completeness_dict)

    # 2. Sensor availability and patterns
    data_avail_fig = interactive_data_availability(time_series_dict)
    top_sensor_fig = interactive_sensor_windows(
        time_series_dict, top_sensor, window_size
    )
    window_counts_fig = interactive_window_counts(time_series_dict, window_size)
    daily_patterns_fig = visualize_daily_patterns(time_series_dict, n_sensors=4)

    # 3. New visualizations
    try:
        # Calendar heatmap for top sensor
        calendar_heatmap_fig = create_calendar_heatmap(time_series_dict, top_sensor)
    except Exception as e:
        print(f"Could not create calendar heatmap: {e}")
        calendar_heatmap_fig = None

    try:
        # Time of day profiles
        time_of_day_fig = create_time_of_day_profiles(time_series_dict)
    except Exception as e:
        print(f"Could not create time of day profiles: {e}")
        time_of_day_fig = None

    try:
        # Completeness trend
        completeness_trend_fig = create_completeness_trend(time_series_dict)
    except Exception as e:
        print(f"Could not create completeness trend: {e}")
        completeness_trend_fig = None

    try:
        # Top sensors comparison
        sensors_comparison_fig = create_sensors_comparison(time_series_dict)
    except Exception as e:
        print(f"Could not create sensors comparison: {e}")
        sensors_comparison_fig = None

    try:
        # Monthly coverage matrix
        coverage_matrix_fig = create_monthly_coverage_matrix(time_series_dict)
    except Exception as e:
        print(f"Could not create coverage matrix: {e}")
        coverage_matrix_fig = None

    try:
        # Sensor clustering
        sensor_clustering_fig = create_sensor_clustering(time_series_dict)
    except Exception as e:
        print(f"Could not create sensor clustering: {e}")
        sensor_clustering_fig = None

    # Load the template
    template_path = get_template_path("dashboard_template.html")
    template = load_template(template_path)

    # Prepare context with all variables for the template
    context = {
        "window_size": window_size,
        "top_sensor": top_sensor,
        "sensor_map_fig": to_html(
            sensor_map_fig, include_plotlyjs="cdn", full_html=False
        ),
        "data_avail_fig": to_html(
            data_avail_fig, include_plotlyjs="cdn", full_html=False
        ),
        "top_sensor_fig": to_html(
            top_sensor_fig, include_plotlyjs="cdn", full_html=False
        ),
        "window_counts_fig": to_html(
            window_counts_fig, include_plotlyjs="cdn", full_html=False
        ),
        "daily_patterns_fig": to_html(
            daily_patterns_fig, include_plotlyjs="cdn", full_html=False
        ),
        "calendar_heatmap_fig": (
            to_html(calendar_heatmap_fig, include_plotlyjs="cdn", full_html=False)
            if calendar_heatmap_fig
            else None
        ),
        "time_of_day_fig": (
            to_html(time_of_day_fig, include_plotlyjs="cdn", full_html=False)
            if time_of_day_fig
            else None
        ),
        "completeness_trend_fig": (
            to_html(completeness_trend_fig, include_plotlyjs="cdn", full_html=False)
            if completeness_trend_fig
            else None
        ),
        "sensors_comparison_fig": (
            to_html(sensors_comparison_fig, include_plotlyjs="cdn", full_html=False)
            if sensors_comparison_fig
            else None
        ),
        "coverage_matrix_fig": (
            to_html(coverage_matrix_fig, include_plotlyjs="cdn", full_html=False)
            if coverage_matrix_fig
            else None
        ),
        "sensor_clustering_fig": (
            to_html(sensor_clustering_fig, include_plotlyjs="cdn", full_html=False)
            if sensor_clustering_fig
            else None
        ),
    }

    # Render the template
    html_content = render_template(template, context)

    # Return the HTML content
    return html_content


if __name__ == "__main__":
    # Create the dashboard
    data_file = os.path.join(paths.RAW_TIMESERIES_DIR, "test_data_1mnth.pkl")
    dashboard_html = create_sensor_window_dashboard(data_file, window_size=24)

    # Save to a file
    output_path = Path(__file__).parent / "index.html"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(dashboard_html)

    print(f"Dashboard created: {output_path}")




================================================
File: eda/components/__init__.py
================================================
from .calendar_heatmap import create_calendar_heatmap
from .completeness_trend import create_completeness_trend
from .counts_bar import interactive_window_counts
from .daily_patterns import visualize_daily_patterns
from .heatmap import interactive_data_availability
from .monthly_data_coverage import create_monthly_coverage_matrix
from .sensor_clustering import create_sensor_clustering
from .sensor_map import create_sensors_map
from .traffic_comparison import create_sensors_comparison
from .traffic_profile import create_time_of_day_profiles
from .window_segments import interactive_sensor_windows

__all__ = [
    "create_calendar_heatmap",
    "create_completeness_trend",
    "interactive_window_counts",
    "visualize_daily_patterns",
    "interactive_data_availability",
    "create_sensors_comparison",
    "create_sensor_clustering",
    "create_sensors_map",
    "create_sensors_comparison",
    "create_time_of_day_profiles",
    "interactive_sensor_windows",
]



================================================
File: eda/components/calendar_heatmap.py
================================================
import plotly.graph_objects as go
import pandas as pd
import numpy as np
import calendar
from datetime import datetime


def create_calendar_heatmap(time_series_dict, sensor_id=None):
    """
    Create a calendar heatmap showing data patterns by day of week and hour.

    Parameters:
    -----------
    time_series_dict : dict
        Dictionary mapping sensor IDs to their time series data
    sensor_id : str, optional
        Specific sensor to visualize. If None, uses the sensor with most data.

    Returns:
    --------
    plotly.graph_objects.Figure
        Calendar heatmap figure
    """
    # If no sensor_id provided, use the one with most data points
    if sensor_id is None:
        sensor_data_counts = {
            sensor_id: len(series) for sensor_id, series in time_series_dict.items()
        }
        sensor_id = max(sensor_data_counts, key=sensor_data_counts.get)

    # Get the time series for this sensor
    series = time_series_dict.get(sensor_id)
    if series is None or len(series) == 0:
        return go.Figure().update_layout(
            title=f"No data available for sensor {sensor_id}"
        )

    # Extract hour and day of week
    df = pd.DataFrame(
        {
            "value": series.values,
            "hour": series.index.hour,
            "day_of_week": series.index.dayofweek,
            "date": series.index.date,
        }
    )

    # Group by hour and day of week, and calculate mean value
    pivot_data = df.pivot_table(
        values="value", index="day_of_week", columns="hour", aggfunc="mean"
    ).fillna(0)

    # Days in order (0=Monday in pandas)
    days = [
        "Monday",
        "Tuesday",
        "Wednesday",
        "Thursday",
        "Friday",
        "Saturday",
        "Sunday",
    ]

    # Create heatmap
    fig = go.Figure(
        data=go.Heatmap(
            z=pivot_data.values,
            x=list(range(24)),  # 24 hours
            y=days,
            colorscale="YlOrRd",
            zmin=0,
            zmax=max(1, pivot_data.values.max()),  # Ensure non-zero upper bound
            hoverongaps=False,
            colorbar=dict(title="Avg Traffic Count", titleside="right"),
            hovertemplate="Hour: %{x}<br>Day: %{y}<br>Avg Value: %{z:.1f}<extra></extra>",
        )
    )

    # Calculate data coverage percentage
    total_slots = 24 * 7
    filled_slots = np.count_nonzero(pivot_data.values)
    coverage_pct = filled_slots / total_slots * 100

    # Update layout
    fig.update_layout(
        title=f"Traffic Patterns by Day & Hour - Sensor {sensor_id} (Data Coverage: {coverage_pct:.1f}%)",
        xaxis_title="Hour of Day",
        yaxis_title="Day of Week",
        xaxis=dict(
            tickmode="array",
            tickvals=list(range(24)),
            ticktext=[f"{h:02d}:00" for h in range(24)],
        ),
        height=500,
        margin=dict(l=50, r=50, t=80, b=50),
    )

    return fig



================================================
File: eda/components/completeness_trend.py
================================================
import plotly.graph_objects as go
import pandas as pd
import numpy as np
from datetime import datetime, timedelta


def create_completeness_trend(time_series_dict):
    """
    Create a visualization showing how data completeness trends over time.

    Parameters:
    -----------
    time_series_dict : dict
        Dictionary mapping sensor IDs to their time series data

    Returns:
    --------
    plotly.graph_objects.Figure
        Completeness trend figure
    """
    # Get the time range from all sensors
    all_timestamps = set()
    for sensor_id, series in time_series_dict.items():
        all_timestamps.update(series.index)

    if not all_timestamps:
        return go.Figure().update_layout(title="No data available")

    # Get the min and max date to establish the full range
    min_date = min(all_timestamps).date()
    max_date = max(all_timestamps).date()

    # Calculate expected readings per day (assuming 15-min intervals = 96 per day)
    expected_per_day = 96

    # Create date range
    date_range = pd.date_range(min_date, max_date, freq="D")

    # Initialize a DataFrame to store completeness by date
    completeness_df = pd.DataFrame(index=date_range)

    # Process each sensor
    for sensor_id, series in time_series_dict.items():
        # Count readings per day
        daily_counts = series.groupby(series.index.date).size()

        # Calculate completeness percentage
        completeness = daily_counts / expected_per_day * 100

        # Add to the DataFrame
        completeness_df[sensor_id] = completeness

    # Calculate overall completeness (average across all sensors)
    completeness_df["overall_avg"] = completeness_df.mean(axis=1)

    # Calculate the 7-day rolling average for smoothing
    completeness_df["rolling_avg"] = (
        completeness_df["overall_avg"].rolling(window=7, min_periods=1).mean()
    )

    # Create the figure
    fig = go.Figure()

    # Add the individual sensor completeness as light traces
    for sensor_id in time_series_dict.keys():
        fig.add_trace(
            go.Scatter(
                x=completeness_df.index,
                y=completeness_df[sensor_id],
                mode="lines",
                line=dict(width=0.5, color="rgba(180,180,180,0.3)"),
                name=sensor_id,
                showlegend=False,
            )
        )

    # Add the overall average
    fig.add_trace(
        go.Scatter(
            x=completeness_df.index,
            y=completeness_df["overall_avg"],
            mode="lines",
            line=dict(width=1, color="rgba(31, 119, 180, 0.8)"),
            name="Daily Average",
        )
    )

    # Add the rolling average
    fig.add_trace(
        go.Scatter(
            x=completeness_df.index,
            y=completeness_df["rolling_avg"],
            mode="lines",
            line=dict(width=3, color="rgb(31, 119, 180)"),
            name="7-Day Rolling Average",
        )
    )

    # Update layout
    fig.update_layout(
        title="Data Completeness Trend Over Time",
        xaxis_title="Date",
        yaxis_title="Completeness (%)",
        yaxis=dict(
            range=[0, 105], tickvals=[0, 25, 50, 75, 100]  # 0-100% with a little margin
        ),
        height=500,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        hovermode="x unified",
    )

    # Add a horizontal reference line at 100%
    fig.add_shape(
        type="line",
        x0=min_date,
        x1=max_date,
        y0=100,
        y1=100,
        line=dict(
            color="rgba(0,100,0,0.5)",
            width=1,
            dash="dash",
        ),
    )

    # Add annotation for 100% line
    fig.add_annotation(
        x=min_date + (max_date - min_date) * 0.02,  # Slightly offset from left edge
        y=100,
        text="100% Complete",
        showarrow=False,
        yshift=10,
        font=dict(size=10, color="rgba(0,100,0,0.7)"),
    )

    return fig



================================================
File: eda/components/counts_bar.py
================================================
import pandas as pd
import plotly.express as px
from dashboards.eda.utils import find_continuous_segments


# Create a window count bar chart
def interactive_window_counts(time_series_dict, window_size=24, n_sensors=20):
    """Create an interactive bar chart of window counts by sensor"""
    # Count windows per sensor
    window_counts = {}

    for sensor_id, series in time_series_dict.items():
        # Find segments without large gaps
        segments = find_continuous_segments(series.index, series.values)

        # Count windows
        total_windows = 0
        for start_seg, end_seg in segments:
            segment_len = end_seg - start_seg
            total_windows += max(0, segment_len - window_size + 1)

        window_counts[sensor_id] = total_windows

    # Sort by window count
    sorted_counts = sorted(window_counts.items(), key=lambda x: x[1], reverse=True)[
        :n_sensors
    ]

    # Create a DataFrame
    count_df = pd.DataFrame(sorted_counts, columns=["sensor_id", "window_count"])

    # Create a bar chart
    fig = px.bar(
        count_df,
        x="sensor_id",
        y="window_count",
        title=f"Number of Available Windows (size={window_size}) by Sensor",
        labels={"sensor_id": "Sensor ID", "window_count": "Number of Windows"},
        color="window_count",
        color_continuous_scale=px.colors.sequential.Viridis,
    )

    fig.update_layout(height=600, xaxis_tickangle=-45)

    return fig



================================================
File: eda/components/daily_patterns.py
================================================
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots


# Create a heatmap of daily patterns
def visualize_daily_patterns(time_series_dict, n_sensors=6):
    """Create a heatmap of daily patterns for top sensors"""
    # Identify sensors with most data points
    sensor_data_counts = {
        sensor_id: len(series) for sensor_id, series in time_series_dict.items()
    }
    top_sensors = sorted(sensor_data_counts, key=sensor_data_counts.get, reverse=True)[
        :n_sensors
    ]

    # Create subplots
    fig = make_subplots(
        rows=n_sensors,
        cols=1,
        subplot_titles=[
            f"Sensor {sensor_id} - Daily Pattern" for sensor_id in top_sensors
        ],
        vertical_spacing=0.08,
    )

    # Process each sensor
    for i, sensor_id in enumerate(top_sensors):
        series = time_series_dict[sensor_id]

        # Create a DataFrame with hour and day of week
        df = pd.DataFrame(
            {
                "value": series.values,
                "hour": series.index.hour,
                "day_of_week": series.index.dayofweek,
            }
        )

        # Group by hour and day of week
        pivot_data = df.pivot_table(
            values="value", index="day_of_week", columns="hour", aggfunc="mean"
        ).fillna(0)

        # Create heatmap
        heatmap = go.Heatmap(
            z=pivot_data.values,
            x=pivot_data.columns,
            y=["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"],
            colorscale="Viridis",
            showscale=(i == 0),  # Only show colorbar for first heatmap
        )

        fig.add_trace(heatmap, row=i + 1, col=1)

        # Update axes
        fig.update_xaxes(
            title_text="Hour of Day" if i == n_sensors - 1 else "", row=i + 1, col=1
        )
        fig.update_yaxes(title_text="Day of Week", row=i + 1, col=1)

    # Update layout
    fig.update_layout(
        height=250 * n_sensors, title_text="Daily Traffic Patterns Across Sensors"
    )

    return fig



================================================
File: eda/components/heatmap.py
================================================
import numpy as np
import pandas as pd
import plotly.express as px


# Create an interactive data availability heatmap
def interactive_data_availability(time_series_dict):
    """Create an interactive heatmap showing data availability across sensors over time"""
    # Get all unique timestamps from all sensors
    all_timestamps = set()
    for sensor_id, series in time_series_dict.items():
        all_timestamps.update(series.index)

    all_timestamps = sorted(all_timestamps)

    # Create a DataFrame with all timestamps and fill with NaN
    data_matrix = pd.DataFrame(index=all_timestamps)

    # For each sensor, add a column to the DataFrame
    for sensor_id, series in time_series_dict.items():
        data_matrix[sensor_id] = np.nan
        # Only fill in data that exists
        data_matrix.loc[series.index, sensor_id] = 1

    # Resample to a lower resolution for better visualization if too many datapoints
    if len(all_timestamps) > 1000:
        data_matrix = data_matrix.resample("1h").mean()

    # Convert to long format for plotly
    data_long = data_matrix.reset_index().melt(
        id_vars="index", var_name="sensor_id", value_name="has_data"
    )

    # Create the heatmap with plotly
    fig = px.density_heatmap(
        data_long,
        x="index",
        y="sensor_id",
        z="has_data",
        color_continuous_scale=[
            [0, "rgba(255,255,255,0)"],  # Transparent for NaN
            [0.5, "rgba(222,235,247,1)"],  # Light blue
            [1, "rgba(49,130,189,1)"],  # Dark blue
        ],
        title="Data Availability Across Sensors (Interactive)",
        labels={
            "index": "Date",
            "sensor_id": "Sensor ID",
            "has_data": "Data Available",
        },
    )

    # Update layout
    fig.update_layout(
        height=800,
        xaxis_title="Date",
        yaxis_title="Sensor ID",
        title_x=0.5,
        coloraxis_showscale=False,
    )

    return fig



================================================
File: eda/components/monthly_data_coverage.py
================================================
import plotly.graph_objects as go
import pandas as pd
import numpy as np
import calendar
from datetime import datetime


def create_monthly_coverage_matrix(time_series_dict, n_sensors=20):
    """
    Create a matrix showing monthly data coverage for top sensors.

    Parameters:
    -----------
    time_series_dict : dict
        Dictionary mapping sensor IDs to their time series data
    n_sensors : int
        Number of top sensors to include

    Returns:
    --------
    plotly.graph_objects.Figure
        Monthly coverage matrix figure
    """
    # Find sensors with most data points
    sensor_data_counts = {
        sensor_id: len(series) for sensor_id, series in time_series_dict.items()
    }
    top_sensors = sorted(sensor_data_counts, key=sensor_data_counts.get, reverse=True)[
        :n_sensors
    ]

    # Get the time range
    all_dates = []
    for series in time_series_dict.values():
        if len(series) > 0:
            all_dates.extend(series.index.tolist())

    if not all_dates:
        return go.Figure().update_layout(title="No data available")

    min_date = min(all_dates).date()
    max_date = max(all_dates).date()

    # Create month-year list
    unique_months = set()
    current_date = pd.Timestamp(min_date)
    while current_date <= pd.Timestamp(max_date):
        unique_months.add((current_date.year, current_date.month))
        current_date += pd.DateOffset(months=1)

    month_labels = [f"{calendar.month_name[m]} {y}" for y, m in sorted(unique_months)]
    month_keys = sorted(unique_months)

    # Expected readings per day and per month
    readings_per_day = 96  # 15-minute intervals

    # Initialize coverage matrix
    coverage_matrix = np.zeros((len(top_sensors), len(month_keys)))

    # Calculate coverage for each sensor and month
    for i, sensor_id in enumerate(top_sensors):
        series = time_series_dict[sensor_id]

        # Group by year and month
        ym_counts = series.groupby([series.index.year, series.index.month]).size()

        # Calculate days in each month-year for expected total readings
        for j, (year, month) in enumerate(month_keys):
            # Calculate days in this month
            if (year, month) in ym_counts.index:
                # Get number of days in this month
                days_in_month = calendar.monthrange(year, month)[1]
                expected_readings = days_in_month * readings_per_day

                # Calculate coverage percentage
                coverage_matrix[i, j] = min(
                    100, ym_counts[(year, month)] / expected_readings * 100
                )

    # Create heatmap
    fig = go.Figure(
        data=go.Heatmap(
            z=coverage_matrix,
            x=month_labels,
            y=[f"Sensor {sensor_id}" for sensor_id in top_sensors],
            colorscale=[
                [0, "rgb(247, 247, 247)"],  # Very light gray for 0%
                [0.2, "rgb(224, 224, 255)"],  # Very light blue for 20%
                [0.5, "rgb(150, 150, 255)"],  # Light blue for 50%
                [0.8, "rgb(67, 67, 255)"],  # Medium blue for 80%
                [1, "rgb(0, 0, 180)"],  # Dark blue for 100%
            ],
            zmin=0,
            zmax=100,
            colorbar=dict(
                title="Coverage %",
                tickvals=[0, 25, 50, 75, 100],
                ticktext=["0%", "25%", "50%", "75%", "100%"],
            ),
            hovertemplate="Sensor: %{y}<br>Month: %{x}<br>Coverage: %{z:.1f}%<extra></extra>",
        )
    )

    # Update layout
    fig.update_layout(
        title="Monthly Data Coverage Matrix (Top Sensors)",
        xaxis_title="Month",
        yaxis_title="Sensor",
        height=600,
        margin=dict(l=150),  # More space for sensor labels
        xaxis=dict(tickangle=45, type="category"),
    )

    return fig



================================================
File: eda/components/sensor_clustering.py
================================================
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA


def create_sensor_clustering(time_series_dict, n_clusters=4):
    """
    Create a visualization of sensors clustered by their traffic patterns.

    Parameters:
    -----------
    time_series_dict : dict
        Dictionary mapping sensor IDs to their time series data
    n_clusters : int
        Number of clusters to identify

    Returns:
    --------
    plotly.graph_objects.Figure
        Sensor clustering figure
    """
    # First, create feature vectors for each sensor
    features = []
    sensor_ids = []

    for sensor_id, series in time_series_dict.items():
        if len(series) < 24:  # Skip sensors with very little data
            continue

        # Create a DataFrame with time components
        df = pd.DataFrame(
            {
                "value": series.values,
                "hour": series.index.hour,
                "day_of_week": series.index.dayofweek,
            }
        )

        # Create hourly profile (average by hour of day)
        hourly_profile = (
            df.groupby("hour")["value"].mean().reindex(range(24)).fillna(0).values
        )

        # Create day of week profile
        dow_profile = (
            df.groupby("day_of_week")["value"].mean().reindex(range(7)).fillna(0).values
        )

        # Create weekend vs weekday ratio feature
        weekday_avg = df[df["day_of_week"] < 5]["value"].mean() or 0
        weekend_avg = df[df["day_of_week"] >= 5]["value"].mean() or 0
        weekend_ratio = weekend_avg / weekday_avg if weekday_avg > 0 else 0

        # Create morning vs evening ratio
        morning = df[(df["hour"] >= 6) & (df["hour"] < 12)]["value"].mean() or 0
        evening = df[(df["hour"] >= 16) & (df["hour"] < 22)]["value"].mean() or 0
        ampm_ratio = morning / evening if evening > 0 else 0

        # Combine all features
        feature_vector = np.concatenate(
            [
                hourly_profile,  # 24 features
                dow_profile,  # 7 features
                [weekend_ratio, ampm_ratio],  # 2 additional features
            ]
        )

        features.append(feature_vector)
        sensor_ids.append(sensor_id)

    if len(features) < n_clusters:
        return go.Figure().update_layout(
            title=f"Not enough data for clustering. Need at least {n_clusters} sensors with sufficient data."
        )

    # Convert to numpy array
    X = np.array(features)

    # Normalize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Apply K-means clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(X_scaled)

    # Apply PCA for visualization
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)

    # Create a DataFrame for plotting
    plot_df = pd.DataFrame(
        {
            "sensor_id": sensor_ids,
            "cluster": clusters.astype(str),
            "pc1": X_pca[:, 0],
            "pc2": X_pca[:, 1],
        }
    )

    # Create scatter plot of clusters
    fig = px.scatter(
        plot_df,
        x="pc1",
        y="pc2",
        color="cluster",
        hover_name="sensor_id",
        title=f"Sensor Clustering Based on Traffic Patterns (k={n_clusters})",
        labels={
            "pc1": f"Principal Component 1 ({pca.explained_variance_ratio_[0]:.1%} variance)",
            "pc2": f"Principal Component 2 ({pca.explained_variance_ratio_[1]:.1%} variance)",
            "cluster": "Cluster",
        },
    )

    # Add cluster centroids
    centroids_pca = pca.transform(kmeans.cluster_centers_)

    fig.add_trace(
        go.Scatter(
            x=centroids_pca[:, 0],
            y=centroids_pca[:, 1],
            mode="markers",
            marker=dict(symbol="x", size=12, color="black", line=dict(width=2)),
            name="Cluster Centroids",
            hoverinfo="skip",
        )
    )

    # Add annotations for cluster numbers
    for i, (x, y) in enumerate(centroids_pca):
        fig.add_annotation(
            x=x,
            y=y,
            text=f"{i}",
            showarrow=False,
            yshift=15,
            font=dict(size=14, color="black"),
        )

    # Update layout
    fig.update_layout(height=600, legend_title="Cluster", hovermode="closest")

    return fig



================================================
File: eda/components/sensor_map.py
================================================
import plotly.express as px
import pandas as pd
import numpy as np
from private_uoapi import LightsailWrapper, LSAuth, LSConfig
import geopandas as gpd
from gnn_package.src.utils.sensor_utils import get_sensor_name_id_map


def create_sensors_map(completeness_dict):
    """
    Create an interactive map of sensor locations colored by data completeness.
    Includes bounds to prevent excessive panning and ensures the map loads properly.
    Fixed zoom level for better visibility of Newcastle upon Tyne area.

    Parameters:
    -----------
    completeness_dict : dict
        Dictionary mapping sensor IDs to their completeness percentage (0-1)

    Returns:
    --------
    plotly.graph_objects.Figure
        Interactive map with sensors
    """
    try:
        # Try to read the shapefile
        shapefile_path = "/Users/administrator/Code/python/phd-project-gnn/gnn_package/data/sensors/sensors.shp"
        gdf = gpd.read_file(shapefile_path)

        # Check the CRS - crucial step!
        print(f"Original GeoDataFrame CRS: {gdf.crs}")

        # If the data is in UK National Grid (EPSG:27700), convert to WGS84 (EPSG:4326)
        if gdf.crs == "EPSG:27700" or str(gdf.crs).find("27700") >= 0:
            print("Converting from EPSG:27700 (UK National Grid) to EPSG:4326 (WGS84)")
            gdf = gdf.to_crs("EPSG:4326")

        # Extract coordinates to a regular pandas DataFrame without geometry objects
        lat_values = []
        lon_values = []
        for point in gdf["geometry"]:
            # After conversion, y should be latitude and x should be longitude
            lat_values.append(float(point.y))
            lon_values.append(float(point.x))

        # Verify coordinates are in reasonable lat/lon range
        print(
            f"Coordinate range - Lat: {min(lat_values)} to {max(lat_values)}, Lon: {min(lon_values)} to {max(lon_values)}"
        )

        # Create a clean DataFrame without geometry objects
        df = pd.DataFrame(
            {"location": gdf["location"].tolist(), "lat": lat_values, "lon": lon_values}
        )

    except Exception as e:
        print(f"Error reading or processing shapefile: {e}")
        print("Attempting to fetch data from API instead...")

        # Fallback to using the API
        config = LSConfig()
        auth = LSAuth(config)
        client = LightsailWrapper(config, auth)
        locations = client.get_traffic_sensors()

        # Create a regular DataFrame (not GeoDataFrame)
        df = pd.DataFrame(locations)

    # Calculate center and bounds with appropriate lat/lon coordinates
    # If we have lat/lon values in df, use them
    if "lat" in df.columns and "lon" in df.columns and len(df) > 0:
        center_lat = np.mean(df["lat"])
        center_lon = np.mean(df["lon"])

        # Calculate appropriate zoom level based on the spread of data
        lat_range = max(df["lat"]) - min(df["lat"])
        lon_range = max(df["lon"]) - min(df["lon"])

        # Adjust zoom based on geographic spread (smaller range = higher zoom)
        # These values are calibrated for the Newcastle area
        if max(lat_range, lon_range) < 0.05:
            zoom_level = 13  # Very localized data
        elif max(lat_range, lon_range) < 0.1:
            zoom_level = 12  # Small area
        elif max(lat_range, lon_range) < 0.3:
            zoom_level = 11  # Medium area (typical for Newcastle)
        else:
            zoom_level = 10  # Larger area

        print(
            f"Auto-calculated zoom level: {zoom_level} based on data spread: {lat_range},{lon_range}"
        )
    else:
        # Fallback values for Newcastle upon Tyne
        center_lat = 54.97
        center_lon = -1.61
        zoom_level = 11

    # Set appropriate bounds
    lat_padding = 0.01  # Reduced padding for tighter view
    lon_padding = 0.01

    if "lat" in df.columns and "lon" in df.columns and len(df) > 0:
        min_lat = max(min(df["lat"]) - lat_padding, -90)
        max_lat = min(max(df["lat"]) + lat_padding, 90)
        min_lon = max(min(df["lon"]) - lon_padding, -180)
        max_lon = min(max(df["lon"]) + lon_padding, 180)
    else:
        # Fallback bounds for Newcastle
        min_lat = 54.93
        max_lat = 55.02
        min_lon = -1.65
        max_lon = -1.55

    # Get mapping between sensor names and IDs
    name_id_map = get_sensor_name_id_map()

    # Add completeness data
    df["sensor_id"] = df["location"].map(name_id_map)
    df["completeness"] = df["sensor_id"].map(lambda x: completeness_dict.get(x, 0))
    df["completeness_pct"] = df["completeness"] * 100

    # Debug info
    print(f"Map center: {center_lat}, {center_lon}")
    print(f"Map bounds: {min_lat}, {min_lon}, {max_lat}, {max_lon}")
    print(f"Number of sensors to display: {len(df)}")
    print(f"Using zoom level: {zoom_level}")

    # Define a custom color scale: red (low) -> yellow (medium) -> green (high)
    # This creates a more intuitive color scale for completeness
    custom_colorscale = [
        [0.0, "rgba(178, 24, 43, 1)"],  # Dark red for very low completeness
        [0.25, "rgba(239, 138, 98, 1)"],  # Light red/orange for low completeness
        [0.5, "rgba(253, 219, 121, 1)"],  # Yellow for medium completeness
        [0.75, "rgba(173, 221, 142, 1)"],  # Light green for good completeness
        [1.0, "rgba(49, 163, 84, 1)"],  # Dark green for excellent completeness
    ]

    # Create the map - note the adjusted size_max and explicit zoom level
    fig = px.scatter_mapbox(
        df,
        lat="lat",
        lon="lon",
        color="completeness_pct",
        size="completeness_pct",
        color_continuous_scale=custom_colorscale,
        range_color=[0, 100],
        size_max=30,  # Smaller markers
        hover_name="location",
        hover_data={
            "completeness_pct": ":.1f",
            "lat": False,
            "lon": False,
            "sensor_id": True,
        },
        title="Sensor Locations and Data Completeness",
        labels={"completeness_pct": "Data Completeness (%)"},
    )

    fig.update_traces(
        marker=dict(opacity=0.8),
        selector=dict(mode="markers"),
    )

    # Update layout with explicit zoom level
    fig.update_layout(
        height=700,
        margin={"r": 0, "t": 30, "l": 0, "b": 0},
        coloraxis_colorbar=dict(
            title="Completeness (%)",
            ticksuffix="%",
            tickvals=[0, 25, 50, 75, 100],
            ticktext=["0%", "25%", "50%", "75%", "100%"],
        ),
        mapbox=dict(
            center=dict(lat=center_lat, lon=center_lon),
            style="carto-positron",
            zoom=zoom_level,  # Explicitly set zoom level
            bounds=dict(west=min_lon, east=max_lon, south=min_lat, north=max_lat),
        ),
    )

    return fig



================================================
File: eda/components/traffic_comparison.py
================================================
import plotly.graph_objects as go
import pandas as pd
import numpy as np
from datetime import datetime, timedelta


def create_sensors_comparison(time_series_dict, n_sensors=5, days_back=30):
    """
    Create an interactive comparison of top sensors' traffic patterns.

    Parameters:
    -----------
    time_series_dict : dict
        Dictionary mapping sensor IDs to their time series data
    n_sensors : int
        Number of top sensors to compare
    days_back : int
        Number of days to look back for the comparison

    Returns:
    --------
    plotly.graph_objects.Figure
        Sensors comparison figure
    """
    # Find sensors with most data points
    sensor_data_counts = {
        sensor_id: len(series) for sensor_id, series in time_series_dict.items()
    }
    top_sensors = sorted(sensor_data_counts, key=sensor_data_counts.get, reverse=True)[
        :n_sensors
    ]

    # Get the current date (or max date in the data)
    all_dates = []
    for series in time_series_dict.values():
        if len(series) > 0:
            all_dates.extend(series.index.tolist())

    if not all_dates:
        return go.Figure().update_layout(title="No data available")

    current_date = max(all_dates).date()
    start_date = current_date - timedelta(days=days_back)

    # Create figure
    fig = go.Figure()

    # Add a trace for each sensor
    for sensor_id in top_sensors:
        series = time_series_dict[sensor_id]

        # Filter to the selected date range
        mask = (series.index.date >= start_date) & (series.index.date <= current_date)
        filtered_series = series[mask]

        # Skip if no data in the range
        if len(filtered_series) == 0:
            continue

        # Resample to hourly data for smoother visualization
        hourly_data = filtered_series.resample("1H").mean()

        # Add trace
        fig.add_trace(
            go.Scatter(
                x=hourly_data.index,
                y=hourly_data.values,
                mode="lines",
                name=f"Sensor {sensor_id}",
                hovertemplate="%{x}<br>Value: %{y:.1f}<extra>Sensor "
                + sensor_id
                + "</extra>",
            )
        )

    # Update layout
    fig.update_layout(
        title=f"Top {n_sensors} Sensors: Traffic Comparison (Last {days_back} Days)",
        xaxis_title="Date",
        yaxis_title="Traffic Count",
        height=600,
        hovermode="closest",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
    )

    # Add range slider
    fig.update_xaxes(
        rangeslider_visible=True,
        rangeselector=dict(
            buttons=list(
                [
                    dict(count=1, label="1d", step="day", stepmode="backward"),
                    dict(count=7, label="1w", step="day", stepmode="backward"),
                    dict(count=1, label="1m", step="month", stepmode="backward"),
                    dict(step="all"),
                ]
            )
        ),
    )

    return fig



================================================
File: eda/components/traffic_profile.py
================================================
import plotly.graph_objects as go
import pandas as pd
import numpy as np
from plotly.subplots import make_subplots


def create_time_of_day_profiles(time_series_dict, top_n=5):
    """
    Create profiles of traffic patterns by time of day, comparing weekday vs weekend.

    Parameters:
    -----------
    time_series_dict : dict
        Dictionary mapping sensor IDs to their time series data
    top_n : int
        Number of top sensors to include (with most data points)

    Returns:
    --------
    plotly.graph_objects.Figure
        Time of day profiles figure
    """
    # Find sensors with most data points
    sensor_data_counts = {
        sensor_id: len(series) for sensor_id, series in time_series_dict.items()
    }
    top_sensors = sorted(sensor_data_counts, key=sensor_data_counts.get, reverse=True)[
        :top_n
    ]

    # Create figure with subplots - one row per sensor
    fig = make_subplots(
        rows=len(top_sensors),
        cols=1,
        shared_xaxes=True,
        subplot_titles=[f"Sensor {sensor_id}" for sensor_id in top_sensors],
        vertical_spacing=0.05,
    )

    # Colors for weekday vs weekend
    colors = {"Weekday": "rgb(31, 119, 180)", "Weekend": "rgb(255, 127, 14)"}

    for i, sensor_id in enumerate(top_sensors):
        # Get the time series for this sensor
        series = time_series_dict[sensor_id]

        # Create DataFrame with time components
        df = pd.DataFrame(
            {
                "value": series.values,
                "hour": series.index.hour,
                "is_weekend": series.index.dayofweek >= 5,  # 5=Sat, 6=Sun
            }
        )

        # Group by hour and weekday/weekend
        weekday_data = df[~df["is_weekend"]].groupby("hour")["value"].mean()
        weekend_data = df[df["is_weekend"]].groupby("hour")["value"].mean()

        # Add weekday line
        fig.add_trace(
            go.Scatter(
                x=list(range(24)),
                y=weekday_data.reindex(range(24)).fillna(0).values,
                mode="lines+markers",
                name="Weekday" if i == 0 else None,  # Only show in legend once
                line=dict(color=colors["Weekday"], width=2),
                marker=dict(size=6),
                showlegend=(i == 0),
                legendgroup="Weekday",
            ),
            row=i + 1,
            col=1,
        )

        # Add weekend line
        fig.add_trace(
            go.Scatter(
                x=list(range(24)),
                y=weekend_data.reindex(range(24)).fillna(0).values,
                mode="lines+markers",
                name="Weekend" if i == 0 else None,  # Only show in legend once
                line=dict(color=colors["Weekend"], width=2, dash="dot"),
                marker=dict(size=6),
                showlegend=(i == 0),
                legendgroup="Weekend",
            ),
            row=i + 1,
            col=1,
        )

    # Update layout
    fig.update_layout(
        height=250 * len(top_sensors),
        title_text="Time of Day Traffic Profiles: Weekday vs Weekend",
        xaxis_title="Hour of Day",
        yaxis_title="Average Traffic Count",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5),
    )

    # Update all x-axes to show hours in 24-hour format
    for i in range(len(top_sensors)):
        fig.update_xaxes(
            tickmode="array",
            tickvals=list(range(0, 24, 2)),  # Show every 2 hours
            ticktext=[f"{h:02d}:00" for h in range(0, 24, 2)],
            row=i + 1,
            col=1,
        )

        # Add y-axis title only to the first subplot
        if i == 0:
            fig.update_yaxes(title_text="Avg Traffic Count", row=i + 1, col=1)

    return fig



================================================
File: eda/components/window_segments.py
================================================
import plotly.graph_objects as go
from dashboards.eda.utils.data_utils import find_continuous_segments


# Create interactive window visualization for a given sensor
def interactive_sensor_windows(time_series_dict, sensor_id, window_size=24, stride=1):
    """Create an interactive visualization of windows for a specific sensor"""
    series = time_series_dict.get(sensor_id)
    if series is None or len(series) == 0:
        return None

    # Find continuous segments
    segments = find_continuous_segments(series.index, series.values)

    # Create a figure
    fig = go.Figure()

    # Add the raw time series
    fig.add_trace(
        go.Scatter(
            x=series.index,
            y=series.values,
            mode="lines",
            name="Raw Data",
            line=dict(color="darkgray"),
        )
    )

    # Add segments and windows
    for start_seg, end_seg in segments:
        segment_indices = series.index[start_seg:end_seg]

        # Add segment highlight
        fig.add_trace(
            go.Scatter(
                x=[
                    segment_indices[0],
                    segment_indices[0],
                    segment_indices[-1],
                    segment_indices[-1],
                ],
                y=[
                    series.values.min(),
                    series.values.max(),
                    series.values.max(),
                    series.values.min(),
                ],
                fill="toself",
                mode="none",
                name=f"Segment: {segment_indices[0].date()} to {segment_indices[-1].date()}",
                fillcolor="rgba(144,238,144,0.2)",
                showlegend=True,
            )
        )

        # Add a few example windows
        n_windows = len(segment_indices) - window_size + 1

        # Only show a few windows to avoid overcrowding
        window_step = max(1, n_windows // 5)

        for i in range(0, n_windows, window_step):
            window_start = segment_indices[i]
            window_end = segment_indices[i + window_size - 1]

            fig.add_trace(
                go.Scatter(
                    x=[window_start, window_start, window_end, window_end],
                    y=[
                        series.values.min(),
                        series.values.max(),
                        series.values.max(),
                        series.values.min(),
                    ],
                    fill="toself",
                    mode="none",
                    name=f"Window: {window_start}",
                    fillcolor="rgba(0,0,255,0.1)",
                    showlegend=False,
                )
            )

    # Update layout
    fig.update_layout(
        title=f"Time Windows for Sensor {sensor_id}",
        xaxis_title="Date",
        yaxis_title="Traffic Count",
        height=800,
        legend=dict(orientation="h", yanchor="bottom", y=-0.4, xanchor="right", x=1),
    )

    return fig




================================================
File: eda/templates/__init__.py
================================================
# Templates package initialization
# This file is used to mark the directory as a Python package

from pathlib import Path


def get_template_path(template_name):
    """
    Get the full path to a template file

    Parameters:
    -----------
    template_name : str
        The name of the template file

    Returns:
    --------
    Path
        Full path to the template file
    """
    return Path(__file__).parent / template_name



================================================
File: eda/templates/dashboard_template.html
================================================
<!DOCTYPE html>
<html>

<head>
    <title>Sensor Analysis Dashboard</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f8f9fa;
            color: #343a40;
        }

        .dashboard-container {
            max-width: 1400px;
            margin: 0 auto;
            background-color: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.1);
        }

        .dashboard-header {
            background: linear-gradient(135deg, #4c78a8 0%, #345d8a 100%);
            color: white;
            padding: 25px;
            text-align: center;
        }

        .dashboard-section {
            padding: 25px;
            margin-bottom: 25px;
            border-bottom: 1px solid #e9ecef;
        }

        h1 {
            margin: 0;
            font-size: 2.2rem;
            font-weight: 600;
        }

        h2 {
            color: #2C3E50;
            margin-top: 0;
            font-size: 1.8rem;
            font-weight: 500;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid #eaeaea;
        }

        .dashboard-description {
            max-width: 800px;
            margin: 15px auto;
            font-size: 1.1rem;
            opacity: 0.9;
        }

        .viz-container {
            margin-top: 20px;
            border: 1px solid #eee;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.05);
        }

        .nav-tabs {
            display: flex;
            background-color: #f1f3f5;
            padding: 0;
            margin: 0;
            list-style: none;
            overflow-x: auto;
            white-space: nowrap;
            border-bottom: 1px solid #dee2e6;
        }

        .nav-tabs li {
            margin: 0;
        }

        .nav-tabs li a {
            display: inline-block;
            padding: 12px 20px;
            text-decoration: none;
            color: #495057;
            font-weight: 500;
            border-bottom: 3px solid transparent;
            transition: all 0.2s ease;
        }

        .nav-tabs li a:hover {
            background-color: #e9ecef;
            border-bottom: 3px solid #4c78a8;
            color: #4c78a8;
        }

        .nav-tabs li a.active {
            background-color: white;
            border-bottom: 3px solid #4c78a8;
            color: #4c78a8;
        }

        .tab-content {
            display: none;
            padding: 25px;
        }

        .tab-content.active {
            display: block;
        }

        .dashboard-footer {
            background-color: #343a40;
            color: white;
            text-align: center;
            padding: 15px;
            font-size: 0.9rem;
        }

        .dashboard-footer a {
            color: #9ec5fe;
            text-decoration: none;
        }

        .dashboard-footer a:hover {
            text-decoration: underline;
        }

        @media (max-width: 768px) {
            .dashboard-header {
                padding: 15px;
            }

            h1 {
                font-size: 1.8rem;
            }

            .dashboard-section {
                padding: 15px;
            }
        }
    </style>

    <!-- Include any necessary JavaScript for tab switching -->
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            // Tab switching functionality
            const tabLinks = document.querySelectorAll('.nav-tabs a');
            const tabContents = document.querySelectorAll('.tab-content');

            tabLinks.forEach(link => {
                link.addEventListener('click', function (e) {
                    e.preventDefault();

                    // Deactivate all tabs
                    tabLinks.forEach(l => l.classList.remove('active'));
                    tabContents.forEach(c => c.classList.remove('active'));

                    // Activate clicked tab
                    this.classList.add('active');
                    const targetTab = document.querySelector(this.getAttribute('href'));
                    if (targetTab) {
                        targetTab.classList.add('active');
                    }
                });
            });

            // Activate first tab by default
            if (tabLinks.length > 0) {
                tabLinks[0].click();
            }
        });
    </script>
</head>

<body>
    <div class="dashboard-container">
        <div class="dashboard-header">
            <h1>Traffic Sensor Analysis Dashboard</h1>
            <p class="dashboard-description">
                Interactive visualization and analysis of traffic sensor data with advanced analytics for pattern
                recognition and data quality assessment.
            </p>
        </div>

        <!-- Tab Navigation -->
        <ul class="nav-tabs">
            <li><a href="#overview-tab" class="active">Overview</a></li>
            <li><a href="#sensor-analysis-tab">Sensor Analysis</a></li>
            <li><a href="#data-quality-tab">Data Quality</a></li>
            <li><a href="#patterns-tab">Traffic Patterns</a></li>
            <li><a href="#advanced-tab">Advanced Analysis</a></li>
        </ul>

        <!-- Tab Content -->
        <!-- OVERVIEW TAB -->
        <div id="overview-tab" class="tab-content active">
            <div class="dashboard-section">
                <h2>Sensor Map</h2>
                <p>Geographic distribution of sensors colored by data completeness percentage.</p>
                <div class="viz-container">
                    {{sensor_map_fig}}
                </div>
            </div>

            <div class="dashboard-section">
                <h2>Data Availability</h2>
                <p>Heatmap showing when data is available across sensors over time.</p>
                <div class="viz-container">
                    {{data_avail_fig}}
                </div>
            </div>

            <div class="dashboard-section">
                <h2>Sensors Comparison</h2>
                <p>Interactive comparison of traffic patterns from top sensors.</p>
                <div class="viz-container">
                    {{sensors_comparison_fig}}
                </div>
            </div>
        </div>

        <!-- SENSOR ANALYSIS TAB -->
        <div id="sensor-analysis-tab" class="tab-content">
            <div class="dashboard-section">
                <h2>Top Sensor Analysis (ID: {{top_sensor}})</h2>
                <p>Detailed analysis of data from the sensor with the most readings.</p>
                <div class="viz-container">
                    {{top_sensor_fig}}
                </div>
            </div>

            <div class="dashboard-section">
                <h2>Calendar Heatmap</h2>
                <p>Traffic patterns by day of week and hour of day for the top sensor.</p>
                <div class="viz-container">
                    {{calendar_heatmap_fig}}
                </div>
            </div>

            <div class="dashboard-section">
                <h2>Window Count Distribution</h2>
                <p>Number of available time windows (size={{window_size}}) for each sensor.</p>
                <div class="viz-container">
                    {{window_counts_fig}}
                </div>
            </div>
        </div>

        <!-- DATA QUALITY TAB -->
        <div id="data-quality-tab" class="tab-content">
            <div class="dashboard-section">
                <h2>Completeness Trend</h2>
                <p>How data completeness changes over time across all sensors.</p>
                <div class="viz-container">
                    {{completeness_trend_fig}}
                </div>
            </div>

            <div class="dashboard-section">
                <h2>Monthly Coverage Matrix</h2>
                <p>Monthly data coverage percentage for top sensors.</p>
                <div class="viz-container">
                    {{coverage_matrix_fig}}
                </div>
            </div>
        </div>

        <!-- TRAFFIC PATTERNS TAB -->
        <div id="patterns-tab" class="tab-content">
            <div class="dashboard-section">
                <h2>Daily Traffic Patterns</h2>
                <p>Average traffic patterns by hour of day and day of week for top sensors.</p>
                <div class="viz-container">
                    {{daily_patterns_fig}}
                </div>
            </div>

            <div class="dashboard-section">
                <h2>Weekday vs. Weekend Profiles</h2>
                <p>Comparison of traffic patterns between weekdays and weekends.</p>
                <div class="viz-container">
                    {{time_of_day_fig}}
                </div>
            </div>
        </div>

        <!-- ADVANCED ANALYSIS TAB -->
        <div id="advanced-tab" class="tab-content">
            <div class="dashboard-section">
                <h2>Sensor Clustering</h2>
                <p>Clustering of sensors based on similar traffic patterns.</p>
                <div class="viz-container">
                    {{sensor_clustering_fig}}
                </div>
            </div>
        </div>

        <div class="dashboard-footer">
            <p>Traffic Sensor Analysis Dashboard | Created with Python and Plotly using data from <a href="https://newcastle.urbanobservatory.ac.uk/">Urban Observatory</a> | <a href="https://carrow.me.uk">Carrow Morris-Wiltshire</a> 2025</p>
        </div>
    </div>
</body>

</html>


================================================
File: eda/utils/__init__.py
================================================
from .data_utils import load_data, find_continuous_segments, load_sensor_geojson
from .template_utils import load_template, render_template, get_template_path

__all__ = [
    "load_data",
    "find_continuous_segments",
    "load_sensor_geojson",
    "load_template",
    "render_template",
    "get_template_path",
]



================================================
File: eda/utils/data_utils.py
================================================
import json
import pickle
import numpy as np
import pandas as pd


# Helper function to load data from file
def load_data(file_path):
    with open(file_path, "rb") as f:
        return pickle.load(f)


# Find continuous segments in time series
def find_continuous_segments(
    time_index, values, gap_threshold=pd.Timedelta(minutes=15)
):
    segments = []
    start_idx = 0

    for i in range(1, len(time_index)):
        time_diff = time_index[i] - time_index[i - 1]

        # Check for gaps in time or values
        if (time_diff > gap_threshold) or (
            np.isnan(values[i - 1]) or np.isnan(values[i])
        ):
            if i - start_idx >= 24:  # Assuming minimum window size of 24
                segments.append((start_idx, i))
            start_idx = i

    # Add the last segment if it's long enough
    if len(time_index) - start_idx >= 24:
        segments.append((start_idx, len(time_index)))

    return segments


# Load sensor location data from file
def load_sensor_geojson(file_path="dashboards/data/sensors.geojson"):
    with open(file_path, "r", encoding="utf8") as f:
        return json.load(f)



================================================
File: eda/utils/template_utils.py
================================================
"""
Template utilities for the dashboard application.
Provides functions for loading and rendering HTML templates.
"""

import re
from pathlib import Path


def load_template(template_path):
    """
    Load an HTML template from file

    Parameters:
    -----------
    template_path : str or Path
        Path to the template file

    Returns:
    --------
    str
        The template content as a string
    """
    with open(template_path, "r", encoding="utf-8") as file:
        return file.read()


def render_template(template, context):
    """
    Simple template rendering function that replaces placeholders with values

    Parameters:
    -----------
    template : str
        The template string with placeholders
    context : dict
        A dictionary of placeholder names and their values

    Returns:
    --------
    str
        The rendered template

    Notes:
    ------
    Supports:
    - Variable interpolation: {{variable}}
    - Conditional blocks: {{#variable}}content{{/variable}}
    - Comments: {{!comment}}
    """
    # Remove comments
    template = re.sub(r"{{!.*?}}", "", template, flags=re.DOTALL)

    # Replace all {{variable}} placeholders with their values
    for key, value in context.items():
        placeholder = f"{{{{{key}}}}}"
        replacement = str(value) if value is not None else ""
        # Simple string replacement instead of regex to avoid escape sequence issues
        template = template.replace(placeholder, replacement)

    # Handle conditional blocks {{#variable}} content {{/variable}}
    for key, value in context.items():
        start_tag = f"{{{{#{key}}}}}"
        end_tag = f"{{{{/{key}}}}}"

        # If the value exists and is truthy, remove just the conditional markers
        if value:
            # Find all occurrences of this conditional block
            start_pos = 0
            while True:
                start_idx = template.find(start_tag, start_pos)
                if start_idx == -1:
                    break

                end_idx = template.find(end_tag, start_idx)
                if end_idx == -1:
                    break

                # Extract the content between tags
                content = template[start_idx + len(start_tag) : end_idx]

                # Replace the entire block with just the content
                template = (
                    template[:start_idx] + content + template[end_idx + len(end_tag) :]
                )

                # Update start position for next iteration
                start_pos = start_idx + len(content)
        else:
            # If the value doesn't exist or is falsy, remove the entire block
            while True:
                start_idx = template.find(start_tag)
                if start_idx == -1:
                    break

                end_idx = template.find(end_tag)
                if end_idx == -1:
                    break

                # Remove the entire block including tags
                template = template[:start_idx] + template[end_idx + len(end_tag) :]

    # Clean up any remaining template tags (useful for optional content)
    while True:
        start_idx = template.find("{{")
        if start_idx == -1:
            break

        end_idx = template.find("}}", start_idx)
        if end_idx == -1:
            break

        template = template[:start_idx] + template[end_idx + 2 :]

    return template


def get_template_path(template_name, template_dir=None):
    """
    Get the full path to a template file

    Parameters:
    -----------
    template_name : str
        The name of the template file
    template_dir : str or Path, optional
        The directory containing templates. If None, uses the default 'templates' directory

    Returns:
    --------
    Path
        Full path to the template file
    """
    if template_dir is None:
        # Navigate to the templates directory relative to this file
        # Going up to utils directory, then up to the project root, then to templates
        template_dir = Path(__file__).parent.parent / "templates"
    else:
        template_dir = Path(template_dir)

    return template_dir / template_name



