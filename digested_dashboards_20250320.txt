Directory structure:
└── dashboards/
    ├── __init__.py
    ├── __main__.py
    ├── digester.sh
    ├── training_winodows.ipynb
    ├── components/
    │   ├── __init__.py
    │   ├── counts_bar.py
    │   ├── daily_patterns.py
    │   ├── heatmap.py
    │   ├── pca_plot.py
    │   ├── sensor_map.py
    │   ├── window_segments.py
    │   └── __pycache__/
    ├── data/
    ├── templates/
    │   ├── __init__.py
    │   └── dashboard_template.html
    └── utils/
        ├── __init__.py
        ├── data_utils.py
        ├── template_utils.py
        └── __pycache__/

================================================
File: __init__.py
================================================



================================================
File: __main__.py
================================================
# Create a multi-page dashboard using HTML and Plotly
from pathlib import Path
from datetime import datetime
from plotly.io import to_html
import plotly.io as pio

from .components import (
    interactive_data_availability,
    interactive_sensor_windows,
    interactive_window_counts,
    visualize_daily_patterns,
    visualize_window_pca,
    create_sensors_map,
)
from .utils import load_data, load_template, render_template, get_template_path
from gnn_package.config import ExperimentConfig


def compute_completeness(time_series_dict):
    """
    Compute data completeness for each sensor.

    Parameters:
    -----------
    time_series_dict : dict
        Dictionary mapping sensor IDs to time series data

    Returns:
    --------
    dict
        Dictionary mapping sensor IDs to completeness percentage (0-1)
    """
    try:
        config = ExperimentConfig(
            "/Users/administrator/Code/python/phd-project-gnn/config.yml"
        )
        start_date = datetime.strptime(config.data.start_date, "%Y-%m-%d %H:%M:%S")
        end_date = datetime.strptime(config.data.end_date, "%Y-%m-%d %H:%M:%S")
    except Exception as e:
        print(f"Warning: Could not load config file, using default dates: {e}")
        # Fallback to using min and max dates from the data
        all_dates = []
        for series in time_series_dict.values():
            if len(series) > 0:
                all_dates.extend(series.index.tolist())
        if all_dates:
            start_date = min(all_dates)
            end_date = max(all_dates)
        else:
            raise ValueError("No data available to compute completeness")

    days_between = (end_date - start_date).total_seconds() / (60 * 60 * 24)
    expected_records = days_between * 24 * 4  # 15-minute intervals = 4 per hour
    print(f"Total days between start and end date: {days_between}")

    completeness_dict = {}
    for sensor_id, series in time_series_dict.items():
        # Remove duplicates to ensure accurate count
        series = series[~series.index.duplicated(keep="first")]
        completeness_dict[sensor_id] = len(series) / expected_records

    return completeness_dict


# Create a comprehensive dashboard combining multiple visualizations
def create_sensor_window_dashboard(data_file="test_data_1yr.pkl", window_size=24):
    """Create a comprehensive dashboard for analyzing sensor time windows"""
    # Load the data
    time_series_dict = load_data(data_file)

    # Identify sensors with most data points for individual analysis
    sensor_data_counts = {
        sensor_id: len(series) for sensor_id, series in time_series_dict.items()
    }
    top_sensor = max(sensor_data_counts, key=sensor_data_counts.get)

    # Set theme
    pio.templates.default = "plotly_white"

    # Compute data completeness
    completeness_dict = compute_completeness(time_series_dict)

    # Create individual visualizations
    # 1. Sensor map with completeness data
    sensor_map_fig = create_sensors_map(completeness_dict)

    # 2. Other visualizations
    data_avail_fig = interactive_data_availability(time_series_dict)
    top_sensor_fig = interactive_sensor_windows(
        time_series_dict, top_sensor, window_size
    )
    window_counts_fig = interactive_window_counts(time_series_dict, window_size)
    daily_patterns_fig = visualize_daily_patterns(time_series_dict, n_sensors=4)

    # Try to create PCA visualization if possible
    try:
        pca_fig = visualize_window_pca(time_series_dict, window_size)
    except Exception as e:
        print(f"Could not create PCA visualization: {e}")
        pca_fig = None

    # Load the template
    template_path = get_template_path("dashboard_template.html")
    template = load_template(template_path)

    # Prepare context with all variables for the template
    context = {
        "window_size": window_size,
        "top_sensor": top_sensor,
        "sensor_map_fig": to_html(
            sensor_map_fig, include_plotlyjs="cdn", full_html=False
        ),
        "data_avail_fig": to_html(
            data_avail_fig, include_plotlyjs="cdn", full_html=False
        ),
        "top_sensor_fig": to_html(
            top_sensor_fig, include_plotlyjs="cdn", full_html=False
        ),
        "window_counts_fig": to_html(
            window_counts_fig, include_plotlyjs="cdn", full_html=False
        ),
        "daily_patterns_fig": to_html(
            daily_patterns_fig, include_plotlyjs="cdn", full_html=False
        ),
        "pca_fig": (
            to_html(pca_fig, include_plotlyjs="cdn", full_html=False)
            if pca_fig
            else None
        ),
    }

    # Render the template
    html_content = render_template(template, context)

    # Return the HTML content
    return html_content


if __name__ == "__main__":
    # Create the dashboard
    dashboard_html = create_sensor_window_dashboard(
        data_file="dashboards/data/test_data_1yr.pkl", window_size=24
    )

    # Save to a file
    output_path = Path(__file__).parent / "index.html"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(dashboard_html)

    print(f"Dashboard created: {output_path}")



================================================
File: digester.sh
================================================
#!/bin/bash

# digester.sh - Script to ingest codebase while excluding large files and data files
# Dependencies: gitingest, nbstripout

set -e  # Exit on error

# Configuration
MAX_FILE_SIZE_KB=500  # Set maximum file size to 500 KB
MAX_FILE_SIZE_BYTES=$((MAX_FILE_SIZE_KB * 1024))
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
OUTPUT_FILE="$PROJECT_ROOT/digested_dashboards_$(date +%Y%m%d).txt"

# Check if gitingest is installed
if ! command -v gitingest &> /dev/null; then
    echo "Error: gitingest is not installed. Please install it first."
    echo "Install with: pip install gitingest"
    exit 1
fi

# Check if nbstripout is installed
if ! command -v nbstripout &> /dev/null; then
    echo "Warning: nbstripout is not installed. Notebooks will not be processed."
    echo "Consider installing with: pip install nbstripout"
    PROCESS_NOTEBOOKS=false
else
    PROCESS_NOTEBOOKS=true
fi

# Process notebooks if nbstripout is available
if [ "$PROCESS_NOTEBOOKS" = true ]; then
    echo "Processing notebooks with nbstripout..."
    find "$SCRIPT_DIR" -name "*.ipynb" -exec nbstripout {} \;
fi

echo "Starting codebase ingestion from gnn_package directory..."
echo "- Max file size: ${MAX_FILE_SIZE_KB}KB"
echo "- Output will be saved to: ${OUTPUT_FILE}"

# Run gitingest on the gnn_package directory
gitingest "$SCRIPT_DIR" \
    -s "${MAX_FILE_SIZE_BYTES}" \
    --exclude-pattern="*.pkl" \
    --exclude-pattern="*.npy" \
    --exclude-pattern="*.csv" \
    --exclude-pattern="*.parquet" \
    --exclude-pattern="*.json" \
    --exclude-pattern="*.gz" \
    --exclude-pattern="*.zip" \
    --exclude-pattern="*.tar" \
    --exclude-pattern="*.h5" \
    --exclude-pattern="*.hdf5" \
    --exclude-pattern="*.pyc" \
    --exclude-pattern="__pycache__/" \
    --exclude-pattern=".ipynb_checkpoints/" \
    --exclude-pattern="cache/" \
    --exclude-pattern="*/cache/*" \
    --exclude-pattern="*.so" \
    --exclude-pattern="*.o" \
    --exclude-pattern="*.a" \
    --exclude-pattern="*.dll" \
    --exclude-pattern="*.geojson" \
    --exclude-pattern="*.shp" \
    --exclude-pattern="*.shx" \
    --exclude-pattern="*.dbf" \
    --exclude-pattern="*.prj" \
    --exclude-pattern="*.cpg" \
    --exclude-pattern="*.pth" \
    --exclude-pattern="*.pt" \
    --exclude-pattern="*.ckpt" \
    --exclude-pattern="*.bin" \
    --exclude-pattern="*.png" \
    --exclude-pattern="*.jpg" \
    --exclude-pattern="*.jpeg" \
    --exclude-pattern="*.gif" \
    --exclude-pattern="*.svg" \
    --exclude-pattern="*.ico" \
    --exclude-pattern="*.pdf" \
    --exclude-pattern="*index.html" \
    --output="$OUTPUT_FILE"

echo "Nom nom, digestion complete! Output saved to $OUTPUT_FILE"


================================================
File: training_winodows.ipynb
================================================
# Jupyter notebook converted to Python script.

# import numpy as np
# import pandas as pd
import pickle
from datetime import datetime, timedelta

# import plotly.graph_objects as go
# import plotly.express as px
# from plotly.subplots import make_subplots
# import matplotlib.pyplot as plt
# from sklearn.decomposition import PCA
# from sklearn.preprocessing import StandardScaler


# Helper function to load data from file
def load_data(file_path="test_data_1yr.pkl"):
    with open(file_path, "rb") as f:
        return pickle.load(f)


# # Find continuous segments in time series
# def find_continuous_segments(time_index, values, gap_threshold=pd.Timedelta(minutes=15)):
#     segments = []
#     start_idx = 0

#     for i in range(1, len(time_index)):
#         time_diff = time_index[i] - time_index[i-1]

#         # Check for gaps in time or values
#         if (time_diff > gap_threshold) or (np.isnan(values[i-1]) or np.isnan(values[i])):
#             if i - start_idx >= 24:  # Assuming minimum window size of 24
#                 segments.append((start_idx, i))
#             start_idx = i

#     # Add the last segment if it's long enough
#     if len(time_index) - start_idx >= 24:
#         segments.append((start_idx, len(time_index)))

#     return segments

# # Create an interactive data availability heatmap
# def interactive_data_availability(time_series_dict):
#     """Create an interactive heatmap showing data availability across sensors over time"""
#     # Get all unique timestamps from all sensors
#     all_timestamps = set()
#     for sensor_id, series in time_series_dict.items():
#         all_timestamps.update(series.index)

#     all_timestamps = sorted(all_timestamps)

#     # Create a DataFrame with all timestamps and fill with NaN
#     data_matrix = pd.DataFrame(index=all_timestamps)

#     # For each sensor, add a column to the DataFrame
#     for sensor_id, series in time_series_dict.items():
#         data_matrix[sensor_id] = np.nan
#         # Only fill in data that exists
#         data_matrix.loc[series.index, sensor_id] = 1

#     # Resample to a lower resolution for better visualization if too many datapoints
#     if len(all_timestamps) > 1000:
#         data_matrix = data_matrix.resample('1H').mean()

#     # Convert to long format for plotly
#     data_long = data_matrix.reset_index().melt(
#         id_vars='index',
#         var_name='sensor_id',
#         value_name='has_data'
#     )

#     # Create the heatmap with plotly
#     fig = px.density_heatmap(
#         data_long,
#         x='index',
#         y='sensor_id',
#         z='has_data',
#         color_continuous_scale=[
#             [0, 'rgba(255,255,255,0)'],  # Transparent for NaN
#             [0.5, 'rgba(222,235,247,1)'],  # Light blue
#             [1, 'rgba(49,130,189,1)']      # Dark blue
#         ],
#         title='Data Availability Across Sensors (Interactive)',
#         labels={'index': 'Date', 'sensor_id': 'Sensor ID', 'has_data': 'Data Available'}
#     )

#     # Update layout
#     fig.update_layout(
#         height=800,
#         xaxis_title='Date',
#         yaxis_title='Sensor ID',
#         title_x=0.5,
#         coloraxis_showscale=False
#     )

#     return fig

# # Create interactive window visualization for a given sensor
# def interactive_sensor_windows(time_series_dict, sensor_id, window_size=24, stride=1):
#     """Create an interactive visualization of windows for a specific sensor"""
#     series = time_series_dict.get(sensor_id)
#     if series is None or len(series) == 0:
#         return None

#     # Find continuous segments
#     segments = find_continuous_segments(series.index, series.values)

#     # Create a figure
#     fig = go.Figure()

#     # Add the raw time series
#     fig.add_trace(go.Scatter(
#         x=series.index,
#         y=series.values,
#         mode='lines',
#         name='Raw Data',
#         line=dict(color='darkgray')
#     ))

#     # Add segments and windows
#     for start_seg, end_seg in segments:
#         segment_indices = series.index[start_seg:end_seg]

#         # Add segment highlight
#         fig.add_trace(go.Scatter(
#             x=[segment_indices[0], segment_indices[0], segment_indices[-1], segment_indices[-1]],
#             y=[series.values.min(), series.values.max(), series.values.max(), series.values.min()],
#             fill="toself",
#             mode='none',
#             name=f'Segment: {segment_indices[0].date()} to {segment_indices[-1].date()}',
#             fillcolor='rgba(144,238,144,0.2)',
#             showlegend=True
#         ))

#         # Add a few example windows
#         n_windows = len(segment_indices) - window_size + 1

#         # Only show a few windows to avoid overcrowding
#         window_step = max(1, n_windows // 5)

#         for i in range(0, n_windows, window_step):
#             window_start = segment_indices[i]
#             window_end = segment_indices[i + window_size - 1]

#             fig.add_trace(go.Scatter(
#                 x=[window_start, window_start, window_end, window_end],
#                 y=[series.values.min(), series.values.max(), series.values.max(), series.values.min()],
#                 fill="toself",
#                 mode='none',
#                 name=f'Window: {window_start}',
#                 fillcolor='rgba(0,0,255,0.1)',
#                 showlegend=False
#             ))

#     # Update layout
#     fig.update_layout(
#         title=f'Time Windows for Sensor {sensor_id}',
#         xaxis_title='Date',
#         yaxis_title='Traffic Count',
#         height=600,
#         legend=dict(
#             orientation="h",
#             yanchor="bottom",
#             y=1.02,
#             xanchor="right",
#             x=1
#         )
#     )

#     return fig

# # Create a dashboard with multiple sensor window visualizations
# def interactive_window_dashboard(time_series_dict, window_size=24, n_sensors=4):
#     """Create a dashboard with window visualizations for multiple sensors"""
#     # Identify sensors with most data points
#     sensor_data_counts = {sensor_id: len(series) for sensor_id, series in time_series_dict.items()}
#     top_sensors = sorted(sensor_data_counts, key=sensor_data_counts.get, reverse=True)[:n_sensors]

#     # Create subplots
#     fig = make_subplots(
#         rows=n_sensors,
#         cols=1,
#         subplot_titles=[f'Sensor {sensor_id}' for sensor_id in top_sensors],
#         vertical_spacing=0.1
#     )

#     # Add data for each sensor
#     for i, sensor_id in enumerate(top_sensors):
#         series = time_series_dict[sensor_id]

#         # Add the raw time series
#         fig.add_trace(
#             go.Scatter(
#                 x=series.index,
#                 y=series.values,
#                 mode='lines',
#                 name=f'Sensor {sensor_id}',
#                 line=dict(color='darkgray')
#             ),
#             row=i+1,
#             col=1
#         )

#         # Find continuous segments
#         segments = find_continuous_segments(series.index, series.values)

#         # Add segment highlights for one example segment
#         for j, (start_seg, end_seg) in enumerate(segments):
#             if j > 2:  # Limit to first 3 segments to avoid overcrowding
#                 break

#             segment_indices = series.index[start_seg:end_seg]

#             # Add segment highlight
#             fig.add_trace(
#                 go.Scatter(
#                     x=[segment_indices[0], segment_indices[0], segment_indices[-1], segment_indices[-1], segment_indices[0]],
#                     y=[series.min(), series.max(), series.max(), series.min(), series.min()],
#                     fill="toself",
#                     mode='none',
#                     name=f'S{sensor_id} Segment {j+1}',
#                     fillcolor=f'rgba(144,238,144,0.2)',
#                     showlegend=True
#                 ),
#                 row=i+1,
#                 col=1
#             )

#     # Update layout
#     fig.update_layout(
#         height=300*n_sensors,
#         title_text="Time Windows Across Multiple Sensors",
#         showlegend=True,
#         legend=dict(orientation="h", y=-0.1)
#     )

#     return fig

# # Create a PCA visualization of sensor windows
# def visualize_window_pca(time_series_dict, window_size=24, n_sensors=10):
#     """Create a PCA visualization of sensor windows to see patterns"""
#     # Collect window data
#     windows_data = []
#     sensor_ids = []

#     # Process top sensors
#     sensor_data_counts = {sensor_id: len(series) for sensor_id, series in time_series_dict.items()}
#     top_sensors = sorted(sensor_data_counts, key=sensor_data_counts.get, reverse=True)[:n_sensors]

#     for sensor_id in top_sensors:
#         series = time_series_dict[sensor_id]

#         # Find continuous segments
#         segments = find_continuous_segments(series.index, series.values)

#         # Extract windows
#         for start_seg, end_seg in segments:
#             segment_values = series.values[start_seg:end_seg]

#             # Create windows
#             for i in range(0, len(segment_values) - window_size + 1, window_size//2):  # 50% overlap
#                 window = segment_values[i:i+window_size]

#                 if not np.isnan(window).any():  # Skip windows with NaN values
#                     windows_data.append(window)
#                     sensor_ids.append(sensor_id)

#     if not windows_data:
#         return None

#     # Convert to numpy array
#     X = np.array(windows_data)

#     # Normalize the data
#     scaler = StandardScaler()
#     X_scaled = scaler.fit_transform(X)

#     # Apply PCA
#     pca = PCA(n_components=2)
#     X_pca = pca.fit_transform(X_scaled)

#     # Create a DataFrame for plotting
#     pca_df = pd.DataFrame({
#         'PC1': X_pca[:, 0],
#         'PC2': X_pca[:, 1],
#         'sensor_id': sensor_ids
#     })

#     # Create a scatter plot
#     fig = px.scatter(
#         pca_df,
#         x='PC1',
#         y='PC2',
#         color='sensor_id',
#         title='PCA of Sensor Windows',
#         labels={'PC1': f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)',
#                 'PC2': f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)'},
#         hover_data=['sensor_id']
#     )

#     fig.update_layout(height=700, width=900)

#     return fig

# # Create a window count bar chart
# def interactive_window_counts(time_series_dict, window_size=24, n_sensors=20):
#     """Create an interactive bar chart of window counts by sensor"""
#     # Count windows per sensor
#     window_counts = {}

#     for sensor_id, series in time_series_dict.items():
#         # Find segments without large gaps
#         segments = find_continuous_segments(series.index, series.values)

#         # Count windows
#         total_windows = 0
#         for start_seg, end_seg in segments:
#             segment_len = end_seg - start_seg
#             total_windows += max(0, segment_len - window_size + 1)

#         window_counts[sensor_id] = total_windows

#     # Sort by window count
#     sorted_counts = sorted(window_counts.items(), key=lambda x: x[1], reverse=True)[:n_sensors]

#     # Create a DataFrame
#     count_df = pd.DataFrame(sorted_counts, columns=['sensor_id', 'window_count'])

#     # Create a bar chart
#     fig = px.bar(
#         count_df,
#         x='sensor_id',
#         y='window_count',
#         title=f'Number of Available Windows (size={window_size}) by Sensor',
#         labels={'sensor_id': 'Sensor ID', 'window_count': 'Number of Windows'},
#         color='window_count',
#         color_continuous_scale=px.colors.sequential.Viridis
#     )

#     fig.update_layout(height=600, xaxis_tickangle=-45)

#     return fig

# # Create a heatmap of daily patterns
# def visualize_daily_patterns(time_series_dict, n_sensors=6):
#     """Create a heatmap of daily patterns for top sensors"""
#     # Identify sensors with most data points
#     sensor_data_counts = {sensor_id: len(series) for sensor_id, series in time_series_dict.items()}
#     top_sensors = sorted(sensor_data_counts, key=sensor_data_counts.get, reverse=True)[:n_sensors]

#     # Create subplots
#     fig = make_subplots(
#         rows=n_sensors,
#         cols=1,
#         subplot_titles=[f'Sensor {sensor_id} - Daily Pattern' for sensor_id in top_sensors],
#         vertical_spacing=0.08
#     )

#     # Process each sensor
#     for i, sensor_id in enumerate(top_sensors):
#         series = time_series_dict[sensor_id]

#         # Create a DataFrame with hour and day of week
#         df = pd.DataFrame({
#             'value': series.values,
#             'hour': series.index.hour,
#             'day_of_week': series.index.dayofweek
#         })

#         # Group by hour and day of week
#         pivot_data = df.pivot_table(
#             values='value',
#             index='day_of_week',
#             columns='hour',
#             aggfunc='mean'
#         ).fillna(0)

#         # Create heatmap
#         heatmap = go.Heatmap(
#             z=pivot_data.values,
#             x=pivot_data.columns,
#             y=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'],
#             colorscale='Viridis',
#             showscale=(i==0),  # Only show colorbar for first heatmap
#         )

#         fig.add_trace(heatmap, row=i+1, col=1)

#         # Update axes
#         fig.update_xaxes(title_text="Hour of Day" if i==n_sensors-1 else "", row=i+1, col=1)
#         fig.update_yaxes(title_text="Day of Week", row=i+1, col=1)

#     # Update layout
#     fig.update_layout(
#         height=250*n_sensors,
#         title_text="Daily Traffic Patterns Across Sensors"
#     )

#     return fig

# # Function to create all interactive visualizations
# def create_interactive_visualizations(data_file='test_data_1yr.pkl', window_size=24):
#     """Create and return a list of interactive visualizations for time window analysis"""
#     # Load the data
#     time_series_dict = load_data(data_file)

#     # Create visualizations
#     visualizations = []

#     # 1. Data availability heatmap
#     viz1 = interactive_data_availability(time_series_dict)
#     visualizations.append(('data_availability', viz1))

#     # 2. Window dashboard for top sensors
#     viz2 = interactive_window_dashboard(time_series_dict, window_size=window_size)
#     visualizations.append(('window_dashboard', viz2))

#     # 3. Window counts
#     viz3 = interactive_window_counts(time_series_dict, window_size=window_size)
#     visualizations.append(('window_counts', viz3))

#     # 4. PCA of windows
#     viz4 = visualize_window_pca(time_series_dict, window_size=window_size)
#     visualizations.append(('window_pca', viz4))

#     # 5. Daily patterns heatmap
#     viz5 = visualize_daily_patterns(time_series_dict)
#     visualizations.append(('daily_patterns', viz5))

#     return visualizations

# # Create a comprehensive dashboard combining multiple visualizations
# def create_sensor_window_dashboard(data_file='test_data_1yr.pkl', window_size=24):
#     """Create a comprehensive dashboard for analyzing sensor time windows"""
#     # Load the data
#     time_series_dict = load_data(data_file)

#     # Identify sensors with most data points for individual analysis
#     sensor_data_counts = {sensor_id: len(series) for sensor_id, series in time_series_dict.items()}
#     top_sensor = max(sensor_data_counts, key=sensor_data_counts.get)

#     # Create a multi-page dashboard using HTML and Plotly
#     from plotly.io import to_html
#     import plotly.io as pio

#     # Set theme
#     pio.templates.default = "plotly_white"

#     # Create individual visualizations
#     data_avail_fig = interactive_data_availability(time_series_dict)
#     top_sensor_fig = interactive_sensor_windows(time_series_dict, top_sensor, window_size)
#     window_counts_fig = interactive_window_counts(time_series_dict, window_size)
#     daily_patterns_fig = visualize_daily_patterns(time_series_dict, n_sensors=4)

#     # Try to create PCA visualization if possible
#     try:
#         pca_fig = visualize_window_pca(time_series_dict, window_size)
#     except:
#         pca_fig = None

#     # Combine into HTML
#     html_content = f"""
#     <!DOCTYPE html>
#     <html>
#     <head>
#         <title>Sensor Window Analysis Dashboard</title>
#         <style>
#             body {{
#                 font-family: Arial, sans-serif;
#                 margin: 0;
#                 padding: 20px;
#                 background-color: #f5f5f5;
#             }}
#             .dashboard-container {{
#                 max-width: 1200px;
#                 margin: 0 auto;
#                 background-color: white;
#                 border-radius: 8px;
#                 overflow: hidden;
#                 box-shadow: 0 0 10px rgba(0,0,0,0.1);
#             }}
#             .dashboard-header {{
#                 background-color: #4C78A8;
#                 color: white;
#                 padding: 20px;
#                 text-align: center;
#             }}
#             .dashboard-section {{
#                 padding: 20px;
#                 margin-bottom: 20px;
#                 border-bottom: 1px solid #eee;
#             }}
#             h1 {{
#                 margin: 0;
#             }}
#             h2 {{
#                 color: #2C3E50;
#                 margin-top: 0;
#             }}
#             .viz-container {{
#                 margin-top: 20px;
#             }}
#         </style>
#     </head>
#     <body>
#         <div class="dashboard-container">
#             <div class="dashboard-header">
#                 <h1>Sensor Time Window Analysis Dashboard</h1>
#                 <p>Window Size: {window_size} time steps</p>
#             </div>

#             <div class="dashboard-section">
#                 <h2>Data Availability Overview</h2>
#                 <p>This heatmap shows when data is available across all sensors. Darker blue indicates data availability.</p>
#                 <div class="viz-container">
#                     {to_html(data_avail_fig, include_plotlyjs='cdn', full_html=False)}
#                 </div>
#             </div>

#             <div class="dashboard-section">
#                 <h2>Time Windows for Top Sensor (ID: {top_sensor})</h2>
#                 <p>This visualization shows the raw data for the sensor with the most data points, highlighting continuous segments and example windows.</p>
#                 <div class="viz-container">
#                     {to_html(top_sensor_fig, include_plotlyjs='cdn', full_html=False)}
#                 </div>
#             </div>

#             <div class="dashboard-section">
#                 <h2>Window Count Distribution</h2>
#                 <p>This bar chart shows the number of available windows for each sensor.</p>
#                 <div class="viz-container">
#                     {to_html(window_counts_fig, include_plotlyjs='cdn', full_html=False)}
#                 </div>
#             </div>

#             <div class="dashboard-section">
#                 <h2>Daily Traffic Patterns</h2>
#                 <p>These heatmaps show the average traffic patterns by hour of day and day of week for top sensors.</p>
#                 <div class="viz-container">
#                     {to_html(daily_patterns_fig, include_plotlyjs='cdn', full_html=False)}
#                 </div>
#             </div>
#     """

#     # Add PCA visualization if available
#     if pca_fig is not None:
#         html_content += f"""
#             <div class="dashboard-section">
#                 <h2>PCA of Sensor Windows</h2>
#                 <p>This scatter plot shows a 2D representation of the window patterns across sensors using Principal Component Analysis.</p>
#                 <div class="viz-container">
#                     {to_html(pca_fig, include_plotlyjs='cdn', full_html=False)}
#                 </div>
#             </div>
#         """

#     # Close HTML
#     html_content += """
#         </div>
#     </body>
#     </html>
#     """

#     # Return the HTML content
#     return html_content

time_series_dict = load_data("data/test_data_1yr.pkl")

time_series_dict.keys()

time_series_dict["10000"]

from datetime import datetime, timedelta

from gnn_package.config import ExperimentConfig


def compute_completeness(time_series_dict):
    config = ExperimentConfig(
        "/Users/administrator/Code/python/phd-project-gnn/config.yml"
    )
    start_date = datetime.strptime(config.data.start_date, "%Y-%m-%d %H:%M:%S")
    end_date = datetime.strptime(config.data.end_date, "%Y-%m-%d %H:%M:%S")
    days_between = (end_date - start_date).total_seconds() / (60 * 60 * 24)
    expected_records = days_between * 24 * 4
    print(f"Total days between start and end date: {days_between}")
    completeness_dict = {}
    for keys, series in time_series_dict.items():
        series = series[~series.index.duplicated(keep="first")]
        completeness_dict[keys] = len(series) / expected_records
    return completeness_dict

start_date = datetime.strptime(config.data.start_date, "%Y-%m-%d %H:%M:%S")
end_date = datetime.strptime(config.data.end_date, "%Y-%m-%d %H:%M:%S")

len(time_series_dict["10000"])
type(time_series_dict["10000"])

completeness_dict = compute_completeness(time_series_dict)
for keys, values in completeness_dict.items():
    print(keys)
    print(values)
    break

import plotly.express as px

from dashboards.utils import load_sensor_geojson


def completeness_map(time_series_dict):
    # Load the sensor geojson
    sensor_geojson = load_sensor_geojson(
        "/Users/administrator/Code/python/phd-project-gnn/dashboards/data/sensors.geojson"
    )

    # Create the map
    fig = px.choropleth_mapbox(
        sensor_geojson,
        geojson=sensor_geojson,
        locations="id",
        featureidkey="properties.id",
        color="completeness",
        color_continuous_scale="Viridis",
        range_color=(0, 1),
        mapbox_style="carto-positron",
        zoom=10,
        center={"lat": 37.7749, "lon": -122.4194},
        opacity=0.5,
        labels={"completeness": "Completeness"},
    )

    return fig

completeness_map(time_series_dict)

# # Or create a complete dashboard
# dashboard_html = create_sensor_window_dashboard(data_file='../test_data_1yr.pkl', window_size=24)

# # Save the HTML to a file
# with open('sensor_dashboard.html', 'w') as f:
#     f.write(dashboard_html)

# print("Dashboard saved to sensor_dashboard.html")



================================================
File: components/__init__.py
================================================
from .counts_bar import interactive_window_counts
from .daily_patterns import visualize_daily_patterns
from .window_segments import interactive_sensor_windows
from .heatmap import interactive_data_availability
from .pca_plot import visualize_window_pca
from .sensor_map import create_sensors_map

__all__ = [
    "interactive_data_availability",
    "interactive_sensor_windows",
    "interactive_window_counts",
    "visualize_daily_patterns",
    "visualize_window_pca",
    "create_sensors_map",
]



================================================
File: components/counts_bar.py
================================================
import pandas as pd
import plotly.express as px
from dashboards.utils import find_continuous_segments


# Create a window count bar chart
def interactive_window_counts(time_series_dict, window_size=24, n_sensors=20):
    """Create an interactive bar chart of window counts by sensor"""
    # Count windows per sensor
    window_counts = {}

    for sensor_id, series in time_series_dict.items():
        # Find segments without large gaps
        segments = find_continuous_segments(series.index, series.values)

        # Count windows
        total_windows = 0
        for start_seg, end_seg in segments:
            segment_len = end_seg - start_seg
            total_windows += max(0, segment_len - window_size + 1)

        window_counts[sensor_id] = total_windows

    # Sort by window count
    sorted_counts = sorted(window_counts.items(), key=lambda x: x[1], reverse=True)[
        :n_sensors
    ]

    # Create a DataFrame
    count_df = pd.DataFrame(sorted_counts, columns=["sensor_id", "window_count"])

    # Create a bar chart
    fig = px.bar(
        count_df,
        x="sensor_id",
        y="window_count",
        title=f"Number of Available Windows (size={window_size}) by Sensor",
        labels={"sensor_id": "Sensor ID", "window_count": "Number of Windows"},
        color="window_count",
        color_continuous_scale=px.colors.sequential.Viridis,
    )

    fig.update_layout(height=600, xaxis_tickangle=-45)

    return fig



================================================
File: components/daily_patterns.py
================================================
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots


# Create a heatmap of daily patterns
def visualize_daily_patterns(time_series_dict, n_sensors=6):
    """Create a heatmap of daily patterns for top sensors"""
    # Identify sensors with most data points
    sensor_data_counts = {
        sensor_id: len(series) for sensor_id, series in time_series_dict.items()
    }
    top_sensors = sorted(sensor_data_counts, key=sensor_data_counts.get, reverse=True)[
        :n_sensors
    ]

    # Create subplots
    fig = make_subplots(
        rows=n_sensors,
        cols=1,
        subplot_titles=[
            f"Sensor {sensor_id} - Daily Pattern" for sensor_id in top_sensors
        ],
        vertical_spacing=0.08,
    )

    # Process each sensor
    for i, sensor_id in enumerate(top_sensors):
        series = time_series_dict[sensor_id]

        # Create a DataFrame with hour and day of week
        df = pd.DataFrame(
            {
                "value": series.values,
                "hour": series.index.hour,
                "day_of_week": series.index.dayofweek,
            }
        )

        # Group by hour and day of week
        pivot_data = df.pivot_table(
            values="value", index="day_of_week", columns="hour", aggfunc="mean"
        ).fillna(0)

        # Create heatmap
        heatmap = go.Heatmap(
            z=pivot_data.values,
            x=pivot_data.columns,
            y=["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"],
            colorscale="Viridis",
            showscale=(i == 0),  # Only show colorbar for first heatmap
        )

        fig.add_trace(heatmap, row=i + 1, col=1)

        # Update axes
        fig.update_xaxes(
            title_text="Hour of Day" if i == n_sensors - 1 else "", row=i + 1, col=1
        )
        fig.update_yaxes(title_text="Day of Week", row=i + 1, col=1)

    # Update layout
    fig.update_layout(
        height=250 * n_sensors, title_text="Daily Traffic Patterns Across Sensors"
    )

    return fig



================================================
File: components/heatmap.py
================================================
import numpy as np
import pandas as pd
import plotly.express as px


# Create an interactive data availability heatmap
def interactive_data_availability(time_series_dict):
    """Create an interactive heatmap showing data availability across sensors over time"""
    # Get all unique timestamps from all sensors
    all_timestamps = set()
    for sensor_id, series in time_series_dict.items():
        all_timestamps.update(series.index)

    all_timestamps = sorted(all_timestamps)

    # Create a DataFrame with all timestamps and fill with NaN
    data_matrix = pd.DataFrame(index=all_timestamps)

    # For each sensor, add a column to the DataFrame
    for sensor_id, series in time_series_dict.items():
        data_matrix[sensor_id] = np.nan
        # Only fill in data that exists
        data_matrix.loc[series.index, sensor_id] = 1

    # Resample to a lower resolution for better visualization if too many datapoints
    if len(all_timestamps) > 1000:
        data_matrix = data_matrix.resample("1h").mean()

    # Convert to long format for plotly
    data_long = data_matrix.reset_index().melt(
        id_vars="index", var_name="sensor_id", value_name="has_data"
    )

    # Create the heatmap with plotly
    fig = px.density_heatmap(
        data_long,
        x="index",
        y="sensor_id",
        z="has_data",
        color_continuous_scale=[
            [0, "rgba(255,255,255,0)"],  # Transparent for NaN
            [0.5, "rgba(222,235,247,1)"],  # Light blue
            [1, "rgba(49,130,189,1)"],  # Dark blue
        ],
        title="Data Availability Across Sensors (Interactive)",
        labels={
            "index": "Date",
            "sensor_id": "Sensor ID",
            "has_data": "Data Available",
        },
    )

    # Update layout
    fig.update_layout(
        height=800,
        xaxis_title="Date",
        yaxis_title="Sensor ID",
        title_x=0.5,
        coloraxis_showscale=False,
    )

    return fig



================================================
File: components/pca_plot.py
================================================
import numpy as np
import pandas as pd
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from dashboards.utils import find_continuous_segments


# Create a PCA visualization of sensor windows
def visualize_window_pca(time_series_dict, window_size=24, n_sensors=10):
    """Create a PCA visualization of sensor windows to see patterns"""
    # Collect window data
    windows_data = []
    sensor_ids = []

    # Process top sensors
    sensor_data_counts = {
        sensor_id: len(series) for sensor_id, series in time_series_dict.items()
    }
    top_sensors = sorted(sensor_data_counts, key=sensor_data_counts.get, reverse=True)[
        :n_sensors
    ]

    for sensor_id in top_sensors:
        series = time_series_dict[sensor_id]

        # Find continuous segments
        segments = find_continuous_segments(series.index, series.values)

        # Extract windows
        for start_seg, end_seg in segments:
            segment_values = series.values[start_seg:end_seg]

            # Create windows
            for i in range(
                0, len(segment_values) - window_size + 1, window_size // 2
            ):  # 50% overlap
                window = segment_values[i : i + window_size]

                if not np.isnan(window).any():  # Skip windows with NaN values
                    windows_data.append(window)
                    sensor_ids.append(sensor_id)

    if not windows_data:
        return None

    # Convert to numpy array
    X = np.array(windows_data)

    # Normalize the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Apply PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)

    # Create a DataFrame for plotting
    pca_df = pd.DataFrame(
        {"PC1": X_pca[:, 0], "PC2": X_pca[:, 1], "sensor_id": sensor_ids}
    )

    # Create a scatter plot
    fig = px.scatter(
        pca_df,
        x="PC1",
        y="PC2",
        color="sensor_id",
        title="PCA of Sensor Windows",
        labels={
            "PC1": f"PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)",
            "PC2": f"PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)",
        },
        hover_data=["sensor_id"],
    )

    fig.update_layout(height=700, width=900)

    return fig



================================================
File: components/sensor_map.py
================================================
import plotly.express as px
import pandas as pd
import numpy as np
from private_uoapi import LightsailWrapper, LSAuth, LSConfig
import geopandas as gpd
from gnn_package.src.preprocessing import get_sensor_name_id_map


def create_sensors_map(completeness_dict):
    """
    Create an interactive map of sensor locations colored by data completeness.
    Includes bounds to prevent excessive panning and ensures the map loads properly.
    Fixed zoom level for better visibility of Newcastle upon Tyne area.

    Parameters:
    -----------
    completeness_dict : dict
        Dictionary mapping sensor IDs to their completeness percentage (0-1)

    Returns:
    --------
    plotly.graph_objects.Figure
        Interactive map with sensors
    """
    try:
        # Try to read the shapefile
        shapefile_path = "/Users/administrator/Code/python/phd-project-gnn/gnn_package/data/sensors/sensors.shp"
        gdf = gpd.read_file(shapefile_path)

        # Check the CRS - crucial step!
        print(f"Original GeoDataFrame CRS: {gdf.crs}")

        # If the data is in UK National Grid (EPSG:27700), convert to WGS84 (EPSG:4326)
        if gdf.crs == "EPSG:27700" or str(gdf.crs).find("27700") >= 0:
            print("Converting from EPSG:27700 (UK National Grid) to EPSG:4326 (WGS84)")
            gdf = gdf.to_crs("EPSG:4326")

        # Extract coordinates to a regular pandas DataFrame without geometry objects
        lat_values = []
        lon_values = []
        for point in gdf["geometry"]:
            # After conversion, y should be latitude and x should be longitude
            lat_values.append(float(point.y))
            lon_values.append(float(point.x))

        # Verify coordinates are in reasonable lat/lon range
        print(
            f"Coordinate range - Lat: {min(lat_values)} to {max(lat_values)}, Lon: {min(lon_values)} to {max(lon_values)}"
        )

        # Create a clean DataFrame without geometry objects
        df = pd.DataFrame(
            {"location": gdf["location"].tolist(), "lat": lat_values, "lon": lon_values}
        )

    except Exception as e:
        print(f"Error reading or processing shapefile: {e}")
        print("Attempting to fetch data from API instead...")

        # Fallback to using the API
        config = LSConfig()
        auth = LSAuth(config)
        client = LightsailWrapper(config, auth)
        locations = client.get_traffic_sensors()

        # Create a regular DataFrame (not GeoDataFrame)
        df = pd.DataFrame(locations)

    # Calculate center and bounds with appropriate lat/lon coordinates
    # If we have lat/lon values in df, use them
    if "lat" in df.columns and "lon" in df.columns and len(df) > 0:
        center_lat = np.mean(df["lat"])
        center_lon = np.mean(df["lon"])

        # Calculate appropriate zoom level based on the spread of data
        lat_range = max(df["lat"]) - min(df["lat"])
        lon_range = max(df["lon"]) - min(df["lon"])

        # Adjust zoom based on geographic spread (smaller range = higher zoom)
        # These values are calibrated for the Newcastle area
        if max(lat_range, lon_range) < 0.05:
            zoom_level = 13  # Very localized data
        elif max(lat_range, lon_range) < 0.1:
            zoom_level = 12  # Small area
        elif max(lat_range, lon_range) < 0.3:
            zoom_level = 11  # Medium area (typical for Newcastle)
        else:
            zoom_level = 10  # Larger area

        print(
            f"Auto-calculated zoom level: {zoom_level} based on data spread: {lat_range},{lon_range}"
        )
    else:
        # Fallback values for Newcastle upon Tyne
        center_lat = 54.97
        center_lon = -1.61
        zoom_level = 11

    # Set appropriate bounds
    lat_padding = 0.01  # Reduced padding for tighter view
    lon_padding = 0.01

    if "lat" in df.columns and "lon" in df.columns and len(df) > 0:
        min_lat = max(min(df["lat"]) - lat_padding, -90)
        max_lat = min(max(df["lat"]) + lat_padding, 90)
        min_lon = max(min(df["lon"]) - lon_padding, -180)
        max_lon = min(max(df["lon"]) + lon_padding, 180)
    else:
        # Fallback bounds for Newcastle
        min_lat = 54.93
        max_lat = 55.02
        min_lon = -1.65
        max_lon = -1.55

    # Get mapping between sensor names and IDs
    name_id_map = get_sensor_name_id_map()

    # Add completeness data
    df["sensor_id"] = df["location"].map(name_id_map)
    df["completeness"] = df["sensor_id"].map(lambda x: completeness_dict.get(x, 0))
    df["completeness_pct"] = df["completeness"] * 100

    # Debug info
    print(f"Map center: {center_lat}, {center_lon}")
    print(f"Map bounds: {min_lat}, {min_lon}, {max_lat}, {max_lon}")
    print(f"Number of sensors to display: {len(df)}")
    print(f"Using zoom level: {zoom_level}")

    # Define a custom color scale: red (low) -> yellow (medium) -> green (high)
    # This creates a more intuitive color scale for completeness
    custom_colorscale = [
        [0.0, "rgba(178, 24, 43, 1)"],  # Dark red for very low completeness
        [0.25, "rgba(239, 138, 98, 1)"],  # Light red/orange for low completeness
        [0.5, "rgba(253, 219, 121, 1)"],  # Yellow for medium completeness
        [0.75, "rgba(173, 221, 142, 1)"],  # Light green for good completeness
        [1.0, "rgba(49, 163, 84, 1)"],  # Dark green for excellent completeness
    ]

    # Create the map - note the adjusted size_max and explicit zoom level
    fig = px.scatter_mapbox(
        df,
        lat="lat",
        lon="lon",
        color="completeness_pct",
        color_continuous_scale=custom_colorscale,
        range_color=[0, 100],
        size=30,
        size_max=30,  # Smaller markers
        hover_name="location",
        hover_data={
            "completeness_pct": ":.1f",
            "lat": False,
            "lon": False,
            "sensor_id": True,
        },
        title="Sensor Locations and Data Completeness",
        labels={"completeness_pct": "Data Completeness (%)"},
    )

    # Update layout with explicit zoom level
    fig.update_layout(
        height=700,
        margin={"r": 0, "t": 30, "l": 0, "b": 0},
        coloraxis_colorbar=dict(
            title="Completeness (%)",
            ticksuffix="%",
            tickvals=[0, 25, 50, 75, 100],
            ticktext=["0%", "25%", "50%", "75%", "100%"],
        ),
        mapbox=dict(
            center=dict(lat=center_lat, lon=center_lon),
            style="carto-positron",
            zoom=zoom_level,  # Explicitly set zoom level
            bounds=dict(west=min_lon, east=max_lon, south=min_lat, north=max_lat),
        ),
    )

    return fig



================================================
File: components/window_segments.py
================================================
import plotly.graph_objects as go
from dashboards.utils.data_utils import find_continuous_segments


# Create interactive window visualization for a given sensor
def interactive_sensor_windows(time_series_dict, sensor_id, window_size=24, stride=1):
    """Create an interactive visualization of windows for a specific sensor"""
    series = time_series_dict.get(sensor_id)
    if series is None or len(series) == 0:
        return None

    # Find continuous segments
    segments = find_continuous_segments(series.index, series.values)

    # Create a figure
    fig = go.Figure()

    # Add the raw time series
    fig.add_trace(
        go.Scatter(
            x=series.index,
            y=series.values,
            mode="lines",
            name="Raw Data",
            line=dict(color="darkgray"),
        )
    )

    # Add segments and windows
    for start_seg, end_seg in segments:
        segment_indices = series.index[start_seg:end_seg]

        # Add segment highlight
        fig.add_trace(
            go.Scatter(
                x=[
                    segment_indices[0],
                    segment_indices[0],
                    segment_indices[-1],
                    segment_indices[-1],
                ],
                y=[
                    series.values.min(),
                    series.values.max(),
                    series.values.max(),
                    series.values.min(),
                ],
                fill="toself",
                mode="none",
                name=f"Segment: {segment_indices[0].date()} to {segment_indices[-1].date()}",
                fillcolor="rgba(144,238,144,0.2)",
                showlegend=True,
            )
        )

        # Add a few example windows
        n_windows = len(segment_indices) - window_size + 1

        # Only show a few windows to avoid overcrowding
        window_step = max(1, n_windows // 5)

        for i in range(0, n_windows, window_step):
            window_start = segment_indices[i]
            window_end = segment_indices[i + window_size - 1]

            fig.add_trace(
                go.Scatter(
                    x=[window_start, window_start, window_end, window_end],
                    y=[
                        series.values.min(),
                        series.values.max(),
                        series.values.max(),
                        series.values.min(),
                    ],
                    fill="toself",
                    mode="none",
                    name=f"Window: {window_start}",
                    fillcolor="rgba(0,0,255,0.1)",
                    showlegend=False,
                )
            )

    # Update layout
    fig.update_layout(
        title=f"Time Windows for Sensor {sensor_id}",
        xaxis_title="Date",
        yaxis_title="Traffic Count",
        height=800,
        legend=dict(orientation="h", yanchor="bottom", y=-0.4, xanchor="right", x=1),
    )

    return fig





================================================
File: templates/__init__.py
================================================
# Templates package initialization
# This file is used to mark the directory as a Python package

from pathlib import Path


def get_template_path(template_name):
    """
    Get the full path to a template file

    Parameters:
    -----------
    template_name : str
        The name of the template file

    Returns:
    --------
    Path
        Full path to the template file
    """
    return Path(__file__).parent / template_name



================================================
File: templates/dashboard_template.html
================================================
<!DOCTYPE html>
<html>

<head>
    <title>Sensor Window Analysis Dashboard</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }

        .dashboard-container {
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }

        .dashboard-header {
            background-color: #4C78A8;
            color: white;
            padding: 20px;
            text-align: center;
        }

        .dashboard-section {
            padding: 20px;
            margin-bottom: 20px;
            border-bottom: 1px solid #eee;
        }

        h1 {
            margin: 0;
        }

        h2 {
            color: #2C3E50;
            margin-top: 0;
        }

        .viz-container {
            margin-top: 20px;
        }
    </style>
</head>

<body>
    <div class="dashboard-container">
        <div class="dashboard-header">
            <h1>Sensor Time Window Analysis Dashboard</h1>
            <p>Window Size: {{window_size}} time steps</p>
        </div>

        <div class="dashboard-section">
            <h2>Sensor Map with Data Completeness</h2>
            <p>This map shows all sensor locations colored by data completeness percentage.</p>
            <div class="viz-container">
                {{sensor_map_fig}}
            </div>
        </div>

        <div class="dashboard-section">
            <h2>Data Availability Overview</h2>
            <p>This heatmap shows when data is available across all sensors. Darker blue indicates data availability.
            </p>
            <div class="viz-container">
                {{data_avail_fig}}
            </div>
        </div>

        <div class="dashboard-section">
            <h2>Time Windows for Top Sensor (ID: {{top_sensor}})</h2>
            <p>This visualization shows the raw data for the sensor with the most data points, highlighting continuous
                segments and example windows.</p>
            <div class="viz-container">
                {{top_sensor_fig}}
            </div>
        </div>

        <div class="dashboard-section">
            <h2>Window Count Distribution</h2>
            <p>This bar chart shows the number of available windows for each sensor.</p>
            <div class="viz-container">
                {{window_counts_fig}}
            </div>
        </div>

        <div class="dashboard-section">
            <h2>Daily Traffic Patterns</h2>
            <p>These heatmaps show the average traffic patterns by hour of day and day of week for top sensors.</p>
            <div class="viz-container">
                {{daily_patterns_fig}}
            </div>
        </div>

        <!-- PCA Section - Will be included conditionally -->
        {{#pca_fig}}
        <div class="dashboard-section">
            <h2>PCA of Sensor Windows</h2>
            <p>This scatter plot shows a 2D representation of the window patterns across sensors using Principal
                Component Analysis.</p>
            <div class="viz-container">
                {{pca_fig}}
            </div>
        </div>
        {{/pca_fig}}
    </div>
</body>

</html>


================================================
File: utils/__init__.py
================================================
from .data_utils import load_data, find_continuous_segments, load_sensor_geojson
from .template_utils import load_template, render_template, get_template_path

__all__ = [
    "load_data",
    "find_continuous_segments",
    "load_sensor_geojson",
    "load_template",
    "render_template",
    "get_template_path",
]



================================================
File: utils/data_utils.py
================================================
import json
import pickle
import numpy as np
import pandas as pd


# Helper function to load data from file
def load_data(file_path="dashboards/data/test_data_1yr.pkl"):
    with open(file_path, "rb") as f:
        return pickle.load(f)


# Find continuous segments in time series
def find_continuous_segments(
    time_index, values, gap_threshold=pd.Timedelta(minutes=15)
):
    segments = []
    start_idx = 0

    for i in range(1, len(time_index)):
        time_diff = time_index[i] - time_index[i - 1]

        # Check for gaps in time or values
        if (time_diff > gap_threshold) or (
            np.isnan(values[i - 1]) or np.isnan(values[i])
        ):
            if i - start_idx >= 24:  # Assuming minimum window size of 24
                segments.append((start_idx, i))
            start_idx = i

    # Add the last segment if it's long enough
    if len(time_index) - start_idx >= 24:
        segments.append((start_idx, len(time_index)))

    return segments


# Load sensor location data from file
def load_sensor_geojson(file_path="dashboards/data/sensors.geojson"):
    with open(file_path, "r", encoding="utf8") as f:
        return json.load(f)



================================================
File: utils/template_utils.py
================================================
"""
Template utilities for the dashboard application.
Provides functions for loading and rendering HTML templates.
"""

import re
from pathlib import Path


def load_template(template_path):
    """
    Load an HTML template from file

    Parameters:
    -----------
    template_path : str or Path
        Path to the template file

    Returns:
    --------
    str
        The template content as a string
    """
    with open(template_path, "r", encoding="utf-8") as file:
        return file.read()


def render_template(template, context):
    """
    Simple template rendering function that replaces placeholders with values

    Parameters:
    -----------
    template : str
        The template string with placeholders
    context : dict
        A dictionary of placeholder names and their values

    Returns:
    --------
    str
        The rendered template

    Notes:
    ------
    Supports:
    - Variable interpolation: {{variable}}
    - Conditional blocks: {{#variable}}content{{/variable}}
    - Comments: {{!comment}}
    """
    # Remove comments
    template = re.sub(r"{{!.*?}}", "", template, flags=re.DOTALL)

    # Replace all {{variable}} placeholders with their values
    for key, value in context.items():
        placeholder = f"{{{{{key}}}}}"
        replacement = str(value) if value is not None else ""
        # Simple string replacement instead of regex to avoid escape sequence issues
        template = template.replace(placeholder, replacement)

    # Handle conditional blocks {{#variable}} content {{/variable}}
    for key, value in context.items():
        start_tag = f"{{{{#{key}}}}}"
        end_tag = f"{{{{/{key}}}}}"

        # If the value exists and is truthy, remove just the conditional markers
        if value:
            # Find all occurrences of this conditional block
            start_pos = 0
            while True:
                start_idx = template.find(start_tag, start_pos)
                if start_idx == -1:
                    break

                end_idx = template.find(end_tag, start_idx)
                if end_idx == -1:
                    break

                # Extract the content between tags
                content = template[start_idx + len(start_tag) : end_idx]

                # Replace the entire block with just the content
                template = (
                    template[:start_idx] + content + template[end_idx + len(end_tag) :]
                )

                # Update start position for next iteration
                start_pos = start_idx + len(content)
        else:
            # If the value doesn't exist or is falsy, remove the entire block
            while True:
                start_idx = template.find(start_tag)
                if start_idx == -1:
                    break

                end_idx = template.find(end_tag)
                if end_idx == -1:
                    break

                # Remove the entire block including tags
                template = template[:start_idx] + template[end_idx + len(end_tag) :]

    # Clean up any remaining template tags (useful for optional content)
    while True:
        start_idx = template.find("{{")
        if start_idx == -1:
            break

        end_idx = template.find("}}", start_idx)
        if end_idx == -1:
            break

        template = template[:start_idx] + template[end_idx + 2 :]

    return template


def get_template_path(template_name, template_dir=None):
    """
    Get the full path to a template file

    Parameters:
    -----------
    template_name : str
        The name of the template file
    template_dir : str or Path, optional
        The directory containing templates. If None, uses the default 'templates' directory

    Returns:
    --------
    Path
        Full path to the template file
    """
    if template_dir is None:
        # Navigate to the templates directory relative to this file
        # Going up to utils directory, then up to the project root, then to templates
        template_dir = Path(__file__).parent.parent / "templates"
    else:
        template_dir = Path(template_dir)

    return template_dir / template_name



