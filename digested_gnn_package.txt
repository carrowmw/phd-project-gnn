Directory structure:
└── gnn_package/
    ├── __init__.py
    ├── __main__.py
    ├── config.yml
    ├── digester.sh
    ├── prediction_service.py
    ├── run_experiment.py
    ├── tune_model.py
    ├── config/
    │   ├── __init__.py
    │   ├── config.py
    │   ├── config_manager.py
    │   ├── paths.py
    │   └── __pycache__/
    ├── data/
    │   ├── preprocessed/
    │   │   ├── .DS_Store
    │   │   ├── graphs/
    │   │   └── timeseries/
    │   ├── raw/
    │   │   └── timeseries/
    │   └── sensors/
    ├── models/
    │   └── Default_Traffic_Prediction_Experiment/
    │       └── config.yml
    ├── results/
    │   ├── .DS_Store
    │   ├── test_1mnth/
    │   │   └── default_experiment_20250418_171039/
    │   │       ├── config.yml
    │   │       └── predictions/
    │   │           └── summary_160137.txt
    │   ├── test_1wk/
    │   │   └── default_experiment_20250423_085553/
    │   │       ├── config.yml
    │   │       └── predictions/
    │   │           └── summary_092319.txt
    │   └── test_1yr/
    │       ├── default_experiment_20250423_093151/
    │       └── default_experiment_20250423_093828/
    └── src/
        ├── data/
        │   ├── __init__.py
        │   ├── data_sources.py
        │   ├── processors.py
        │   └── __pycache__/
        ├── dataloaders/
        │   ├── __init__.py
        │   ├── dataloaders.py
        │   └── __pycache__/
        ├── models/
        │   ├── __init__.py
        │   ├── stgnn.py
        │   └── __pycache__/
        ├── preprocessing/
        │   ├── __init__.py
        │   ├── fetch_sensor_data.py
        │   ├── graph_analysis.py
        │   ├── graph_computation.py
        │   ├── graph_manipulation.py
        │   ├── graph_utils.py
        │   ├── graph_visualization.py
        │   ├── timeseries_preprocessor.py
        │   └── __pycache__/
        ├── training/
        │   ├── __init__.py
        │   ├── stgnn_prediction.py
        │   ├── stgnn_training.py
        │   └── __pycache__/
        ├── tuning/
        │   ├── __init__.py
        │   ├── experiment_manager.py
        │   ├── objective.py
        │   ├── parameter_space.py
        │   ├── tuning_utils.py
        │   └── __pycache__/
        ├── utils/
        │   ├── __init__.py
        │   ├── config_utils.py
        │   ├── data_utils.py
        │   ├── sensor_utils.py
        │   └── __pycache__/
        └── visualization/
            ├── dashboard.py
            ├── prediction_plots.py
            └── __pycache__/

================================================
File: __init__.py
================================================
from .config import paths
from .src.utils import data_utils, sensor_utils

from .config.paths import *
from .src import preprocessing
from .src import dataloaders
from .src import models
from .src import training

__all__ = ["paths", "data_utils", "preprocessing", "dataloaders", "models", "training"]



================================================
File: __main__.py
================================================
#!/usr/bin/env python
# train_model.py - Example script for training a GNN model using centralized configuration

import os
import argparse
import pickle
import torch
import matplotlib.pyplot as plt
from datetime import datetime

# Import gnn_package modules
from gnn_package.config import get_config, create_default_config, ExperimentConfig
from gnn_package import training
from gnn_package.src.utils.sensor_utils import get_sensor_name_id_map


def main():
    """Main function to run training with centralized configuration."""

    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description="Train a GNN model with centralized configuration"
    )
    parser.add_argument("--config", type=str, help="Path to config file")
    parser.add_argument("--data", type=str, help="Path to data file")
    parser.add_argument(
        "--create-config", action="store_true", help="Create a default config file"
    )
    parser.add_argument("--output", type=str, help="Output directory for results")
    args = parser.parse_args()

    # Create a default config if requested
    if args.create_config:
        config_path = args.config or "config.yml"
        print(f"Creating default configuration at {config_path}")
        config = create_default_config(config_path)
        print(
            "Default configuration created. Edit it as needed, then run the script again."
        )
        return

    # Load configuration
    if args.config:
        config = ExperimentConfig(args.config)
    else:
        # Use global configuration or create default if none exists
        try:
            config = get_config()
        except FileNotFoundError:
            print("No configuration found. Creating default configuration.")
            config = create_default_config("config.yml")
            print(
                "Default configuration created. Edit it as needed, then run the script again."
            )
            return

    # Print configuration details
    print(
        f"Running experiment: {config.experiment.name} (v{config.experiment.version})"
    )
    print(f"Description: {config.experiment.description}")
    print(
        f"Data config: window_size={config.data.general.window_size}, horizon={config.data.general.horizon}"
    )
    print(
        f"Model config: hidden_dim={config.model.hidden_dim}, layers={config.model.num_layers}"
    )
    print(
        f"Training config: epochs={config.training.num_epochs}, lr={config.training.learning_rate}"
    )

    # Set up paths
    raw_file_name = args.data or os.path.join(
        "gnn_package/data/raw/timeseries",
        f"test_data_{config.data.prediction.days_back}d.pkl",
    )
    print(f"Using data file: {raw_file_name}")

    # Set up output directory
    if args.output:
        output_dir = args.output
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(
            config.paths.results_dir,
            f"{config.experiment.name.replace(' ', '_')}_{timestamp}",
        )

    os.makedirs(output_dir, exist_ok=True)
    print(f"Results will be saved to: {output_dir}")

    # Save the configuration used for this run
    config_save_path = os.path.join(output_dir, "run_config.yml")
    config.save(config_save_path)
    print(f"Configuration saved to: {config_save_path}")

    # Preprocess data with centralized configuration
    print("Preprocessing data...")
    data_package = training.preprocess_data(
        data_file=raw_file_name,
        config=config,
    )

    # Save preprocessed data
    preprocessed_file_path = os.path.join(output_dir, "preprocessed_data.pkl")
    with open(preprocessed_file_path, "wb") as f:
        pickle.dump(data_package, f)
    print(f"Preprocessed data saved to: {preprocessed_file_path}")

    # Train model with centralized configuration
    print("Training model...")
    results = training.train_model(
        data_package=data_package,
        config=config,
    )

    # Save the trained model
    model_path = os.path.join(output_dir, "model.pth")
    torch.save(results["model"].state_dict(), model_path)
    print(f"Trained model saved to: {model_path}")

    # Save training plot
    plt.figure(figsize=(10, 6))
    plt.plot(results["train_losses"], label="Training Loss")
    plt.plot(results["val_losses"], label="Validation Loss")
    plt.title(f"Training Results: {config.experiment.name}")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True, alpha=0.3)

    plot_path = os.path.join(output_dir, "training_plot.png")
    plt.savefig(plot_path, dpi=300, bbox_inches="tight")
    print(f"Training plot saved to: {plot_path}")

    # Save training metrics
    metrics = {
        "train_losses": results["train_losses"],
        "val_losses": results["val_losses"],
        "best_val_loss": results["best_val_loss"],
        "config": config._config_dict,
    }

    metrics_path = os.path.join(output_dir, "metrics.pkl")
    with open(metrics_path, "wb") as f:
        pickle.dump(metrics, f)
    print(f"Training metrics saved to: {metrics_path}")

    print("Training complete!")


if __name__ == "__main__":
    main()



================================================
File: config.yml
================================================
data:
  general:
    batch_size: 32
    bbox_coords:
    - - -1.65327
      - 54.93188
    - - -1.54993
      - 54.93188
    - - -1.54993
      - 55.02084
    - - -1.65327
      - 55.02084
    bbox_crs: EPSG:4326
    buffer_factor: 1.0
    custom_filter: '["highway"~"footway|path|pedestrian|steps|corridor|track|service|living_street|residential|unclassified"]["area"!~"yes"]["access"!~"private"]'
    epsilon: 0.5
    gap_threshold_minutes: 15
    graph_prefix: 25022025_test
    horizon: 6
    max_distance: 100.0
    missing_value: -999.0
    network_type: walk
    normalization_factor: 10000
    place_name: Newcastle upon Tyne, UK
    resampling_frequency: 15min
    road_network_crs: EPSG:27700
    sensor_id_prefix: '1'
    sigma_squared: 0.1
    standardize: true
    stride: 1
    tolerance_decimal_places: 6
    window_size: 24
  prediction:
    days_back: 1
  training:
    cutoff_date: null
    cv_split_index: -1
    end_date: '2024-02-25 00:00:00'
    n_splits: 3
    split_method: rolling_window
    start_date: '2024-02-18 00:00:00'
    train_ratio: 0.8
    use_cross_validation: true
experiment:
  description: Traffic prediction using spatial-temporal GNN
  name: Default Traffic Prediction Experiment
  tags:
  - traffic
  - gnn
  - prediction
  version: 1.0.0
model:
  decoder_layers: 2
  dropout: 0.2
  hidden_dim: 64
  input_dim: 1
  num_gc_layers: 2
  num_layers: 2
  output_dim: 1
paths:
  data_cache: data/cache
  model_save_path: models
  results_dir: results
training:
  learning_rate: 0.001
  num_epochs: 50
  patience: 10
  weight_decay: 1.0e-05
visualization:
  dashboard_template: dashboard.html
  default_sensors_to_plot: 6
  max_sensors_in_heatmap: 50



================================================
File: digester.sh
================================================
#!/bin/bash

# digester.sh - Script to ingest codebase while excluding large files and data files
# Dependencies: gitingest, nbstripout

set -e  # Exit on error

# Configuration
MAX_FILE_SIZE_KB=500  # Set maximum file size to 500 KB
MAX_FILE_SIZE_BYTES=$((MAX_FILE_SIZE_KB * 1024))
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
OUTPUT_FILE="$PROJECT_ROOT/digested_gnn_package.txt"

# Check if gitingest is installed
if ! command -v gitingest &> /dev/null; then
    echo "Error: gitingest is not installed. Please install it first."
    echo "Install with: pip install gitingest"
    exit 1
fi

# Check if nbstripout is installed
if ! command -v nbstripout &> /dev/null; then
    echo "Warning: nbstripout is not installed. Notebooks will not be processed."
    echo "Consider installing with: pip install nbstripout"
    PROCESS_NOTEBOOKS=false
else
    PROCESS_NOTEBOOKS=true
fi

# Process notebooks if nbstripout is available
if [ "$PROCESS_NOTEBOOKS" = true ]; then
    echo "Processing notebooks with nbstripout..."
    find "$SCRIPT_DIR" -name "*.ipynb" -exec nbstripout {} \;
fi

echo "Starting codebase ingestion from gnn_package directory..."
echo "- Max file size: ${MAX_FILE_SIZE_KB}KB"
echo "- Output will be saved to: ${OUTPUT_FILE}"

# Run gitingest on the gnn_package directory
gitingest "$SCRIPT_DIR" \
    -s "${MAX_FILE_SIZE_BYTES}" \
    --exclude-pattern="*.pkl" \
    --exclude-pattern="*.npy" \
    --exclude-pattern="*.csv" \
    --exclude-pattern="*.parquet" \
    --exclude-pattern="*.json" \
    --exclude-pattern="*.gz" \
    --exclude-pattern="*.zip" \
    --exclude-pattern="*.tar" \
    --exclude-pattern="*.h5" \
    --exclude-pattern="*.hdf5" \
    --exclude-pattern="*.pyc" \
    --exclude-pattern="__pycache__/" \
    --exclude-pattern=".ipynb_checkpoints/" \
    --exclude-pattern="cache/" \
    --exclude-pattern="*/cache/*" \
    --exclude-pattern="*.so" \
    --exclude-pattern="*.o" \
    --exclude-pattern="*.a" \
    --exclude-pattern="*.dll" \
    --exclude-pattern="*.geojson" \
    --exclude-pattern="*.shp" \
    --exclude-pattern="*.shx" \
    --exclude-pattern="*.dbf" \
    --exclude-pattern="*.prj" \
    --exclude-pattern="*.cpg" \
    --exclude-pattern="*.pth" \
    --exclude-pattern="*.pt" \
    --exclude-pattern="*.ckpt" \
    --exclude-pattern="*.bin" \
    --exclude-pattern="*.png" \
    --exclude-pattern="*.jpg" \
    --exclude-pattern="*.jpeg" \
    --exclude-pattern="*.gif" \
    --exclude-pattern="*.svg" \
    --exclude-pattern="*.ico" \
    --exclude-pattern="*.pdf" \
    --output="$OUTPUT_FILE"

echo "Nom nom, digestion complete! Output saved to $OUTPUT_FILE"


================================================
File: prediction_service.py
================================================
#!/usr/bin/env python
# prediction_service.py

import os
import sys
import asyncio
import logging
import pandas as pd
from datetime import datetime
from pathlib import Path

from gnn_package import training
from gnn_package.config import ExperimentConfig
from gnn_package.src.utils.config_utils import create_prediction_config
from gnn_package.src.visualization.prediction_plots import (
    plot_sensors_grid,
    plot_error_distribution,
    save_visualization_pack,
)

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("prediction_service.log"),
    ],
)
logger = logging.getLogger("prediction-service")


async def run_prediction_service(model_path, output_dir=None, visualize=True):
    """
    Run the prediction service.

    Parameters:
    -----------
    model_path : str
        Path to the trained model
    output_dir : str, optional
        Directory to save predictions (defaults to 'predictions/YYYY-MM-DD')
    visualize : bool
        Whether to generate plots and visualizations
    """
    try:
        # Create prediction configuration
        logger.info("Creating prediction configuration")

        # Try to load config from model directory first
        model_dir = Path(model_path).parent
        config_path = model_dir / "config.yml"

        if config_path.exists():
            logger.info(f"Loading configuration from model directory: {config_path}")
            # Load as prediction config
            prediction_config = ExperimentConfig(
                str(config_path), is_prediction_mode=True
            )
        else:
            logger.info("Creating default prediction configuration")
            prediction_config = create_prediction_config()

        # Make sure days_back is set to a reasonable value for prediction
        if (
            not hasattr(prediction_config.data.prediction, "days_back")
            or prediction_config.data.prediction.days_back < 1
        ):
            logger.info("Setting days_back to default (7)")
            prediction_config.data.prediction.days_back = 7

        # Set up output directory
        if output_dir is None:
            today = datetime.now().strftime("%Y-%m-%d")
            output_dir = f"predictions/{today}"

        # Create timestamps for consistent file naming
        timestamp = datetime.now().strftime("%H%M%S")

        os.makedirs(output_dir, exist_ok=True)
        output_file = f"{output_dir}/predictions_{timestamp}.csv"

        # Run prediction
        logger.info(f"Running prediction using model: {model_path}")
        logger.info(f"Output will be saved to: {output_file}")

        start_time = datetime.now()
        predictions = await training.predict_all_sensors_with_validation(
            model_path=model_path,
            config=prediction_config,
            output_file=output_file,
            plot=False,  # Disable internal plotting
        )
        end_time = datetime.now()

        # Log results
        if predictions and "dataframe" in predictions:
            df = predictions["dataframe"]
            logger.info(
                f"Generated {len(df)} predictions for {df['node_id'].nunique()} sensors"
            )

            # Calculate error metrics
            mse = (df["error"] ** 2).mean()
            mae = df["abs_error"].mean()
            logger.info(f"Overall MSE: {mse:.4f}, MAE: {mae:.4f}")

            # Save summary statistics
            summary_file = f"{output_dir}/summary_{timestamp}.txt"

            with open(summary_file, "w") as f:
                f.write(f"Prediction Summary\n")
                f.write(f"=================\n\n")
                f.write(f"Date/Time: {datetime.now()}\n")
                f.write(f"Model: {model_path}\n")
                f.write(
                    f"Execution time: {(end_time - start_time).total_seconds():.2f} seconds\n\n"
                )

                # Extract standardization stats if available
                standardization_stats = {}
                if "data" in predictions and "metadata" in predictions["data"]:
                    metadata = predictions["data"]["metadata"]
                    standardization_stats = metadata.get("preprocessing_stats", {}).get(
                        "standardization", {}
                    )

                f.write(
                    f"Standardization mean: {standardization_stats.get('mean', 'N/A')}\n"
                )
                f.write(
                    f"Standardization std: {standardization_stats.get('std', 'N/A')}\n\n"
                )
                f.write(f"Predictions: {len(df)}\n")
                f.write(f"Sensors: {df['node_id'].nunique()}\n")
                f.write(f"Overall MSE: {mse:.4f}\n")
                f.write(f"Overall MAE: {mae:.4f}\n\n")
                f.write("Prediction summary by horizon:\n")
                f.write(df.groupby("horizon")[["abs_error"]].mean().to_string())

            logger.info(f"Summary saved to {summary_file}")

            # Generate visualizations if requested
            if visualize:
                logger.info("Generating visualizations...")
                try:
                    # Save comprehensive visualization pack
                    viz_paths = save_visualization_pack(
                        predictions_df=df,
                        results_dict=predictions,
                        output_dir=output_dir,
                        timestamp=timestamp,
                    )

                    logger.info(f"Visualizations saved to: {output_dir}")
                    for viz_type, path in viz_paths.items():
                        logger.info(f"  - {viz_type}: {path}")

                except Exception as e:
                    logger.error(f"Error generating visualizations: {e}")

            return {
                "success": True,
                "predictions_file": output_file,
                "summary_file": summary_file,
                "metrics": {"mse": mse, "mae": mae},
                "visualizations": viz_paths if visualize else None,
            }
        else:
            logger.error("Prediction failed or returned no results")
            return {"success": False, "error": "No predictions generated"}

    except Exception as e:
        logger.exception(f"Error in prediction service: {str(e)}")
        return {"success": False, "error": str(e)}


if __name__ == "__main__":
    # Get model path from command line or use default
    if len(sys.argv) > 1:
        model_path = sys.argv[1]
    else:
        model_path = "results/test_1wk/model.pth"

    # Get output directory if provided
    output_dir = sys.argv[2] if len(sys.argv) > 2 else None

    # Get visualization flag if provided
    visualize = True
    if len(sys.argv) > 3:
        visualize = sys.argv[3].lower() in ("yes", "true", "t", "1")

    # Run the prediction service
    result = asyncio.run(run_prediction_service(model_path, output_dir, visualize))

    # Exit with appropriate code
    sys.exit(0 if result["success"] else 1)



================================================
File: run_experiment.py
================================================
#!/usr/bin/env python
"""
Experiment runner for GNN traffic prediction

Usage: python run_experiment.py [--config CONFIG_PATH] [--output OUTPUT_DIR]
"""
import os
import sys
import json
import pickle
import argparse
import asyncio
from datetime import datetime
import matplotlib.pyplot as plt
import torch
import numpy as np

from gnn_package import training, preprocessing
from gnn_package import paths
from gnn_package.config import get_config, ExperimentConfig
from gnn_package.src.utils.data_utils import convert_numpy_types


runtime_stats = {
    "preprocessing": {},
    "standardization": {},
    "training": {},
}


async def main():
    """Main function to run an experiment"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Run a training experiment")
    parser.add_argument("--config", type=str, help="Path to custom config file")
    parser.add_argument(
        "--output", type=str, help="Output directory for experiment results"
    )
    parser.add_argument("--data", type=str, help="Path to data file")
    args = parser.parse_args()

    # Create experiment timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Initialize config
    if args.config and os.path.exists(args.config):
        print(f"Loading configuration from: {args.config}")
        config = ExperimentConfig(args.config)
        experiment_name = f"{os.path.basename(args.config).split('.')[0]}_{timestamp}"
    else:
        print("Using global configuration")
        config = get_config()
        experiment_name = f"default_experiment_{timestamp}"

    # Print configuration
    print(f"Experiment: {config.experiment.name} (v{config.experiment.version})")
    print(
        f"Data config: window_size={config.data.general.window_size}, horizon={config.data.general.horizon}"
    )
    print(
        f"Model config: hidden_dim={config.model.hidden_dim}, layers={config.model.num_layers}"
    )
    print(
        f"Training config: epochs={config.training.num_epochs}, lr={config.training.learning_rate}"
    )

    # Create output directory
    if args.output:
        output_dir = os.path.join(args.output, experiment_name)
    else:
        output_dir = os.path.join("experiments", experiment_name)
    os.makedirs(output_dir, exist_ok=True)

    # Set data file
    if args.data:
        raw_file_path = args.data
    else:
        raw_file_name = "test_data_1wk.pkl"
        raw_dir = paths.RAW_TIMESERIES_DIR
        raw_file_path = os.path.join(raw_dir, raw_file_name)

    print(f"Using data file: {raw_file_path}")
    runtime_stats["data_file"] = raw_file_path

    # Preprocess data
    preprocessed_file_name = f"data_loaders_{os.path.basename(raw_file_path)}"
    preprocessed_path = os.path.join(output_dir, preprocessed_file_name)

    # Preprocess data
    print("Preprocessing data...")
    preprocess_start = datetime.now()

    # Before preprocessing
    print("\n===== Starting preprocessing =====")

    # In main function
    data_package = await training.preprocess_data(
        data_file=raw_file_path, config=config
    )

    # After preprocessing
    print("\n===== Preprocessing completed =====")
    print(f"Data package keys: {data_package.keys()}")

    preprocess_end = datetime.now()
    print(f"Preprocessing completed in {preprocess_end - preprocess_start}")

    if data_package is None:
        raise ValueError(
            "preprocessing returned None - check the preprocessing pipeline"
        )
    print(f"Data loaders type: {type(data_package)}")
    if isinstance(data_package, dict):
        print(f"Data loaders keys: {data_package.keys()}")

    # Extract standardization stats if available
    preprocessing_stats = {}
    if (
        isinstance(data_package, dict)
        and "preprocessing_stats" in data_package["metadata"]
    ):
        preprocessing_stats = data_package["metadata"]["preprocessing_stats"]

    runtime_stats["preprocessing"] = {
        "start_time": preprocess_start.isoformat(),
        "end_time": preprocess_end.isoformat(),
        "duration_seconds": (preprocess_end - preprocess_start).total_seconds(),
        "standardization": preprocessing_stats.get("standardization", {}),
    }

    # Extract standardization stats if available (from the processor's internal data)
    # This depends on how your DataProcessor exposes the stats
    if hasattr(data_package["metadata"], "preprocessing_stats"):
        runtime_stats["standardization"] = (
            data_package.metadata.preprocessing_stats.get("standardization", {})
        )

    # Save preprocessed data
    with open(preprocessed_path, "wb") as f:
        pickle.dump(data_package, f)
    print(f"Preprocessed data saved to: {preprocessed_path}")

    # Train the model
    print("Training model...")
    training_start = datetime.now()
    results = training.train_model(data_package=data_package, config=config)
    training_end = datetime.now()

    # Store training statistics
    runtime_stats["training"] = {
        "start_time": training_start.isoformat(),
        "end_time": training_end.isoformat(),
        "duration_seconds": (training_end - training_start).total_seconds(),
        "epochs_trained": len(results["train_losses"]),
        "best_epoch": np.argmin(results["val_losses"]),
        "best_validation_loss": results["best_val_loss"],
    }

    # Save experiment outputs

    # 1. Save the model
    model_path = os.path.join(output_dir, "model.pth")
    torch.save(results["model"].state_dict(), model_path)
    print(f"Model saved to: {model_path}")

    # 2. Save a copy of the config used
    config_path = os.path.join(output_dir, "config.yml")
    config.save(config_path)
    print(f"Configuration saved to: {config_path}")

    # 3. Save runtime statistics
    stats_path = os.path.join(output_dir, "runtime_stats.json")
    # Convert numpy types to native Python types for JSON serialization
    runtime_stats_converted = convert_numpy_types(runtime_stats)

    with open(stats_path, "w") as f:
        json.dump(runtime_stats_converted, f, indent=2)
    print(f"Runtime statistics saved to: {stats_path}")

    # 4. Save performance metrics
    performance_path = os.path.join(output_dir, "performance.json")
    performance = {
        "best_validation_loss": results["best_val_loss"],
        "final_training_loss": results["train_losses"][-1],
        "final_validation_loss": results["val_losses"][-1],
        "epochs_trained": len(results["train_losses"]),
        "early_stopping": len(results["train_losses"]) < config.training.num_epochs,
        "training_losses": results["train_losses"],
        "validation_losses": results["val_losses"],
    }
    with open(performance_path, "w") as f:
        json.dump(performance, f, indent=2)
    print(f"Performance metrics saved to: {performance_path}")

    # 5. Generate a training curve plot
    plt.figure(figsize=(10, 6))
    plt.plot(results["train_losses"], label="Training Loss")
    plt.plot(results["val_losses"], label="Validation Loss")
    plt.axhline(
        y=results["best_val_loss"],
        color="r",
        linestyle="--",
        label=f"Best Val Loss: {results['best_val_loss']:.4f}",
    )
    plt.title("Training and Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plot_path = os.path.join(output_dir, "training_curve.png")
    plt.savefig(plot_path, dpi=300, bbox_inches="tight")
    print(f"Training curve plot saved to: {plot_path}")

    print(f"\nExperiment completed! All results saved to: {output_dir}")
    return output_dir


if __name__ == "__main__":
    try:
        output_dir = asyncio.run(main())
        print(f"Successfully completed experiment with results in: {output_dir}")
        sys.exit(0)
    except Exception as e:
        print(f"ERROR: Experiment failed: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)



================================================
File: tune_model.py
================================================
#!/usr/bin/env python
# tune_model.py - Script for hyperparameter tuning using the tuning module

import os
import argparse
import logging
from pathlib import Path
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("tuning.log"),
    ],
)
logger = logging.getLogger("tuning")

# Import tuning module
from gnn_package.src.tuning import (
    tune_hyperparameters,
    run_multi_stage_tuning,
    get_default_param_space,
)
from gnn_package.config import get_config, ExperimentConfig


def main():
    """Run hyperparameter tuning"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description="Hyperparameter tuning for GNN traffic prediction"
    )

    # Required arguments
    parser.add_argument(
        "--data",
        type=str,
        required=True,
        help="Path to the data file (e.g., data/raw/timeseries/test_data_1wk.pkl)",
    )

    # Optional arguments
    parser.add_argument("--config", type=str, help="Path to a custom config file")
    parser.add_argument("--output", type=str, help="Directory to save tuning results")
    parser.add_argument("--experiment", type=str, help="Name of the experiment")
    parser.add_argument(
        "--trials", type=int, default=20, help="Number of trials to run (default: 20)"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        help="Number of epochs per trial (default: use config value)",
    )
    parser.add_argument(
        "--multi-stage",
        action="store_true",
        help="Run multi-stage tuning with increasing data and epochs",
    )
    parser.add_argument(
        "--quick",
        action="store_true",
        help="Run a quick tuning with fewer trials and epochs (for testing)",
    )

    args = parser.parse_args()

    # Load config if provided
    if args.config:
        config = ExperimentConfig(args.config)
        logger.info(f"Loaded custom config from {args.config}")
    else:
        config = get_config()
        logger.info("Using default config")

    # Set up paths
    data_path = Path(args.data)
    if not data_path.exists():
        logger.error(f"Data file not found: {data_path}")
        return

    # Generate experiment name if not provided
    if args.experiment:
        experiment_name = args.experiment
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        dataset_name = data_path.stem
        experiment_name = f"tuning_{dataset_name}_{timestamp}"

    # Set output directory
    if args.output:
        output_dir = Path(args.output)
    else:
        output_dir = Path(f"results/tuning/{experiment_name}")

    # Run tuning
    if args.quick:
        logger.info("Running quick tuning (reduced trials and epochs for testing)")
        n_trials = 5
        n_epochs = 5
    else:
        n_trials = args.trials
        n_epochs = args.epochs

    if args.multi_stage:
        logger.info(
            f"Running multi-stage tuning with experiment name: {experiment_name}"
        )

        # Define stages for multi-stage tuning
        if args.quick:
            # Quick version for testing
            n_trials_stages = [3, 2]
            n_epochs_stages = [3, 5]
            data_fraction_stages = [0.3, 1.0]
        else:
            # Full version
            n_trials_stages = [15, 10, 5]
            n_epochs_stages = [10, 20, None]  # None uses the config value
            data_fraction_stages = [0.25, 0.5, 1.0]

        results = run_multi_stage_tuning(
            data_file=data_path,
            experiment_name=experiment_name,
            output_dir=output_dir,
            config=config,
            n_trials_stages=n_trials_stages,
            n_epochs_stages=n_epochs_stages,
            data_fraction_stages=data_fraction_stages,
        )

        logger.info(
            f"Multi-stage tuning completed. Results saved to {results['output_dir']}"
        )
        logger.info(f"Best parameters: {results['best_params']}")
    else:
        logger.info(
            f"Running hyperparameter tuning with experiment name: {experiment_name}"
        )

        results = tune_hyperparameters(
            data_file=data_path,
            experiment_name=experiment_name,
            n_trials=n_trials,
            n_epochs=n_epochs,
            output_dir=output_dir,
            config=config,
            retrain_best=True,
        )

        logger.info(f"Tuning completed. Results saved to {results['output_dir']}")
        logger.info(f"Best parameters: {results['best_params']}")
        logger.info(f"Best validation loss: {results['best_value']}")


if __name__ == "__main__":
    main()



================================================
File: config/__init__.py
================================================
# gnn_package/config/__init__.py
from .config import (
    DataConfig,
    ExperimentConfig,
    ModelConfig,
    PathsConfig,
    TrainingConfig,
    VisualizationConfig,
    ExperimentMetadata,
)

from .config_manager import (
    get_config,
    reset_config,
    create_default_config,
    load_yaml_config,
)

__all__ = [
    "DataConfig",
    "ExperimentConfig",
    "ModelConfig",
    "PathsConfig",
    "TrainingConfig",
    "VisualizationConfig",
    "ExperimentMetadata",
    "get_config",
    "reset_config",
    "create_default_config",
    "load_yaml_config",
]



================================================
File: config/config.py
================================================
# gnn_package/src/utils/config.py

import os
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional
import yaml
from datetime import timedelta
import pandas as pd


@dataclass
class ExperimentMetadata:
    """Metadata for the experiment."""

    name: str
    description: str
    version: str
    tags: List[str] = field(default_factory=list)


@dataclass
class GeneralDataConfig:
    """Configuration for data processing shared across training and prediction."""

    # Time series-related parameters
    window_size: int = 24
    horizon: int = 6
    stride: int = 1
    gap_threshold_minutes: int = 15
    missing_value: float = -1.0
    resampling_frequency: str = "15min"
    standardize: bool = True
    batch_size: int = 32
    buffer_factor: float = 1.0

    # Graph-related parameters
    graph_prefix: str = "25022025_test"  # TO DEPRECATE
    sigma_squared: float = 0.1
    epsilon: float = 0.5
    normalization_factor: int = 10000
    max_distance: float = 100.0  # For connected components
    tolerance_decimal_places: int = 6  # For coordinate comparison
    resampling_frequency: str = "15min"
    missing_value: float = -999.0
    sensor_id_prefix: str = "1"  # Added for sensor ID formatting
    bbox_coords: List[float] = field(
        default_factory=lambda: [
            [-1.65327, 54.93188],
            [-1.54993, 54.93188],
            [-1.54993, 55.02084],
            [-1.65327, 55.02084],
        ]
    )
    place_name: str = "Newcastle upon Tyne, UK"  # For osmnx graph creation
    bbox_crs: str = "EPSG:4326"
    road_network_crs: str = "EPSG:27700"
    network_type: str = "walk"
    custom_filter: str = """["highway"~"footway|path|pedestrian|steps|corridor|'
        'track|service|living_street|residential|unclassified"]'
        '["area"!~"yes"]["access"!~"private"]"""

    @property
    def gap_threshold(self) -> pd.Timedelta:
        """Get the gap threshold as a pandas Timedelta."""
        return pd.Timedelta(minutes=self.gap_threshold_minutes)


@dataclass
class TrainingDataConfig:
    """Configuration specific to training data processing."""

    start_date: str = "2024-02-18 00:00:00"
    end_date: str = "2024-02-25 00:00:00"
    n_splits: int = 3  # For rolling window splits
    use_cross_validation: bool = True
    split_method: str = "rolling_window"  # Options: "rolling_window", "time_based"
    train_ratio: float = 0.8  # For rolling window splits
    cutoff_date: str = None  # Instead of train_ratio for time-based splits
    cv_split_index: int = -1  # For cross-validation


@dataclass
class PredictionDataConfig:
    """Configuration specific to prediction/testing."""

    days_back: int  # How much historical data to use


@dataclass
class DataConfig:
    """Complete data configuration."""

    general: GeneralDataConfig
    training: TrainingDataConfig
    prediction: PredictionDataConfig


@dataclass
class ModelConfig:
    """Configuration for the model architecture."""

    input_dim: int = 1
    hidden_dim: int = 64
    output_dim: int = 1
    num_layers: int = 2
    dropout: float = 0.2
    num_gc_layers: int = 2
    use_self_loops: bool = True
    gcn_normalization: str = "symmetric"  # "symmetric", "random_walk", or "none"
    decoder_layers: int = 2
    attention_heads: int = 1
    layer_norm: bool = False


@dataclass
class TrainingConfig:
    """Configuration for model training."""

    learning_rate: float = 0.001
    weight_decay: float = 1e-5
    num_epochs: int = 50
    patience: int = 10
    device: Optional[str] = None


@dataclass
class PathsConfig:
    """Configuration for file paths."""

    model_save_path: str
    data_cache: str
    results_dir: str

    def __post_init__(self):
        """Convert string paths to Path objects."""
        self.model_save_path = Path(self.model_save_path)
        self.data_cache = Path(self.data_cache)
        self.results_dir = Path(self.results_dir)


@dataclass
class VisualizationConfig:
    """Configuration for visualization components."""

    dashboard_template: str
    default_sensors_to_plot: int
    max_sensors_in_heatmap: int


class ExperimentConfig:
    """Main configuration class for experiments."""

    def __init__(
        self, config_path: Optional[str] = None, is_prediction_mode: bool = False
    ):
        """
        Initialize configuration from a YAML file.

        Parameters:
        -----------
        config_path : str, optional
            Path to the YAML configuration file
        is_prediction_mode : bool
            Whether this configuration is for prediction (vs training)
        """
        self._initializing = True
        self.is_prediction_mode = is_prediction_mode

        if config_path is None:
            config_path = os.path.join(os.getcwd(), "config.yml")

        self.config_path = Path(config_path)
        self._load_config()

        # Mark initialization as complete and freeze the config
        self._initialzing = False
        self._frozen = True

        # Validate the configuration
        self.validate()

        # Log the configuration
        self.log()

    def __setattr__(self, name, value):
        if hasattr(self, "_frozen") and self._frozen:
            raise AttributeError(
                f"Cannot modify configuration after initialization: {name}"
            )
        super().__setattr__(name, value)

    def _load_config(self):
        """Load configuration from YAML file."""
        if not self.config_path.exists():
            raise FileNotFoundError(f"Config file not found: {self.config_path}")

        with open(self.config_path, "r") as f:
            config_dict = yaml.safe_load(f)

        # Initialize sub-configs with proper handling of nested structures
        self.experiment = ExperimentMetadata(**config_dict.get("experiment", {}))

        # Properly handle nested data configuration
        data_dict = config_dict.get("data", {})
        self.data = DataConfig(
            general=GeneralDataConfig(**data_dict.get("general", {})),
            training=TrainingDataConfig(**data_dict.get("training", {})),
            prediction=PredictionDataConfig(**data_dict.get("prediction", {})),
        )

        self.model = ModelConfig(**config_dict.get("model", {}))
        self.training = TrainingConfig(**config_dict.get("training", {}))
        self.paths = PathsConfig(**config_dict.get("paths", {}))
        self.visualization = VisualizationConfig(**config_dict.get("visualization", {}))

        # Store the raw dict for any additional access
        self._config_dict = config_dict

    def validate(self) -> bool:
        """
        Validate that all required configuration values are present and valid.

        Returns:
        --------
        bool
            True if validation passes (raises exceptions otherwise)
        """
        # Check for required model parameters
        required_model_params = [
            "input_dim",
            "hidden_dim",
            "output_dim",
            "num_layers",
            "dropout",
            "num_gc_layers",
            "use_self_loops",
            "gcn_normalization",
            "decoder_layers",
        ]

        for param in required_model_params:
            if not hasattr(self.model, param):
                raise ValueError(f"Missing required config value: model.{param}")

        # Check for required data parameters
        required_data_params = [
            "window_size",
            "horizon",
            "missing_value",
        ]

        for param in required_data_params:
            if not hasattr(self.data.general, param):
                # Add detailed debugging info
                print(f"DEBUG: Validation failed for {param}")
                print(
                    f"DEBUG: self.data.general has attributes: {dir(self.data.general)}"
                )
                print(
                    f"DEBUG: Raw config contains: {self._config_dict.get('data', {}).get('general', {})}"
                )
                raise ValueError(f"Missing required config value: data.general.{param}")

        # Check for required training parameters
        required_training_params = [
            "learning_rate",
            "weight_decay",
            "num_epochs",
            "patience",
            # Add any other required training parameters
        ]

        for param in required_training_params:
            if not hasattr(self.training, param):
                raise ValueError(f"Missing required config value: training.{param}")

        # Type checking and value validation
        if self.data.general.window_size <= 0:
            raise ValueError("window_size must be positive")
        if self.data.general.horizon <= 0:
            raise ValueError("horizon must be positive")
        if self.training.learning_rate <= 0:
            raise ValueError("learning_rate must be positive")
        if self.model.num_gc_layers <= 0:
            raise ValueError("num_gc_layers must be positive")

        return True

    def log(self, logger=None):
        """
        Log the configuration details.

        Parameters:
        -----------
        logger : logging.Logger, optional
            Logger to use. If None, creates or gets a default logger.
        """
        if logger is None:
            import logging

            logger = logging.getLogger(__name__)

        logger.info("Configuration loaded from: %s", self.config_path)

        logger.info(f"Experiment: {self.experiment.name} (v{self.experiment.version})")
        logger.info(f"Description: {self.experiment.description}")
        logger.info(
            f"Data config: window_size={self.data.general.window_size}, horizon={self.data.general.horizon}"
        )
        logger.info(
            f"Model config: hidden_dim={self.model.hidden_dim}, layers={self.model.num_layers}"
        )
        logger.info(
            f"Training config: epochs={self.training.num_epochs}, lr={self.training.learning_rate}"
        )

        # Log paths
        logger.info(f"Model save path: {self.paths.model_save_path}")
        logger.info(f"Results directory: {self.paths.results_dir}")

    def save(self, path: Optional[str] = None):
        """
        Save the current configuration to a YAML file.
        """
        save_path = Path(path) if path else self.config_path

        # Create nested dictionary from dataclasses
        config_dict = {
            "experiment": self._dataclass_to_dict(self.experiment),
            "model": self._dataclass_to_dict(self.model),
            "training": self._dataclass_to_dict(self.training),
            "paths": self._dataclass_to_dict(self.paths),
            "visualization": self._dataclass_to_dict(self.visualization),
        }

        # Handle the nested data section specially
        data_dict = {}
        data = self.data
        if hasattr(data, "general"):
            data_dict["general"] = self._dataclass_to_dict(data.general)
        if hasattr(data, "training"):
            data_dict["training"] = self._dataclass_to_dict(data.training)
        if hasattr(data, "prediction"):
            data_dict["prediction"] = self._dataclass_to_dict(data.prediction)

        config_dict["data"] = data_dict

        # Ensure the directory exists
        os.makedirs(save_path.parent, exist_ok=True)

        with open(save_path, "w") as f:
            yaml.dump(config_dict, f, default_flow_style=False)

    @staticmethod
    def _dataclass_to_dict(obj):
        """Convert a dataclass instance to a dictionary, including nested dataclasses."""
        if obj is None:
            return None

        result = {}
        for field_name in obj.__dataclass_fields__:
            value = getattr(obj, field_name)

            # Handle Path objects
            if isinstance(value, Path):
                value = str(value)
            # Handle nested dataclasses
            elif hasattr(value, "__dataclass_fields__"):
                value = ExperimentConfig._dataclass_to_dict(value)
            # Handle lists that might contain dataclasses
            elif isinstance(value, list):
                value = [
                    (
                        ExperimentConfig._dataclass_to_dict(item)
                        if hasattr(item, "__dataclass_fields__")
                        else item
                    )
                    for item in value
                ]

            result[field_name] = value
        return result

    def get(self, key: str, default: Any = None) -> Any:
        """
        Get a configuration value by its key path.

        Parameters:
        -----------
        key : str
            Dot-separated path to the configuration value (e.g., 'model.hidden_dim')
        default : Any
            Default value to return if the key is not found

        Returns:
        --------
        Any
            The configuration value or the default
        """
        keys = key.split(".")
        value = self._config_dict

        for k in keys:
            if isinstance(value, dict) and k in value:
                value = value[k]
            else:
                return default

        return value

    def __str__(self):
        """String representation of the configuration."""
        return (
            f"ExperimentConfig(\n"
            f"  experiment: {self.experiment.name} (v{self.experiment.version})\n"
            f"  data: window_size={self.data.general.window_size}, horizon={self.data.general.horizon}\n"
            f"  model: hidden_dim={self.model.hidden_dim}, layers={self.model.num_layers}\n"
            f"  training: epochs={self.training.num_epochs}, lr={self.training.learning_rate}\n"
            f")"
        )



================================================
File: config/config_manager.py
================================================
# gnn_package/config/config_manager.py

import os
import logging
from pathlib import Path
from typing import Optional, Dict, Any
import yaml

from .config import (
    ExperimentConfig,
    DataConfig,
    ModelConfig,
    TrainingConfig,
    PathsConfig,
    VisualizationConfig,
)

# Set up logging
logger = logging.getLogger(__name__)

# Singleton pattern for global configuration
_CONFIG_INSTANCE = None


def get_config(
    config_path: Optional[str] = None, verbose: bool = True
) -> ExperimentConfig:
    """
    Get or create the global configuration instance.

    Parameters:
    -----------
    config_path : str, optional
        Path to the configuration file. If not provided, will use the existing
        instance or look for a default config.yml in the current directory.
    verbose : bool
        Whether to print information about the configuration being used

    Returns:
    --------
    ExperimentConfig
        The global configuration instance
    """
    global _CONFIG_INSTANCE

    # Return existing instance if available and no new path provided
    if _CONFIG_INSTANCE is not None and config_path is None:
        if verbose:
            print(f"Using existing configuration from: {_CONFIG_INSTANCE.config_path}")
        return _CONFIG_INSTANCE

    # Create new instance if path provided or no instance exists
    if config_path is not None or _CONFIG_INSTANCE is None:
        try:
            if config_path is None:
                # Look for config.yml in current directory
                config_path = os.path.join(os.getcwd(), "config.yml")
                if verbose:
                    print(f"Looking for configuration file at: {config_path}")

            _CONFIG_INSTANCE = ExperimentConfig(config_path)
            if verbose:
                print(f"Loaded configuration from: {_CONFIG_INSTANCE.config_path}")

        except FileNotFoundError:
            if verbose:
                print(f"Configuration file not found at: {config_path}")
                print("Creating default configuration.")

            # Create default config.yml in current directory
            default_config_path = os.path.join(os.getcwd(), "config.yml")
            _CONFIG_INSTANCE = create_default_config(default_config_path)

            if verbose:
                print(f"Default configuration created at: {default_config_path}")

    return _CONFIG_INSTANCE


def reset_config():
    """Reset the global configuration instance."""
    print("reset_config: Resetting global config instance.")
    global _CONFIG_INSTANCE
    _CONFIG_INSTANCE = None


def create_default_config(output_path: str = "config.yml") -> ExperimentConfig:
    """
    Create a default configuration file and return its instance.

    Parameters:
    -----------
    output_path : str
        Path where to save the default configuration file

    Returns:
    --------
    ExperimentConfig
        The created configuration instance
    """
    # Default experiment metadata
    experiment = {
        "name": "Default Traffic Prediction Experiment",
        "description": "Traffic prediction using spatial-temporal GNN",
        "version": "1.0.0",
        "tags": ["traffic", "gnn", "prediction"],
    }

    # Default data configuration
    data = {
        "general": {
            "window_size": 24,
            "horizon": 6,
            "stride": 1,
            "batch_size": 32,
            "gap_threshold_minutes": 15,
            "standardize": True,
            "missing_value": -999.0,
            "resampling_frequency": "15min",
            "buffer_factor": 1.0,
            "graph_prefix": "25022025_test",
            "sensor_id_prefix": "1",
            "sigma_squared": 0.1,
            "epsilon": 0.5,
            "normalization_factor": 10000,
            "max_distance": 100.0,
            "tolerance_decimal_places": 6,
            # Network parameters
            "bbox_coords": [
                [-1.65327, 54.93188],
                [-1.54993, 54.93188],
                [-1.54993, 55.02084],
                [-1.65327, 55.02084],
            ],
            "place_name": "Newcastle upon Tyne, UK",
            "bbox_crs": "EPSG:4326",
            "road_network_crs": "EPSG:27700",
            "network_type": "walk",
            "custom_filter": '["highway"~"footway|path|pedestrian|steps|corridor|'
            'track|service|living_street|residential|unclassified"]'
            '["area"!~"yes"]["access"!~"private"]',
        },
        "training": {
            "start_date": "2024-02-18 00:00:00",
            "end_date": "2024-02-25 00:00:00",
            "n_splits": 3,
            "use_cross_validation": True,
            "split_method": "rolling_window",
            "train_ratio": 0.8,
            "cutoff_date": None,
            "cv_split_index": -1,
        },
        "prediction": {
            "days_back": 14,
        },
    }

    # Default model configuration
    model = {
        "input_dim": 1,
        "hidden_dim": 64,
        "output_dim": 1,
        "num_layers": 2,
        "dropout": 0.2,
        "num_gc_layers": 2,
        "decoder_layers": 2,
    }

    # Default training configuration
    training = {
        "learning_rate": 0.001,
        "weight_decay": 1e-5,
        "num_epochs": 50,
        "patience": 10,
    }

    # Default paths
    paths = {
        "model_save_path": "models",
        "data_cache": "data/cache",
        "results_dir": "results",
    }

    # Default visualization configuration
    visualization = {
        "dashboard_template": "dashboard.html",
        "default_sensors_to_plot": 6,
        "max_sensors_in_heatmap": 50,
    }

    # Create the configuration dictionary
    config_dict = {
        "experiment": experiment,
        "data": data,
        "model": model,
        "training": training,
        "paths": paths,
        "visualization": visualization,
    }

    # Save to file
    with open(output_path, "w") as f:
        yaml.dump(config_dict, f, default_flow_style=False)

    # Create and return instance
    config = ExperimentConfig(output_path)
    global _CONFIG_INSTANCE
    _CONFIG_INSTANCE = config

    return config


def load_yaml_config(config_path: str) -> Dict[str, Any]:
    """
    Load a YAML configuration file.

    Parameters:
    -----------
    config_path : str
        Path to the YAML configuration file

    Returns:
    --------
    Dict[str, Any]
        The loaded configuration dictionary
    """
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config file not found: {config_path}")

    with open(config_path, "r") as f:
        config_dict = yaml.load(f, Loader=yaml.Loader)

    return config_dict



================================================
File: config/paths.py
================================================
# Defines all of the paths used in the project

from pathlib import Path
from typing import Dict
import os

# Root directory of the project
ROOT_DIR = Path(__file__).parent.parent.parent.resolve()
assert ROOT_DIR.exists(), f"Invalid ROOT_DIR: {ROOT_DIR}"
assert (
    ROOT_DIR.name == "phd-project-gnn"
), f"Invalid ROOT_DIR - Check Parents: {ROOT_DIR}"

# Package directory

PACKAGE_DIR = ROOT_DIR / "gnn_package"

# Source directory
SRC_DIR = PACKAGE_DIR / "src"

# Data directory
DATA_DIR = PACKAGE_DIR / "data"

# Sensor locations data directory
SENSORS_DATA_DIR = DATA_DIR / "sensors"

# Raw data directory
RAW_DATA_DIR = DATA_DIR / "raw"
RAW_TIMESERIES_DIR = RAW_DATA_DIR / "timeseries"

# Preprocessed data directories
PREPROCESSED_DATA_DIR = DATA_DIR / "preprocessed"
PREPROCESSED_GRAPH_DIR = PREPROCESSED_DATA_DIR / "graphs"
PREPROCESSED_TIMESERIES_DIR = PREPROCESSED_DATA_DIR / "timeseries"

# Model directories


# Config directories


# Create directories if they don't exist
DIRS: Dict[str, Path] = {
    "data": DATA_DIR,
    "sensors": SENSORS_DATA_DIR,
    "preprocessed": PREPROCESSED_DATA_DIR,
    # "interim_data": INTERIM_DATA_DIR,
    # "models": MODELS_DIR,
    # "checkpoints": CHECKPOINTS_DIR,
    # "artifacts": ARTIFACTS_DIR,
    # "config": CONFIG_DIR,
    # "model_config": MODEL_CONFIG_DIR,
    # "data_config": DATA_CONFIG_DIR,
    # "results": RESULTS_DIR,
    # "figures": FIGURES_DIR,
    # "logs": LOGS_DIR
}

for dir_path in DIRS.values():
    dir_path.mkdir(parents=True, exist_ok=True)


def get_path(name: str) -> Path:
    """Get path by name from DIRS dictionary."""
    if name not in DIRS:
        raise KeyError(f"Path '{name}' not found in DIRS dictionary.")
    return DIRS[name]


def add_path(name: str, path: Path) -> None:
    """Add new path to DIRS dictionary."""
    DIRS[name] = path
    path.mkdir(parents=True, exist_ok=True)




================================================
File: data/preprocessed/.DS_Store
================================================
[Non-text file]






================================================
File: models/Default_Traffic_Prediction_Experiment/config.yml
================================================
data:
  general:
    batch_size: 32
    bbox_coords:
    - - -1.65327
      - 54.93188
    - - -1.54993
      - 54.93188
    - - -1.54993
      - 55.02084
    - - -1.65327
      - 55.02084
    bbox_crs: EPSG:4326
    buffer_factor: 1.0
    custom_filter: '["highway"~"footway|path|pedestrian|steps|corridor|track|service|living_street|residential|unclassified"]["area"!~"yes"]["access"!~"private"]'
    epsilon: 0.5
    gap_threshold_minutes: 15
    graph_prefix: 25022025_test
    horizon: 6
    max_distance: 100.0
    missing_value: -999.0
    network_type: walk
    normalization_factor: 10000
    place_name: Newcastle upon Tyne, UK
    resampling_frequency: 15min
    road_network_crs: EPSG:27700
    sensor_id_prefix: '1'
    sigma_squared: 0.1
    standardize: true
    stride: 1
    tolerance_decimal_places: 6
    window_size: 24
  prediction:
    days_back: 1
  training:
    cutoff_date: null
    cv_split_index: -1
    end_date: '2024-02-25 00:00:00'
    n_splits: 3
    split_method: rolling_window
    start_date: '2024-02-18 00:00:00'
    train_ratio: 0.8
    use_cross_validation: true
experiment:
  description: Traffic prediction using spatial-temporal GNN
  name: Default Traffic Prediction Experiment
  tags:
  - traffic
  - gnn
  - prediction
  version: 1.0.0
model:
  attention_heads: 1
  decoder_layers: 2
  dropout: 0.2
  gcn_normalization: symmetric
  hidden_dim: 64
  input_dim: 1
  layer_norm: false
  num_gc_layers: 2
  num_layers: 2
  output_dim: 1
  use_self_loops: true
paths:
  data_cache: data/cache
  model_save_path: models
  results_dir: results
training:
  device: null
  learning_rate: 0.001
  num_epochs: 50
  patience: 10
  weight_decay: 1.0e-05
visualization:
  dashboard_template: dashboard.html
  default_sensors_to_plot: 6
  max_sensors_in_heatmap: 50



================================================
File: results/.DS_Store
================================================
[Non-text file]


================================================
File: results/test_1mnth/default_experiment_20250418_171039/config.yml
================================================
data:
  general:
    batch_size: 32
    bbox_coords:
    - - -1.65327
      - 54.93188
    - - -1.54993
      - 54.93188
    - - -1.54993
      - 55.02084
    - - -1.65327
      - 55.02084
    bbox_crs: EPSG:4326
    buffer_factor: 1.0
    custom_filter: '["highway"~"footway|path|pedestrian|steps|corridor|track|service|living_street|residential|unclassified"]["area"!~"yes"]["access"!~"private"]'
    epsilon: 0.5
    gap_threshold_minutes: 15
    graph_prefix: 25022025_test
    horizon: 6
    max_distance: 100.0
    missing_value: -999.0
    network_type: walk
    normalization_factor: 10000
    place_name: Newcastle upon Tyne, UK
    resampling_frequency: 15min
    road_network_crs: EPSG:27700
    sensor_id_prefix: '1'
    sigma_squared: 0.1
    standardize: true
    stride: 1
    tolerance_decimal_places: 6
    window_size: 24
  prediction:
    days_back: 1
  training:
    cutoff_date: null
    cv_split_index: -1
    end_date: '2024-02-25 00:00:00'
    n_splits: 3
    split_method: rolling_window
    start_date: '2024-02-18 00:00:00'
    train_ratio: 0.8
    use_cross_validation: true
experiment:
  description: Traffic prediction using spatial-temporal GNN
  name: Default Traffic Prediction Experiment
  tags:
  - traffic
  - gnn
  - prediction
  version: 1.0.0
model:
  attention_heads: 1
  decoder_layers: 2
  dropout: 0.2
  gcn_normalization: symmetric
  hidden_dim: 64
  input_dim: 1
  layer_norm: false
  num_gc_layers: 2
  num_layers: 2
  output_dim: 1
  use_self_loops: true
paths:
  data_cache: data/cache
  model_save_path: models
  results_dir: results
training:
  device: null
  learning_rate: 0.001
  num_epochs: 50
  patience: 10
  weight_decay: 1.0e-05
visualization:
  dashboard_template: dashboard.html
  default_sensors_to_plot: 6
  max_sensors_in_heatmap: 50



================================================
File: results/test_1mnth/default_experiment_20250418_171039/predictions/summary_160137.txt
================================================
Prediction Summary
=================

Date/Time: 2025-04-22 16:01:42.840017
Model: results/test_1mnth/default_experiment_20250418_171039/model.pth
Execution time: 5.84 seconds

Standardization mean: 15.628493950771798
Standardization std: 39.84288958634343

Predictions: 354
Sensors: 59
Overall MSE: 5638.6511
Overall MAE: 6.2388

Prediction summary by horizon:
         abs_error
horizon           
1        17.514480
2         0.677238
3         0.576530
4         0.597107
5         0.570874
6        17.496560


================================================
File: results/test_1wk/default_experiment_20250423_085553/config.yml
================================================
data:
  general:
    batch_size: 32
    bbox_coords:
    - - -1.65327
      - 54.93188
    - - -1.54993
      - 54.93188
    - - -1.54993
      - 55.02084
    - - -1.65327
      - 55.02084
    bbox_crs: EPSG:4326
    buffer_factor: 1.0
    custom_filter: '["highway"~"footway|path|pedestrian|steps|corridor|track|service|living_street|residential|unclassified"]["area"!~"yes"]["access"!~"private"]'
    epsilon: 0.5
    gap_threshold_minutes: 15
    graph_prefix: 25022025_test
    horizon: 6
    max_distance: 100.0
    missing_value: -999.0
    network_type: walk
    normalization_factor: 10000
    place_name: Newcastle upon Tyne, UK
    resampling_frequency: 15min
    road_network_crs: EPSG:27700
    sensor_id_prefix: '1'
    sigma_squared: 0.1
    standardize: true
    stride: 1
    tolerance_decimal_places: 6
    window_size: 24
  prediction:
    days_back: 1
  training:
    cutoff_date: null
    cv_split_index: -1
    end_date: '2024-02-25 00:00:00'
    n_splits: 3
    split_method: rolling_window
    start_date: '2024-02-18 00:00:00'
    train_ratio: 0.8
    use_cross_validation: true
experiment:
  description: Traffic prediction using spatial-temporal GNN
  name: Default Traffic Prediction Experiment
  tags:
  - traffic
  - gnn
  - prediction
  version: 1.0.0
model:
  attention_heads: 1
  decoder_layers: 2
  dropout: 0.2
  gcn_normalization: symmetric
  hidden_dim: 64
  input_dim: 1
  layer_norm: false
  num_gc_layers: 2
  num_layers: 2
  output_dim: 1
  use_self_loops: true
paths:
  data_cache: data/cache
  model_save_path: models
  results_dir: results
training:
  device: null
  learning_rate: 0.001
  num_epochs: 50
  patience: 10
  weight_decay: 1.0e-05
visualization:
  dashboard_template: dashboard.html
  default_sensors_to_plot: 6
  max_sensors_in_heatmap: 50



================================================
File: results/test_1wk/default_experiment_20250423_085553/predictions/summary_092319.txt
================================================
Prediction Summary
=================

Date/Time: 2025-04-23 09:23:25.122018
Model: results/test_1wk/default_experiment_20250423_085553/model.pth
Execution time: 6.10 seconds

Standardization mean: 18.89549475416581
Standardization std: 44.869104722470134

Predictions: 354
Sensors: 59
Overall MSE: 11272.0777
Overall MAE: 12.2326

Prediction summary by horizon:
         abs_error
horizon           
1         0.973565
2        17.911451
3        17.910920
4        17.871178
5        17.841290
6         0.887404




================================================
File: src/data/__init__.py
================================================



================================================
File: src/data/data_sources.py
================================================
# gnn_package/src/data/data_sources.py
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from datetime import datetime, timedelta
import pandas as pd
from pathlib import Path

from private_uoapi import (
    LSConfig,
    LSAuth,
    LightsailWrapper,
    DateRangeParams,
    convert_to_dataframe,
)
from gnn_package.src.utils.sensor_utils import get_sensor_name_id_map
from gnn_package.src.preprocessing import load_sensor_data


class DataSource(ABC):
    """Abstract base class for data sources"""

    @abstractmethod
    async def get_data(self, config) -> Dict[str, pd.Series]:
        """Get time series data according to the provided configuration"""
        pass


class FileDataSource(DataSource):
    """Data source that loads data from a file"""

    def __init__(self, file_path: Path):
        self.file_path = file_path

    async def get_data(self, config) -> Dict[str, pd.Series]:
        """Load time series data from a file"""

        # Load data from file
        return load_sensor_data(self.file_path)


class APIDataSource(DataSource):
    """Data source that fetches data from API for prediction"""

    async def get_data(self, config) -> Dict[str, pd.Series]:
        """Fetch recent data from API based on prediction config"""

        # Initialize API client
        api_config = LSConfig()
        auth = LSAuth(api_config)
        client = LightsailWrapper(api_config, auth)

        # Get sensor name to ID mapping
        name_id_map = get_sensor_name_id_map(config=config)
        id_to_name_map = {v: k for k, v in name_id_map.items()}

        # Determine date range for API request
        days_back = config.data.prediction.days_back
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)

        # Create date range parameters
        date_range_params = DateRangeParams(
            start_date=start_date,
            end_date=end_date,
            max_date_range=timedelta(days=days_back + 1),
        )

        # Fetch data from API
        count_data = await client.get_traffic_data(date_range_params)
        counts_df = convert_to_dataframe(count_data)

        # Create time series dictionary
        time_series_dict = {}

        for node_id in name_id_map.values():
            # Look up location name for this node ID
            location = id_to_name_map.get(node_id)
            if not location:
                continue

            # Filter data for this location
            df = counts_df[counts_df["location"] == location]

            if df.empty:
                continue

            # Create time series
            series = pd.Series(df["value"].values, index=df["dt"])

            # Remove duplicates
            series = series[~series.index.duplicated(keep="first")]

            # Store in dictionary
            time_series_dict[node_id] = series

        return time_series_dict



================================================
File: src/data/processors.py
================================================
# gnn_package/src/data/processors.py
from enum import Enum
from typing import Dict, List, Optional, Union, TypedDict, Any
import pandas as pd
from pathlib import Path

from gnn_package.src.preprocessing import TimeSeriesPreprocessor, resample_sensor_data
from gnn_package.src.dataloaders import create_dataloader

from gnn_package.config import ExperimentConfig
from gnn_package.src.data.data_sources import DataSource, FileDataSource, APIDataSource

# Define the structure of the data loaders, graph data, and time series data
# {
#     "data_loaders": {
#         "train_loader": train_loader,  # Only in training mode
#         "val_loader": val_loader,      # In both modes
#     },
#     "graph_data": {
#         "adj_matrix": adj_matrix,
#         "node_ids": node_ids,
#     },
#     "time_series": {
#         "validation": validation_dict,  # Original series for validation
#         "input": input_dict,           # Series used for prediction (optional)
#     },
#     "metadata": {
#         "preprocessing_stats": preprocessing_stats,
#         "mode": "training" or "prediction"
#     }
# }


class DataLoaders(TypedDict):
    train_loader: Optional[Any]  # Use specific loader type if available
    val_loader: Any


class GraphData(TypedDict):
    adj_matrix: Any  # numpy.ndarray or similar
    node_ids: List[str]


class TimeSeriesData(TypedDict):
    validation: Dict[str, Any]  # Dict mapping node IDs to time series
    input: Optional[Dict[str, Any]]


class ProcessorMetadata(TypedDict):
    preprocessing_stats: Dict[str, Any]
    mode: str  # "training" or "prediction"


class ProcessorResult(TypedDict):
    data_loaders: DataLoaders
    graph_data: GraphData
    time_series: TimeSeriesData
    metadata: ProcessorMetadata


class ProcessorMode(Enum):
    TRAINING = "training"
    PREDICTION = "prediction"


class DataProcessorFactory:
    """Factory for creating data processors based on mode"""

    @staticmethod
    def create_processor(
        mode: ProcessorMode,
        config: ExperimentConfig,
        data_source: Optional[DataSource] = None,
    ):
        """Create the appropriate data processor based on mode"""
        if mode == ProcessorMode.TRAINING:
            return TrainingDataProcessor(config, data_source)
        elif mode == ProcessorMode.PREDICTION:
            return PredictionDataProcessor(config, data_source)
        else:
            raise ValueError(f"Unknown processor mode: {mode}")

    @staticmethod
    def create_from_config(config: ExperimentConfig, data_file: Optional[Path] = None):
        """Create appropriate processor and data source based on config"""
        # Determine if we're in prediction mode based on provided config
        is_prediction = (
            hasattr(config, "is_prediction_mode") and config.is_prediction_mode
        )

        # Create appropriate data source
        if is_prediction:
            data_source = APIDataSource()
            mode = ProcessorMode.PREDICTION
        else:
            if data_file is None:
                raise ValueError("Data file must be provided for training mode")
            data_source = FileDataSource(data_file)
            mode = ProcessorMode.TRAINING

        return DataProcessorFactory.create_processor(mode, config, data_source)


class BaseDataProcessor:
    """Base class for data processors"""

    def __init__(
        self, config: ExperimentConfig, data_source: Optional[DataSource] = None
    ):
        self.config = config
        self.data_source = data_source
        self.resampled_data = None  # Will be set during processing

    async def get_data(self) -> Dict[str, pd.Series]:
        """Get raw data from the data source"""
        if self.data_source is None:
            raise ValueError("Data source not provided")
        return await self.data_source.get_data(self.config)

    async def process_data(self):
        """Process data based on configuration"""
        raise NotImplementedError("Subclasses must implement this method")

    def _load_graph_data(self):
        """Load and process graph data with filtering for available sensors"""
        from gnn_package.src.preprocessing import (
            load_graph_data,
            compute_adjacency_matrix,
        )

        # Load graph data
        adj_matrix, node_ids, _ = load_graph_data(
            prefix=self.config.data.general.graph_prefix, return_df=False
        )

        # Only filter if we have resampled_data
        if self.resampled_data is not None:
            # Get set of sensors in resampled data
            available_sensors = set(self.resampled_data.keys())
            valid_indices = [
                i for i, node_id in enumerate(node_ids) if node_id in available_sensors
            ]

            print(
                f"Found {len(valid_indices)} nodes that match available sensors (out of {len(node_ids)})"
            )

            if len(valid_indices) < len(node_ids):
                # Filter the adjacency matrix and node_ids
                print("Filtering adjacency matrix to match available sensors...")
                import numpy as np

                filtered_node_ids = [node_ids[i] for i in valid_indices]
                filtered_adj_matrix = adj_matrix[np.ix_(valid_indices, valid_indices)]

                print(f"Filtered adjacency matrix shape: {filtered_adj_matrix.shape}")
                print(f"Filtered node_ids length: {len(filtered_node_ids)}")

                # Replace with filtered versions
                adj_matrix = filtered_adj_matrix
                node_ids = filtered_node_ids

        # Compute weighted adjacency
        weighted_adj = compute_adjacency_matrix(adj_matrix, config=self.config)

        return weighted_adj, node_ids


class TrainingDataProcessor(BaseDataProcessor):
    """Processor for training data with complex splitting"""

    async def process_data(self):
        """Process data for training with full validation splits"""
        print("TrainingDataProcessor.process_data: Starting...")

        try:
            # Get raw data
            print("Fetching raw data...")
            raw_data = await self.get_data()
            print(f"Raw data fetched: {type(raw_data)}")
            if not raw_data:
                print("WARNING: Raw data is empty or None!")
                return None

            # Resample data
            print("Resampling data...")
            resampled_data = resample_sensor_data(raw_data, config=self.config)
            print(f"Resampled data: {type(resampled_data)}")

            # Extract stats if they exist, and store them for later access
            self.preprocessing_stats = {"standardization": {}}
            if isinstance(resampled_data, dict) and "__stats__" in resampled_data:
                self.preprocessing_stats["standardization"] = resampled_data[
                    "__stats__"
                ]
                # Remove the stats from the main dictionary so it doesn't interfere with later processing
                stats = resampled_data.pop("__stats__")
                print(f"Extracted standardization stats: {stats}")

            # Process data with appropriate splitting
            print("Creating TimeSeriesPreprocessor...")
            processor = TimeSeriesPreprocessor(config=self.config)

            # Use the appropriate split method based on config
            split_method = self.config.data.training.split_method
            print(f"Using split method: {split_method}")

            if split_method == "time_based":
                print("Creating time-based split...")
                split_data = processor.create_time_based_split(
                    resampled_data, config=self.config
                )
            elif split_method == "rolling_window":
                print("Creating rolling window splits...")
                split_data = processor.create_rolling_window_splits(
                    resampled_data, config=self.config
                )
            else:
                raise ValueError(f"Unknown split method: {split_method}")

            print(f"Split data created: {type(split_data)}")
            if split_data is None or not split_data:
                print("WARNING: Split data is empty or None!")
                return None

            # Continue with window creation
            print("Creating windows for training and validation...")

            # Use the first split (or only split if time-based)
            if not isinstance(split_data, list) or not split_data:
                print("ERROR: split_data is not a proper list of splits")
                return None

            # Get the first split
            first_split = split_data[0]
            print(f"Using split with keys: {first_split.keys()}")

            # Create windows for training data
            print("Creating training windows...")
            X_train, masks_train, _ = processor.create_windows_from_grid(
                first_split["train"], config=self.config
            )

            # Create windows for validation data
            print("Creating validation windows...")
            X_val, masks_val, _ = processor.create_windows_from_grid(
                first_split["val"], config=self.config
            )

            print("Loading graph data...")

            # Store resampled data for filtering in _load_graph_data
            self.resampled_data = resampled_data
            adj_matrix, node_ids = self._load_graph_data()

            # Debug info
            print(
                f"Loaded graph with {len(node_ids)} nodes, adjacency matrix shape: {adj_matrix.shape}"
            )
            print(f"Resampled data has {len(resampled_data)} sensors")

            # Check for format consistency
            print(f"First few resampled data keys: {list(resampled_data.keys())[:5]}")
            print(f"First few node_ids: {node_ids[:5]}")

            # Convert node IDs to consistent format if needed
            sample_data_key = next(iter(resampled_data.keys()))
            if type(sample_data_key) != type(node_ids[0]):
                print(
                    f"Converting node IDs from {type(node_ids[0])} to {type(sample_data_key)}"
                )
                if isinstance(sample_data_key, str):
                    node_ids = [str(nid) for nid in node_ids]
                elif isinstance(sample_data_key, int):
                    node_ids = [int(nid) for nid in node_ids]

            # Get set of sensors in resampled data
            available_sensors = set(resampled_data.keys())
            valid_indices = [
                i for i, node_id in enumerate(node_ids) if node_id in available_sensors
            ]

            print(
                f"Found {len(valid_indices)} nodes that match available sensors (out of {len(node_ids)})"
            )

            if len(valid_indices) < len(node_ids):
                # Filter the adjacency matrix and node_ids
                print("Filtering adjacency matrix to match available sensors...")
                import numpy as np

                filtered_node_ids = [node_ids[i] for i in valid_indices]
                filtered_adj_matrix = adj_matrix[np.ix_(valid_indices, valid_indices)]

                print(f"Filtered adjacency matrix shape: {filtered_adj_matrix.shape}")
                print(f"Filtered node_ids length: {len(filtered_node_ids)}")

                # Replace with filtered versions
                adj_matrix = filtered_adj_matrix
                node_ids = filtered_node_ids

            # Create data loaders
            print("Creating data loaders...")

            train_loader = create_dataloader(
                X_train,
                masks_train,
                adj_matrix,
                node_ids,
                self.config.data.general.window_size,
                self.config.data.general.horizon,
                self.config.data.general.batch_size,
                shuffle=True,
            )

            val_loader = create_dataloader(
                X_val,
                masks_val,
                adj_matrix,
                node_ids,
                self.config.data.general.window_size,
                self.config.data.general.horizon,
                self.config.data.general.batch_size,
                shuffle=False,
            )

            # Now create the result dictionary
            result = {
                "data_loaders": {
                    "train_loader": train_loader,
                    "val_loader": val_loader,
                },
                "graph_data": {
                    "adj_matrix": adj_matrix,
                    "node_ids": node_ids,
                },
                "time_series": {
                    "validation": resampled_data,  # Or appropriate validation data
                    "input": None,  # Not needed for training
                },
                "metadata": {
                    "preprocessing_stats": self.preprocessing_stats,
                    "mode": "training",
                },
            }
            print(f"Returning result with keys: {result.keys()}")
            return result

        except Exception as e:
            print(f"ERROR in TrainingDataProcessor.process_data: {e}")
            import traceback

            traceback.print_exc()
            return None  # Return None on error


class PredictionDataProcessor(BaseDataProcessor):
    """Processor for prediction data without complex validation splits"""

    async def process_data(self):
        """Process data for prediction with simple holdout for last few points"""

        # Get raw data
        raw_data = await self.get_data()

        # Resample data
        resampled_data = resample_sensor_data(raw_data, config=self.config)

        # Extract stats if they exist, and store them for later access
        self.preprocessing_stats = {"standardization": {}}
        if "__stats__" in resampled_data:
            self.preprocessing_stats["standardization"] = resampled_data["__stats__"]
            # Remove the stats from the main dictionary so it doesn't interfere with later processing
            stats = resampled_data.pop("__stats__")

        # Create simple validation split (last horizon points)
        horizon = self.config.data.general.horizon

        # For each sensor, hold out the last 'horizon' points for validation
        validation_dict = {}
        input_dict = {}

        for node_id, series in resampled_data.items():
            if len(series) > horizon:
                # Keep full series for validation purposes
                validation_dict[node_id] = series
                # Use shortened series for prediction input
                input_dict[node_id] = series[:-horizon]
            else:
                # Not enough data - use same data for both but note the limitation
                validation_dict[node_id] = series
                input_dict[node_id] = series

        # Process the input data to create windows
        processor = TimeSeriesPreprocessor(config=self.config)
        X_input, masks_input, _ = processor.create_windows_from_grid(
            input_dict, config=self.config
        )

        # Store resampled data for filtering in _load_graph_data
        self.resampled_data = resampled_data

        # Load graph data
        adj_matrix, node_ids = self._load_graph_data()

        # Create dataloader for prediction (just validation, no training)
        val_loader = create_dataloader(
            X_input,
            masks_input,
            adj_matrix,
            node_ids,
            self.config.data.general.window_size,
            self.config.data.general.horizon,
            self.config.data.general.batch_size,
            shuffle=False,
        )

        result = {
            "data_loaders": {
                "val_loader": val_loader,
                # No train_loader needed
            },
            "graph_data": {
                "adj_matrix": adj_matrix,
                "node_ids": node_ids,
            },
            "time_series": {
                "validation": validation_dict,
                "input": input_dict,
            },
            "metadata": {
                "preprocessing_stats": self.preprocessing_stats,
                "mode": "prediction",
            },
        }
        return result




================================================
File: src/dataloaders/__init__.py
================================================
from .dataloaders import create_dataloader, SpatioTemporalDataset, collate_fn

__all__ = [
    "create_dataloader",
    "SpatioTemporalDataset",
    "collate_fn",
]



================================================
File: src/dataloaders/dataloaders.py
================================================
# gnn_package/src/preprocessing/dataloaders.py

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader


class SpatioTemporalDataset(Dataset):
    def __init__(
        self,
        X_by_sensor,
        masks_by_sensor,
        adj_matrix,
        node_ids,
        window_size,
        horizon,
    ):
        """
        Parameters:
        ----------
        X_by_sensor : Dict[str, np.ndarray]
            Dictionary containing the input data for each sensor.
        masks_by_sensor : Dict[str, np.ndarray]
            Dictionary containing the masks for each sensor.
        adj_matrix : np.ndarray
            Adjacency matrix of the graph.
        node_ids : List[str]
            List of node IDs.
        window_size : int
            Size of the input window.
        horizon : int
            Number of time steps to predict ahead.
        """
        self.X_by_sensor = X_by_sensor
        self.masks_by_sensor = masks_by_sensor
        self.adj_matrix = torch.FloatTensor(adj_matrix)
        self.node_ids = node_ids
        self.window_size = window_size
        self.horizon = horizon

        # Create flattened index mapping (node_id, window_idx)
        self.sample_indices = []
        for node_id in self.node_ids:
            if node_id in X_by_sensor:
                windows = X_by_sensor[node_id]
                for window_idx in range(len(windows)):
                    self.sample_indices.append((node_id, window_idx))

        print(
            f"Created dataset with {len(self.sample_indices)} total samples across {len(node_ids)} nodes"
        )

    def __len__(self):
        """Return the number of windows (time steps)."""
        # Find the sensor with the minimum number of windows
        min_windows = min(len(windows) for windows in self.X_by_sensor.values())
        return min_windows

    # Works with the original TimeSeriesPreprocessor segmented windows
    # def __getitem__(self, idx):
    #     # Get the node_id and window_idx for this sample
    #     node_id, window_idx = self.sample_indices[idx]

    #     # Get node index in adjacency matrix
    #     node_idx = self.node_ids.index(node_id)

    #     # Get input window (history) and target window (future)
    #     x_window = self.X_by_sensor[node_id][
    #         window_idx, : self.window_size - self.horizon
    #     ]
    #     x_mask = self.masks_by_sensor[node_id][
    #         window_idx, : self.window_size - self.horizon
    #     ]

    #     y_window = self.X_by_sensor[node_id][window_idx, -self.horizon :]
    #     y_mask = self.masks_by_sensor[node_id][window_idx, -self.horizon :]

    #     return {
    #         "x": torch.FloatTensor(x_window),
    #         "x_mask": torch.FloatTensor(x_mask),
    #         "y": torch.FloatTensor(y_window),
    #         "y_mask": torch.FloatTensor(y_mask),
    #         "node_idx": node_idx,
    #         "adj": self.adj_matrix,
    #     }

    def __getitem__(self, idx):
        """
        Get data for window index idx across all sensors.

        Returns all sensors' data for this window to represent a system snapshot.
        """
        # idx now represents a window index, not a (node_id, window_idx) pair
        window_idx = idx

        # Create tensors for all nodes at this window idx
        x_windows = []
        x_masks = []
        y_windows = []
        y_masks = []
        node_indices = []

        for i, node_id in enumerate(self.node_ids):
            if node_id in self.X_by_sensor and window_idx < len(
                self.X_by_sensor[node_id]
            ):
                # Get input window and masks
                x_window = self.X_by_sensor[node_id][
                    window_idx, : self.window_size - self.horizon
                ]
                x_mask = self.masks_by_sensor[node_id][
                    window_idx, : self.window_size - self.horizon
                ]

                # Get target window and masks
                y_window = self.X_by_sensor[node_id][window_idx, -self.horizon :]
                y_mask = self.masks_by_sensor[node_id][window_idx, -self.horizon :]

                x_windows.append(torch.FloatTensor(x_window))
                x_masks.append(torch.FloatTensor(x_mask))
                y_windows.append(torch.FloatTensor(y_window))
                y_masks.append(torch.FloatTensor(y_mask))
                node_indices.append(i)

        # Stack into tensors [num_nodes, seq_len]
        x = torch.stack(x_windows)
        x_mask = torch.stack(x_masks)
        y = torch.stack(y_windows)
        y_mask = torch.stack(y_masks)

        return {
            "x": x,
            "x_mask": x_mask,
            "y": y,
            "y_mask": y_mask,
            "node_indices": torch.tensor(node_indices),
            "adj": self.adj_matrix,
        }


def collate_fn(batch):
    """
    Custom collate function for batching system snapshots.
    Each item in the batch already contains all sensors for a specific time window.
    """
    # Extract tensors from batch
    x = torch.stack([item["x"] for item in batch])
    x_mask = torch.stack([item["x_mask"] for item in batch])
    y = torch.stack([item["y"] for item in batch])
    y_mask = torch.stack([item["y_mask"] for item in batch])

    # Use the first item's adjacency matrix and node indices
    adj = batch[0]["adj"]
    node_indices = batch[0]["node_indices"]

    return {
        "x": x,  # [batch_size, num_nodes, seq_len, 1]
        "x_mask": x_mask,  # [batch_size, num_nodes, seq_len, 1]
        "y": y,  # [batch_size, num_nodes, horizon, 1]
        "y_mask": y_mask,  # [batch_size, num_nodes, horizon, 1]
        "node_indices": node_indices,
        "adj": adj,
    }


def create_dataloader(
    X_by_sensor,
    masks_by_sensor,
    adj_matrix,
    node_ids,
    window_size,
    horizon,
    batch_size,
    shuffle,
):
    """
    Create a DataLoader that can handle varying numbers of windows per sensor.
    """
    dataset = SpatioTemporalDataset(
        X_by_sensor, masks_by_sensor, adj_matrix, node_ids, window_size, horizon
    )

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        collate_fn=collate_fn,
    )

    return dataloader




================================================
File: src/models/__init__.py
================================================
from .stgnn import STGNN, STGNNTrainer, create_stgnn_model

__all__ = ["STGNN", "STGNNTrainer", "create_stgnn_model"]



================================================
File: src/models/stgnn.py
================================================
# gnn_package/src/models/stgnn.py

import torch
import torch.nn as nn
import torch.nn.functional as F
import logging

from gnn_package.config import get_config

logger = logging.getLogger(__name__)


class GraphConvolution(nn.Module):
    def __init__(self, config, layer_id, in_features, out_features, bias=True):
        """
        Initialize the GraphConvolution layer with explicit parameters.

        Parameters:
        -----------
        config : ExperimentConfig
            Configuration object containing global settings
        layer_id : int
            Identifier for this layer
        in_features : int
            Number of input features
        out_features : int
            Number of output features
        bias : bool
            Whether to include bias term (this can remain a default)
        """
        super(GraphConvolution, self).__init__()

        # Store parameters without defaults
        self.in_features = in_features
        self.out_features = out_features
        self.layer_id = layer_id

        # Get required values from config
        self.use_self_loop = config.model.use_self_loops
        self.normalization = config.model.gcn_normalization
        self.missing_value = config.data.general.missing_value

        # Define learnable parameters
        self.weight = nn.Parameter(
            torch.FloatTensor(self.in_features, self.out_features)
        )
        if bias:
            self.bias = nn.Parameter(torch.FloatTensor(self.out_features))
        else:
            self.register_parameter("bias", None)

        # Initialize parameters
        self.reset_parameters()

    def reset_parameters(self):
        """Initialize weights using Glorot initialization"""
        nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            nn.init.zeros_(self.bias)

    def forward(self, x, adj, mask=None):
        """
        x: Node features [batch_size, num_nodes, in_features] or [batch_size, in_features]
        adj: Adjacency matrix [num_nodes, num_nodes]
        mask: Mask for valid values [batch_size, num_nodes, 1] or [batch_size, 1]

        Returns:
        --------
        Tensor of shape [batch_size, num_nodes, out_features]
        """
        # Create a binary mask where 1 = valid data, 0 = missing data
        missing_mask = (x != self.missing_value).float()

        # Apply the mask and replace missing values with zeros for computation
        x_masked = x * missing_mask

        # If a separate mask is provided, combine it with the missing mask
        if mask is not None:
            combined_mask = missing_mask * mask
        else:
            combined_mask = missing_mask

        # Check if we're dealing with batched input
        is_batched = len(x.shape) == 3

        if is_batched:
            batch_size, num_nodes, in_features = x.shape
        else:
            num_nodes, in_features = x.shape

        # Check that input features match weight dimensions
        if in_features != self.in_features:
            raise ValueError(
                f"Input features ({in_features}) don't match weight dimensions ({self.in_features})"
            )

        # Check that adjacency matrix dimensions match num_nodes
        if adj.shape[0] != num_nodes:
            raise ValueError(
                f"Adjacency matrix dimension ({adj.shape[0]}) doesn't match number of nodes ({num_nodes})"
            )

        # Add identity to allow self-loops
        adj_with_self = adj + torch.eye(adj.size(0), device=adj.device)

        # Normalize adjacency matrix
        rowsum = adj_with_self.sum(dim=1)
        d_inv_sqrt = torch.pow(rowsum, -0.5)
        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.0
        d_mat_inv_sqrt = torch.diag(d_inv_sqrt)
        normalized_adj = torch.matmul(
            torch.matmul(d_mat_inv_sqrt, adj_with_self), d_mat_inv_sqrt
        )

        # Transform node features differently depending on whether we have batched input
        if is_batched:
            # Handle batched data - need to process each batch separately
            outputs = []

            for b in range(batch_size):
                # Extract features for this batch
                batch_features = x_masked[b]  # [num_nodes, in_features]

                # Transform node features
                batch_support = torch.matmul(
                    batch_features, self.weight
                )  # [num_nodes, out_features]

                # Propagate using normalized adjacency
                batch_output = torch.matmul(
                    normalized_adj, batch_support
                )  # [num_nodes, out_features]

                # Add to outputs
                outputs.append(batch_output)

            # Stack back to batched tensor
            output = torch.stack(
                outputs, dim=0
            )  # [batch_size, num_nodes, out_features]

            # Re-apply mask
            if mask is not None:
                output = output * combined_mask
        else:
            # Transform node features
            support = torch.matmul(x_masked, self.weight)  # [num_nodes, out_features]

            # Propagate using normalized adjacency
            output = torch.matmul(normalized_adj, support)  # [num_nodes, out_features]

            # Re-apply mask
            if mask is not None:
                output = output * combined_mask

        # Add bias if needed
        if self.bias is not None:
            return output + self.bias
        else:
            return output


class AttentionLayer(nn.Module):
    """
    Attention layer to focus on most relevant nodes and timestamps.
    """

    def __init__(self, input_dim):
        super(AttentionLayer, self).__init__()
        self.attention = nn.Linear(input_dim, 1)

    def forward(self, x, mask=None):
        """
        x: Input tensor [batch_size, seq_len/num_nodes, features]
        mask: Binary mask [batch_size, seq_len/num_nodes, 1]
        """
        # Calculate attention scores
        attention_scores = self.attention(x)  # [batch_size, seq_len/num_nodes, 1]

        # Apply mask if provided (set scores to a large negative value)
        if mask is not None:
            # Convert -1 values to mask
            if len(mask.shape) == len(x.shape):
                mask = (mask != -1).float() * (x != -1).float()
            else:
                mask = (x != -1).float()

            # Set masked positions to large negative value
            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)

        # Apply softmax to get attention weights
        attention_weights = F.softmax(attention_scores, dim=1)

        # Apply attention to input
        context = torch.sum(x * attention_weights, dim=1)

        return context, attention_weights


class TemporalGCN(nn.Module):
    """
    Temporal Graph Convolutional Network with attention for missing data.
    """

    def __init__(self, config, input_dim, hidden_dim, output_dim):
        """
        Initialize the TemporalGCN with explicit parameters.

        Parameters:
        -----------
        config : ExperimentConfig
            Configuration object
        input_dim : int
            Input feature dimension
        hidden_dim : int
            Hidden layer dimension
        output_dim : int
            Output feature dimension
        """
        super(TemporalGCN, self).__init__()

        # Store parameters without defaults
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim

        # Get values from config
        num_layers = config.model.num_layers
        dropout = config.model.dropout
        num_gc_layers = config.model.num_gc_layers  # This should be required in config

        # Graph Convolutional layers
        self.gc_layers = nn.ModuleList()

        # First layer (input_dim to hidden_dim)
        self.gc_layers.append(
            GraphConvolution(
                config=config,
                layer_id=0,
                in_features=self.input_dim,
                out_features=self.hidden_dim,
            )
        )

        # Additional layers (hidden_dim to hidden_dim)
        for i in range(1, num_gc_layers):
            self.gc_layers.append(
                GraphConvolution(
                    config=config,
                    layer_id=i,
                    in_features=self.hidden_dim,
                    out_features=self.hidden_dim,
                )
            )

        # Attention layers
        self.node_attention = AttentionLayer(hidden_dim)
        self.temporal_attention = AttentionLayer(hidden_dim)

        # Recurrent layer for temporal patterns
        self.gru = nn.GRU(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
        )

        # Output layer
        self.fc_out = nn.Linear(hidden_dim, output_dim)

        # Dropout for regularization
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, adj, mask=None):
        """
        x: Node features [batch_size, num_nodes, seq_len, input_dim]
        adj: Adjacency matrix [num_nodes, num_nodes]
        mask: Mask for valid values [batch_size, num_nodes, seq_len, input_dim] or [batch_size, num_nodes, seq_len]
        """
        batch_size, num_nodes, seq_len, features = x.size()

        # Handle mask dimensions
        if mask is not None:
            # If mask has 3 dimensions [batch, nodes, seq_len], expand to 4
            if len(mask.shape) == 3:
                mask = mask.unsqueeze(-1)  # Add feature dimension

                # If we need to match multiple features
                if features > 1:
                    mask = mask.expand(-1, -1, -1, features)

        # Process each time step through the GCN layers
        outputs = []

        for t in range(seq_len):
            # Get features at this time step
            x_t = x[:, :, t, :]  # [batch_size, num_nodes, features]

            # Create mask for this timestep
            if mask is not None:
                mask_t = mask[:, :, t, :]  # [batch_size, num_nodes, features]
            else:
                mask_t = None

            # Apply GCN layers
            for gc_layer in self.gc_layers:
                x_t = gc_layer(x_t, adj, mask_t)
                x_t = F.relu(x_t)  # Activation
                x_t = self.dropout(x_t)  # Apply dropout

            # Store the processed features for this timestep
            outputs.append(x_t)  # [batch_size, num_nodes, hidden_dim]

        # Stack outputs along time dimension
        # This gives us [batch_size, num_nodes, seq_len, hidden_dim]
        temporal_features = torch.stack(outputs, dim=2)

        # Process each node's temporal sequence with GRU
        node_outputs = []

        for n in range(num_nodes):
            # Get temporal data for this node across all batches
            # Shape: [batch_size, seq_len, hidden_dim]
            node_temporal_data = temporal_features[:, n, :, :]

            # Pass through GRU
            # Output shape: [batch_size, seq_len, hidden_dim]
            node_gru_out, _ = self.gru(node_temporal_data)

            # Add to collected outputs
            node_outputs.append(node_gru_out)

        # Stack back to full tensor
        # Shape: [batch_size, num_nodes, seq_len, hidden_dim]
        gru_output = torch.stack(node_outputs, dim=1)

        # Apply final FC layer for output
        # Shape: [batch_size, num_nodes, seq_len, output_dim]
        out = self.fc_out(gru_output)

        # Apply mask if provided to ensure missing values stay missing
        if mask is not None:
            # Ensure mask has right shape
            if mask.shape[-1] == 1 and out.shape[-1] > 1:
                # Expand last dimension if needed
                mask = mask.expand(-1, -1, -1, out.shape[-1])

            # Apply mask
            out = out * mask

        return out


class STGNN(nn.Module):
    """
    Spatio-Temporal Graph Neural Network with attention for traffic prediction
    """

    def __init__(self, config):
        """
        Initialize STGNN model with configuration.

        Parameters:
        -----------
        config : ExperimentConfig
            Configuration object
        """
        super(STGNN, self).__init__()

        # Get required parameters from config
        input_dim = config.model.input_dim
        hidden_dim = config.model.hidden_dim
        output_dim = config.model.output_dim
        horizon = config.data.general.horizon
        dropout = config.model.dropout
        decoder_layers = (
            config.model.decoder_layers
        )  # This should be required in config

        self.horizon = horizon

        # Encoder: process historical data
        self.encoder = TemporalGCN(
            config=config,
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            output_dim=hidden_dim,
        )

        if decoder_layers == 1:
            # If only one layer, use a simple linear layer
            self.decoder = nn.Linear(hidden_dim, output_dim * horizon)
        else:
            decoder_layers_list = []
            decoder_layers_list.append(nn.Linear(hidden_dim, hidden_dim))
            decoder_layers_list.append(nn.ReLU())
            decoder_layers_list.append(nn.Dropout(dropout))
            decoder_layers_list.append(nn.Linear(hidden_dim, output_dim * horizon))
            self.decoder = nn.Sequential(*decoder_layers_list)

    def forward(self, x, adj, x_mask=None):
        """
        x: Input features [batch_size, num_nodes, seq_len, input_dim]
        adj: Adjacency matrix [num_nodes, num_nodes]
        x_mask: Mask for input [batch_size, num_nodes, seq_len, input_dim]

        Returns:
        Predictions [batch_size, num_nodes, horizon, output_dim]
        """
        # Check for proper shape
        assert len(x.shape) == 4, f"Expected 4D input but got shape {x.shape}"

        batch_size, num_nodes, seq_len, _ = x.size()

        # Encode the input sequence
        # Output shape: [batch_size, num_nodes, seq_len, hidden_dim]
        encoded = self.encoder(x, adj, x_mask)

        # Use the last time step for each node to predict future
        # Shape: [batch_size, num_nodes, hidden_dim]
        last_hidden = encoded[:, :, -1, :]

        # Predict future values
        # Shape: [batch_size, num_nodes, output_dim * horizon]
        future_flat = self.decoder(last_hidden)

        # Reshape to separate time steps
        # Shape: [batch_size, num_nodes, horizon, output_dim]
        predictions = future_flat.reshape(batch_size, num_nodes, self.horizon, -1)

        return predictions


class STGNNTrainer:
    def __init__(self, model, config):
        """
        Initialize the trainer with the model and config.

        Parameters:
        -----------
        model : STGNN
            The model to train
        config : ExperimentConfig
            Configuration object
        """
        # Get device from config or auto-detect
        device_name = getattr(config.training, "device", None)
        if device_name:
            device = torch.device(device_name)
        else:
            if torch.backends.mps.is_available():
                device = torch.device("mps")
            elif torch.cuda.is_available():
                device = torch.device("cuda")
            else:
                device = torch.device("cpu")

        logger.info(f"Using device: {device}")

        # Create optimizer based on config
        learning_rate = config.training.learning_rate
        weight_decay = config.training.weight_decay

        optimizer = torch.optim.Adam(
            model.parameters(), lr=learning_rate, weight_decay=weight_decay
        )

        # Use MSE loss
        criterion = torch.nn.MSELoss(reduction="none")

        self.model = model.to(device)
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        self.config = config

        logger.info(
            f"STGNNTrainer.__init__(): Model inistialized with {sum(p.numel() for p in model.parameters())} parameters"
        )
        logger.info(f"STGNNTrainer.__init__(): Optimizer: {optimizer}")
        logger.info(f"STGNNTrainer.__init__(): Loss function: {criterion}")

    def train_epoch(self, dataloader):
        """Train for one epoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0

        for batch in dataloader:
            # Move data to device
            x = batch["x"].to(self.device)
            x_mask = batch["x_mask"].to(self.device)
            y = batch["y"].to(self.device)
            y_mask = batch["y_mask"].to(self.device)
            adj = batch["adj"].to(self.device)

            # Forward pass
            self.optimizer.zero_grad()
            y_pred = self.model(x, adj, x_mask)

            # Compute loss on valid points only
            loss = self.criterion(y_pred, y)
            if y_mask is not None:
                # Count non-zero elements in mask
                mask_sum = y_mask.sum()
                if mask_sum > 0:
                    loss = (loss * y_mask).sum() / mask_sum
                else:
                    loss = torch.tensor(0.0, device=self.device)

            # Backward pass
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        return total_loss / max(1, num_batches)

    def evaluate(self, dataloader):
        """Evaluate the model on a validation or test set"""
        self.model.eval()
        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            for batch in dataloader:
                # Move data to device
                x = batch["x"].to(self.device)
                x_mask = batch["x_mask"].to(self.device)
                y = batch["y"].to(self.device)
                y_mask = batch["y_mask"].to(self.device)
                adj = batch["adj"].to(self.device)

                # Forward pass
                y_pred = self.model(x, adj, x_mask)

                # Compute loss on valid points only
                loss = self.criterion(y_pred, y)
                if y_mask is not None:
                    # Count non-zero elements in mask
                    mask_sum = y_mask.sum()
                    if mask_sum > 0:
                        loss = (loss * y_mask).sum() / mask_sum
                    else:
                        loss = torch.tensor(0.0, device=self.device)

                total_loss += loss.item()
                num_batches += 1

        return total_loss / max(1, num_batches)


def create_stgnn_model(config):
    """
    Create a Spatio-Temporal GNN model with parameters from configuration.

    Parameters:
    -----------
    config : ExperimentConfig
        Configuration object

    Returns:
    --------
    STGNN
        Configured model instance
    """
    # Validate that all required configuration parameters are present
    try:
        # This will raise an error if any required parameter is missing
        config.validate()
    except ValueError as e:
        logger.error(f"Invalid configuration: {e}")
        raise

    # Log the configuration being used
    logger.info(f"Creating STGNN model with config from {config.config_path}")
    logger.info(f"  input_dim: {config.model.input_dim}")
    logger.info(f"  hidden_dim: {config.model.hidden_dim}")
    logger.info(f"  output_dim: {config.model.output_dim}")
    logger.info(f"  horizon: {config.data.general.horizon}")
    logger.info(f"  num_layers: {config.model.num_layers}")
    logger.info(f"  num_gc_layers: {config.model.num_gc_layers}")
    logger.info(f"  decoder_layers: {config.model.decoder_layers}")
    logger.info(f"  dropout: {config.model.dropout}")

    # Create model with config
    model = STGNN(config=config)

    return model




================================================
File: src/preprocessing/__init__.py
================================================
# gnn_package/src/preprocessing/__init__.py

from .fetch_sensor_data import (
    fetch_and_save_sensor_data,
    load_sensor_data,
)

from .graph_utils import (
    get_street_network_gdfs,
    load_graph_data,
)
from .graph_manipulation import (
    snap_points_to_network,
    connect_components,
    create_adjacency_matrix,
)
from .graph_computation import (
    compute_adjacency_matrix,
)

from .timeseries_preprocessor import (
    TimeSeriesPreprocessor,
    resample_sensor_data,
    standardize_sensor_data,  # Add this line
)


__all__ = [
    "get_street_network_gdfs",
    "load_graph_data",
    "snap_points_to_network",
    "connect_components",
    "create_adjacency_matrix",
    "compute_adjacency_matrix",
    "TimeSeriesPreprocessor",
    "resample_sensor_data",
    "standardize_sensor_data",  # Add this line
    "fetch_and_save_sensor_data",
    "load_sensor_data",
]



================================================
File: src/preprocessing/fetch_sensor_data.py
================================================
# gnn_package/src/preprocessing/fetch_sensor_data.py

import pickle
from datetime import datetime, timedelta
import pandas as pd
import os
from private_uoapi import (
    LSConfig,
    LSAuth,
    LightsailWrapper,
    DateRangeParams,
    convert_to_dataframe,
)
from gnn_package.src.utils.sensor_utils import get_sensor_name_id_map
from gnn_package.config import get_config


def load_sensor_data(data_file):
    """
    Load sensor data from a pickle file.

    Parameters:
    -----------
    data_file : str
        Path to the pickle file

    Returns:
    --------
    dict
        Dictionary mapping sensor IDs to time series data

    Raises:
    -------
    FileNotFoundError
        If the data file doesn't exist
    """
    if os.path.exists(data_file):
        print(f"Loading sensor data from {data_file}")
        with open(data_file, "rb") as f:
            return pickle.load(f)
    else:
        raise FileNotFoundError(
            f"Sensor data file {data_file} not found. "
            f"Run fetch_sensor_data.py to create it."
        )


async def fetch_and_save_sensor_data(
    data_file,
    days_back=None,
    start_date=None,
    end_date=None,
    config=None,
):
    """
    Fetch sensor data from API and save to file.

    Parameters:
    -----------
    data_file : str
        Path where to save the data
    days_back : int, optional
        Number of days of historical data to fetch, overrides config if provided
    start_date, end_date : datetime, optional
        Specific date range to fetch, overrides days_back if provided
    config : ExperimentConfig, optional
        Centralized configuration object
    """
    # Get configuration
    if config is None:
        config = get_config()

    # Use parameter or config value
    if days_back is None:
        days_back = config.data.prediction.days_back

    print(f"Fetching sensor data from API")

    # Initialize API client
    config = LSConfig()
    auth = LSAuth(config)
    client = LightsailWrapper(config, auth)

    print(f"Using base URL: {config.base_url}")
    print(f"Using username: {config.username}")
    print(f"Using secret key: {'*' * len(config.secret_key)}")

    # Get sensor locations
    sensor_locations = client.get_traffic_sensors()
    sensor_locations = pd.DataFrame(sensor_locations)

    # Determine date range
    if start_date is None or end_date is None:
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)

    # Create date range parameters
    date_range_params = DateRangeParams(
        start_date=start_date,
        end_date=end_date,
        max_date_range=timedelta(days=400),
    )

    # Get sensor name to ID mapping
    name_id_map = get_sensor_name_id_map()

    # Fetch data
    count_data = await client.get_traffic_data(date_range_params)
    counts_df = convert_to_dataframe(count_data)

    # Create time series dictionary
    counts_dict = {}
    for location in sensor_locations["location"]:
        df = counts_df[counts_df["location"] == location]
        series = pd.Series(df["value"].values, index=df["dt"])
        location_id = name_id_map[location]
        counts_dict[location_id] = series if not df.empty else None

    # Filter out None values and remove duplicates
    results_containing_data = {}
    for node_id, data in counts_dict.items():
        if data is not None:
            data = data[~data.index.duplicated(keep="first")]
            results_containing_data[node_id] = data

    # Save to file
    with open(data_file, "wb") as f:
        pickle.dump(results_containing_data, f)

    print(f"Saved sensor data to {data_file}")
    return results_containing_data



================================================
File: src/preprocessing/graph_analysis.py
================================================
# gnn_package/src/preprocessing/graph_analysis.py
import networkx as nx
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point, box
import numpy as np


def analyze_network_graph(G):
    """
    Analyze the network properties.

    Parameters:
    G (networkx.MultiDiGraph): Network graph to analyze
    """
    print("\nAnalyzing network properties...")

    # Get the graph's CRS
    graph_crs = G.graph.get("crs", "Unknown")
    print(f"Network CRS: {graph_crs}")

    # Basic network statistics
    stats = {
        "Nodes": len(G.nodes()),
        "Edges": len(G.edges()),
        "Average node degree": np.mean([d for n, d in G.degree()]),
        "Network type": "Directed" if G.is_directed() else "Undirected",
    }

    # Print statistics
    print("\nNetwork Statistics:")
    for key, value in stats.items():
        print(f"{key}: {value}")

    # Calculate network area
    try:
        # Get network bounds
        nodes = pd.DataFrame(
            {
                "x": [G.nodes[node]["x"] for node in G.nodes()],
                "y": [G.nodes[node]["y"] for node in G.nodes()],
            }
        )

        # Create a polygon from the bounds
        bbox = box(
            nodes["x"].min(), nodes["y"].min(), nodes["x"].max(), nodes["y"].max()
        )

        # Since we're already in EPSG:27700 (British National Grid),
        # we can calculate the area directly
        area = bbox.area

        # Convert to km²
        area_km2 = area / 1_000_000  # Convert square meters to square kilometers

        print(f"\nNetwork area: {area_km2:.2f} km²")

        # Calculate network density
        network_length = sum(d.get("length", 0) for u, v, d in G.edges(data=True))

        density = network_length / area if area > 0 else 0
        print(f"Network density: {density:.2f} meters per square meter")

        # Add to stats
        stats.update(
            {
                "Area (km²)": area_km2,
                "Total network length (km)": network_length / 1000,
                "Network density (km/km²)": density,
            }
        )

    except Exception as e:
        print(f"\nWarning: Could not calculate network area: {str(e)}")

    # Additional network metrics
    try:
        # Average street length
        avg_street_length = np.mean(
            [d.get("length", 0) for u, v, d in G.edges(data=True)]
        )
        print(f"Average street segment length: {avg_street_length:.2f} meters")

        # Number of connected components
        if G.is_directed():
            n_components = nx.number_weakly_connected_components(G)
            print(f"Number of weakly connected components: {n_components}")
        else:
            n_components = nx.number_connected_components(G)
            print(f"Number of connected components: {n_components}")

        stats["Average segment length (m)"] = avg_street_length
        stats["Number of components"] = n_components

    except Exception as e:
        print(f"\nWarning: Could not calculate some network metrics: {str(e)}")

    return stats


def analyze_graph_components(G, snapped_points_gdf, tolerance=1e-6):
    """
    Analyze which components the snapped points belong to and verify network connectivity.

    Args:
        G: NetworkX graph (directed or undirected)
        snapped_points_gdf: GeoDataFrame of snapped points
        tolerance: Distance tolerance for considering a point connected to the network

    Returns:
        GeoDataFrame with component information and connectivity status
    """
    print("\nAnalyzing network connectivity...")
    print(f"\nDetected a {'directed' if G.is_directed() else 'undirected'} graph.")

    # First verify if the graph is directed
    if G.is_directed():
        components = list(nx.weakly_connected_components(G))
    else:
        components = list(nx.connected_components(G))

    # Get all network nodes as Points using x and y coordinates from node attributes
    network_nodes = {}
    for node in G.nodes():
        # Get coordinates from node attributes
        node_data = G.nodes[node]
        if "x" in node_data and "y" in node_data:
            coords = (node_data["x"], node_data["y"])
            network_nodes[node] = Point(coords)
        else:
            # If node is already a coordinate tuple
            try:
                if isinstance(node, (tuple, list)) and len(node) >= 2:
                    network_nodes[node] = Point(node)
            except Exception:
                print(f"Warning: Could not get coordinates for node {node}")
                continue

    # Create a mapping of nodes to their component index
    node_to_component = {}
    for i, component in enumerate(components):
        for node in component:
            node_to_component[node] = i

    # Check each snapped point
    point_components = []
    unconnected_points = []

    for idx, point in snapped_points_gdf.iterrows():
        coords = tuple(round(x, 6) for x in (point.geometry.x, point.geometry.y))

        # Find the closest network node and its component
        min_dist = float("inf")
        closest_node = None
        component_idx = None

        for node, node_point in network_nodes.items():
            dist = point.geometry.distance(node_point)
            if dist < min_dist:
                min_dist = dist
                closest_node = node
                component_idx = node_to_component.get(node, -1)

        # Check if the point is connected (within tolerance)
        if min_dist <= tolerance:
            if min_dist > 0:  # Only print warning if not exact match
                print(
                    f"Warning: Point {point.original_id} was not exactly on network node but within {min_dist:.6f} units of node {closest_node}."
                )
        else:
            component_idx = -1
            unconnected_points.append(
                {
                    "original_id": point.original_id,
                    "coords": coords,
                    "min_distance": min_dist,
                }
            )

        point_components.append(
            {
                "original_id": point.original_id,
                "component": component_idx,
                "geometry": point.geometry,
                "connected": component_idx != -1,
                "distance_to_network": min_dist,
            }
        )

    # Create new GeoDataFrame with component information
    result_gdf = gpd.GeoDataFrame(point_components, crs=snapped_points_gdf.crs)

    # Print summary statistics
    print("\nNetwork Connectivity Analysis:")
    print(f"Total points: {len(result_gdf)}")
    print(f"Connected points: {sum(result_gdf['connected'])}")
    print(f"Unconnected points: {sum(~result_gdf['connected'])}")

    if unconnected_points:
        print("\nWARNING: The following points are not connected to the network:")
        for p in unconnected_points:
            print(
                f"Point ID: {p['original_id']}: distance to nearest node = {p['min_distance']:.6f}"
            )

    print("\nPoints per component:")
    component_counts = result_gdf[result_gdf["connected"]]["component"].value_counts()
    print(component_counts)

    # Calculate average distance to network
    avg_distance = np.mean(result_gdf["distance_to_network"])
    max_distance = np.max(result_gdf["distance_to_network"])
    print(f"\nAverage distance to network: {avg_distance:.6f}")
    print(f"Maximum distance to network: {max_distance:.6f}")

    return result_gdf


def validate_snapped_points(snapped_points_gdf, network_gdf, tolerance=6):
    """
    Validate that all snapped points exist in the network.

    Parameters:
    snapped_points_gdf (GeoDataFrame): GeoDataFrame of snapped points
    network_gdf (GeoDataFrame): Network edges GeoDataFrame
    tolerance (int): Number of decimal places for coordinate rounding

    Returns:
    GeoDataFrame: Only the valid points that exist in network
    """
    # Get all network nodes from the edges
    network_nodes = set()
    for idx, row in network_gdf.iterrows():
        coords = list(row.geometry.coords)
        for coord in coords:
            network_nodes.add(tuple(round(x, tolerance) for x in coord))

    # Validate points
    valid_points = []
    invalid_points = []

    for idx, point in snapped_points_gdf.iterrows():
        coords = tuple(
            round(x, tolerance) for x in (point.geometry.x, point.geometry.y)
        )
        if coords in network_nodes:
            valid_points.append(point)
        else:
            invalid_points.append(point.original_id)
            print(f"Warning: Point {point.original_id} not found in network")

    # Print summary
    print("\nValidation Summary:")
    print(f"Total points checked: {len(snapped_points_gdf)}")
    print(f"Valid points: {len(valid_points)}")
    print(f"Invalid points: {len(invalid_points)}")

    if invalid_points:
        print("\nInvalid point IDs:", invalid_points)

    return gpd.GeoDataFrame(valid_points, crs=snapped_points_gdf.crs)



================================================
File: src/preprocessing/graph_computation.py
================================================
# gnn_package/src/preprocessing/graph_computation.py

import numpy as np
import networkx as nx
import geopandas as gpd
from shapely.geometry import Point, LineString
from itertools import combinations

from gnn_package.config import get_config


def compute_shortest_paths(
    network_gdf,
    snapped_points_gdf,
    config=None,
):
    """
    Compute shortest paths between all pairs of snapped points.
    Assumes points have been validated using validate_snapped_points().

    Parameters:
    network_gdf (GeoDataFrame): Network edges
    snapped_points_gdf (GeoDataFrame): Validated snapped sensor points
    tolerance (int): Number of decimal places for coordinate rounding

    Returns:
    GeoDataFrame: Shortest paths between points
    """
    if config is None:
        config = get_config()

    tolerance = config.data.general.tolerance_decimal_places

    # Create NetworkX graph from network GeoDataFrame
    G = nx.Graph()
    for idx, row in network_gdf.iterrows():
        coords = list(row.geometry.coords)
        for i in range(len(coords) - 1):
            start = tuple(round(x, tolerance) for x in coords[i])
            end = tuple(round(x, tolerance) for x in coords[i + 1])
            weight = Point(coords[i]).distance(Point(coords[i + 1]))
            G.add_edge(start, end, weight=weight)

    # Get point pairs with rounded coordinates
    point_coords = {
        row.original_id: tuple(
            round(x, tolerance) for x in (row.geometry.x, row.geometry.y)
        )
        for idx, row in snapped_points_gdf.iterrows()
    }

    point_pairs = list(combinations(point_coords.items(), 2))
    print(f"Attempting to find paths between {len(point_pairs)} pairs of points")

    # Compute paths
    paths = []
    total_pairs = len(point_pairs)
    failed_pairs = 0

    for i, ((id1, start_point), (id2, end_point)) in enumerate(point_pairs):
        if start_point == end_point:
            continue

        try:
            path_length = nx.shortest_path_length(
                G, start_point, end_point, weight="weight"
            )
            path = nx.shortest_path(G, start_point, end_point, weight="weight")
            path_line = LineString([Point(p) for p in path])

            paths.append(
                {
                    "start_id": id1,
                    "end_id": id2,
                    "geometry": path_line,
                    "path_length": path_length,
                    "n_points": len(path),
                }
            )

        except nx.NetworkXNoPath:
            failed_pairs += 1
            print(f"No path found between points {id1} and {id2}")
            continue

        if (i + 1) % 100 == 0:
            print(f"Processed {i + 1}/{total_pairs} pairs...")

    if paths:
        paths_gdf = gpd.GeoDataFrame(paths, crs=snapped_points_gdf.crs)
        paths_gdf = paths_gdf.sort_values("path_length")

        print("\nPath finding summary:")
        print(f"Total pairs attempted: {total_pairs}")
        print(f"Failed pairs: {failed_pairs}")
        print(f"Successful paths: {len(paths)}")

        return paths_gdf
    else:
        print("No valid paths found!")
        return None


def create_weighted_graph_from_paths(paths_gdf):
    """
    Create a NetworkX graph from shortest paths data where:
    - Nodes are sensor locations
    - Edges connect sensors with weights as path lengths

    Parameters:
    -----------
    paths_gdf : GeoDataFrame
        Contains shortest paths data with start_id, end_id, and path_length

    Returns:
    --------
    G : NetworkX Graph
        Undirected weighted graph of sensor connections
    """
    # Create new undirected graph
    G = nx.Graph()

    # Add edges with weights
    for idx, row in paths_gdf.iterrows():
        G.add_edge(
            row["start_id"],
            row["end_id"],
            weight=row["path_length"],
            n_points=row["n_points"],
        )

    # Print some basic statistics
    print(f"Graph Statistics:")
    print(f"Number of nodes: {G.number_of_nodes()}")
    print(f"Number of edges: {G.number_of_edges()}")
    print(
        f"Average path length: {np.mean([d['weight'] for (u, v, d) in G.edges(data=True)]):.2f} meters"
    )
    print(
        f"Min path length: {min([d['weight'] for (u, v, d) in G.edges(data=True)]):.2f} meters"
    )
    print(
        f"Max path length: {max([d['weight'] for (u, v, d) in G.edges(data=True)]):.2f} meters"
    )

    # Check if graph is connected
    is_connected = nx.is_connected(G)
    print(f"Graph is {'connected' if is_connected else 'not connected'}")

    if not is_connected:
        components = list(nx.connected_components(G))
        print(f"Number of connected components: {len(components)}")
        print(f"Sizes of components: {[len(c) for c in components]}")

    return G


def compute_adjacency_matrix(
    adj_matrix: np.ndarray,
    config=None,
) -> np.ndarray:
    """
    Computes a weighted adjacency matrix from a distance matrix using a Gaussian kernel function.
    The function first normalizes distances, then applies a Gaussian decay and thresholds weak connections.

    Parameters:
    -----------
    adj_matrix : np.ndarray
        Input matrix of distances between nodes
    sigma_squared : float, default=0.1
        Variance parameter that controls the rate of weight decay with distance.
        Smaller values cause weights to decay more quickly, while larger values
        preserve stronger long-range connections.
    epsilon : float, default=0.95
        Threshold for keeping connections. Any connection with weight below epsilon
        is removed (set to 0). For small geographical areas, a lower value like 0.5
        may be more appropriate to ensure connectivity.

    Returns:
    --------
    np.ndarray
        Weighted adjacency matrix where weights are computed using a Gaussian kernel
        function (e^(-d²/σ²)) and thresholded by epsilon. Self-connections (diagonal
        elements) are set to 0.

    Notes:
    ------
    - Distances are normalized by dividing by 10000 before computation
    - The Gaussian kernel means weights decay exponentially with squared distance
    - Higher epsilon values lead to sparser graphs as more weak connections are removed
    """
    # sigma_squared is the variance of the Gaussian kernel which controls how quickly the connection strength decays with distance
    # smaller sigma squared means weights decay more quickly with distance
    # epsilon is the threshold for the weights
    # a high value e.g. 0.95 means that only very strong connections are kept
    # for small areas epsilon=0.5 will likely be fully connected

    # Get configuration
    if config is None:
        config = get_config()

    sigma_squared = config.data.general.sigma_squared
    epsilon = config.data.general.epsilon
    normalization_factor = config.data.general.normalization_factor

    a = adj_matrix / normalization_factor  # Normalize distances
    a_squared = a * a  # Square distances
    n = a.shape[0]
    w_mask = np.ones([n, n]) - np.identity(n)  # Mask of ones except for the diagonal
    w = (
        np.exp(-a_squared / sigma_squared)
        * (np.exp(-a_squared / sigma_squared) >= epsilon)
        * w_mask
    )  # Test whether the weights are greater than epsilon, apply the mask, and multiply again to return real values of weights
    return w



================================================
File: src/preprocessing/graph_manipulation.py
================================================
# gnn_package/src/preprocessing/graph_manipulation.py

import numpy as np
import pandas as pd
import geopandas as gpd
import networkx as nx
from shapely.ops import nearest_points
from shapely.geometry import Point, LineString, MultiLineString
from itertools import combinations

from gnn_package.config import get_config


def snap_points_to_network(
    points_gdf,
    network_gdf,
    config=None,
):
    """
    Snap points to their nearest location on the network.

    Parameters:
    -----------
    points_gdf : GeoDataFrame
        GeoDataFrame containing points to snap
    network_gdf : GeoDataFrame
        Network edges as GeoDataFrame
    config : ExperimentConfig, optional
        Centralized configuration object. If not provided, will use global config.
    tolerance_decimal_places : int, optional
        Rounding tolerance for coordinate comparison, overrides config if provided

    Returns:
    --------
    GeoDataFrame
        Points snapped to nearest network vertices
    """

    # Get configuration
    if config is None:
        config = get_config()

    tolerance_decimal_places = config.data.general.tolerance_decimal_places

    # Create unified network geometry
    print("Creating unified network geometry...")
    network_unary = network_gdf.geometry.union_all()

    # Get all network vertices
    print("Extracting network vertices...")
    network_vertices = set()
    for geom in network_gdf.geometry:
        if isinstance(geom, LineString):
            network_vertices.update(
                [
                    tuple(round(x, tolerance_decimal_places) for x in coord)
                    for coord in geom.coords
                ]
            )

    print(f"Number of network vertices: {len(network_vertices)}")

    # Snap points to network
    snapped_points = []
    unsnapped_points = []

    for idx, point in points_gdf.iterrows():
        try:
            # Get the nearest point on the network
            nearest_geom = nearest_points(point.geometry, network_unary)[1]
            point_coord = (
                round(nearest_geom.x, tolerance_decimal_places),
                round(nearest_geom.y, tolerance_decimal_places),
            )

            # Find the closest network vertex
            min_dist = float("inf")
            closest_vertex = None

            for vertex in network_vertices:
                dist = Point(vertex).distance(Point(point_coord))
                if dist < min_dist:
                    min_dist = dist
                    closest_vertex = vertex

            if closest_vertex is None:
                print(
                    f"Warning: Could not find closest vertex for point {point.get('id', idx)}"
                )
                unsnapped_points.append(point.get("id", idx))
                continue

            # Create point record
            point_record = {
                "original_id": point.get("id", idx),
                "geometry": Point(closest_vertex),
                "snap_distance": min_dist,
            }

            # Add any additional attributes from the original points
            for col in points_gdf.columns:
                if col not in ["geometry", "id"]:
                    point_record[col] = point[col]

            snapped_points.append(point_record)

        except Exception as e:
            print(f"Error processing point {point.get('id', idx)}: {str(e)}")
            unsnapped_points.append(point.get("id", idx))

    # Create result GeoDataFrame
    result_gdf = gpd.GeoDataFrame(snapped_points, crs=points_gdf.crs)

    # Print summary
    print("\nSnapping Summary:")
    print(f"Total points processed: {len(points_gdf)}")
    print(f"Successfully snapped points: {len(snapped_points)}")
    print(f"Failed to snap points: {len(unsnapped_points)}")

    if unsnapped_points:
        print("\nPoints that failed to snap:", unsnapped_points)

    return result_gdf


def connect_components(edges_gdf, config=None):
    """
    Connect nearby components in the network using NetworkX for speed.

    Parameters:
    -----------
    edges_gdf : GeoDataFrame
        Network edges
    config : ExperimentConfig, optional
        Centralized configuration object. If not provided, will use global config.
    max_distance : float, optional
        Maximum distance to connect components, overrides config if provided

    Returns:
    --------
    GeoDataFrame
        Updated network edges with new connections
    """

    # Get configuration
    if config is None:
        config = get_config()

    max_distance = config.data.general.max_distance

    # First convert to NetworkX graph for faster component analysis
    G = nx.Graph()

    # Add edges from the GeoDataFrame
    for idx, row in edges_gdf.iterrows():
        coords = list(row.geometry.coords)
        for i in range(len(coords) - 1):
            start = coords[i]
            end = coords[i + 1]
            G.add_edge(start, end, geometry=row.geometry)

    # Get initial components
    components = list(nx.connected_components(G))
    print(f"Initial number of components: {len(components)}")

    # Track new connections
    new_connections = []
    connections_made = 0

    # Connect components using NetworkX for speed
    for i, comp1 in enumerate(components):
        comp1_list = list(comp1)

        for j, comp2 in enumerate(components[i + 1 :], i + 1):
            comp2_list = list(comp2)

            # Find closest pair of nodes between components
            min_dist = float("inf")
            closest_pair = None

            # Use numpy for vectorized distance calculation
            coords1 = np.array(comp1_list)
            coords2 = np.array(comp2_list)

            # Calculate pairwise distances using broadcasting
            distances = np.sqrt(
                np.sum(
                    (coords1[:, np.newaxis, :] - coords2[np.newaxis, :, :]) ** 2, axis=2
                )
            )
            min_idx = np.argmin(distances)
            min_dist = distances.flat[min_idx]

            if min_dist < max_distance:
                idx1, idx2 = np.unravel_index(min_idx, distances.shape)
                closest_pair = (tuple(coords1[idx1]), tuple(coords2[idx2]))

                # Add edge to graph and track new connection
                G.add_edge(*closest_pair)
                new_connections.append(
                    {
                        "geometry": LineString([closest_pair[0], closest_pair[1]]),
                        "length": min_dist,
                        "type": "connection",
                    }
                )
                connections_made += 1

                if connections_made % 10 == 0:
                    print(f"Made {connections_made} connections...")

    # Convert new connections to GeoDataFrame
    if new_connections:
        connections_gdf = gpd.GeoDataFrame(new_connections, crs=edges_gdf.crs)
        edges_connected = pd.concat([edges_gdf, connections_gdf])
        print(f"Added {len(new_connections)} new connections")
    else:
        edges_connected = edges_gdf.copy()

    # Verify final connectivity
    final_components = nx.number_connected_components(G)
    print(f"Final number of components: {final_components}")

    return edges_connected


def create_adjacency_matrix(snapped_points_gdf, network_gdf):
    """
    Create adjacency matrix from snapped points and network.

    Parameters:
    snapped_points_gdf (GeoDataFrame): Snapped sensor points
    network_gdf (GeoDataFrame): Network edges

    Returns:
    tuple: (adjacency matrix, node IDs)
    """
    # Calculate shortest paths between all pairs of points
    paths = []
    point_pairs = list(combinations(snapped_points_gdf.iterrows(), 2))

    print(f"Calculating paths between {len(point_pairs)} point pairs...")

    # Create a NetworkX graph for shortest path calculation
    G = nx.Graph()

    # Add edges from network
    for _, row in network_gdf.iterrows():
        line = row.geometry
        coords = list(line.coords)
        for i in range(len(coords) - 1):
            G.add_edge(
                coords[i],
                coords[i + 1],
                weight=Point(coords[i]).distance(Point(coords[i + 1])),
            )

    # Calculate paths between all pairs
    for (_, point1), (_, point2) in point_pairs:
        try:
            path_length = nx.shortest_path_length(
                G,
                source=tuple(point1.geometry.coords)[0],
                target=tuple(point2.geometry.coords)[0],
                weight="weight",
            )

            paths.append(
                {
                    "start_id": point1.original_id,
                    "end_id": point2.original_id,
                    "distance": path_length,
                }
            )

        except nx.NetworkXNoPath:
            print(
                f"No path between points {point1.original_id} and {point2.original_id}"
            )

    # Create the adjacency matrix
    if paths:
        # Create DataFrame from paths
        paths_df = pd.DataFrame(paths)

        # Get unique node IDs
        node_ids = sorted(snapped_points_gdf.original_id.unique())

        # Create empty matrix
        n = len(node_ids)
        adj_matrix = np.zeros((n, n))

        # Fill matrix with distances
        id_to_idx = {id_: i for i, id_ in enumerate(node_ids)}
        for _, row in paths_df.iterrows():
            i = id_to_idx[row.start_id]
            j = id_to_idx[row.end_id]
            adj_matrix[i, j] = row.distance
            adj_matrix[j, i] = row.distance  # Symmetric matrix

        return adj_matrix, node_ids
    else:
        print("No valid paths found!")
        return None, None


def explode_multilinestrings(gdf):
    # Create list to store new rows
    rows = []

    # Iterate through each row in the GDF
    for idx, row in gdf.iterrows():
        if isinstance(row.geometry, MultiLineString):
            # If geometry is MultiLineString, create new row for each LineString
            for line in row.geometry.geoms:
                new_row = row.copy()
                new_row.geometry = line
                rows.append(new_row)
        else:
            # If geometry is already LineString, keep as is
            rows.append(row)

    # Create new GeoDataFrame from expanded rows
    new_gdf = gpd.GeoDataFrame(rows, crs=gdf.crs)
    return new_gdf



================================================
File: src/preprocessing/graph_utils.py
================================================
# gnn_package/src/preprocessing/graph_utils.py

import os
import json
from pathlib import Path
import numpy as np
import pandas as pd
import osmnx as ox
import networkx as nx
import geopandas as gpd
import private_uoapi
from shapely.geometry import Polygon
from gnn_package.config.paths import (
    PREPROCESSED_GRAPH_DIR,
    SENSORS_DATA_DIR,
)
from gnn_package.src.utils.sensor_utils import get_sensor_name_id_map
from gnn_package.config import get_config


def read_or_create_sensor_nodes():
    FILE_PATH = SENSORS_DATA_DIR / "sensors.shp"
    if os.path.exists(FILE_PATH):
        print("Reading private sensors from file")
        sensors_gdf = gpd.read_file(FILE_PATH)
        return sensors_gdf
    else:
        config = private_uoapi.LSConfig()
        auth = private_uoapi.LSAuth(config)
        client = private_uoapi.LightsailWrapper(config, auth)
        locations = client.get_traffic_sensors()
        locations = pd.DataFrame(locations)
        sensors_gdf = gpd.GeoDataFrame(
            locations["location"],
            geometry=gpd.points_from_xy(locations["lon"], locations["lat"]),
            crs="EPSG:4326",
        )
        sensors_gdf = sensors_gdf.to_crs("EPSG:27700")
        # Add sensor IDs to the GeoDataFrame
        sensor_name_id_map = get_sensor_name_id_map()
        sensors_gdf["id"] = sensors_gdf["location"].apply(
            lambda x: sensor_name_id_map[x]
        )
        print(f"DEBUG: Column names: {sensors_gdf.columns}")
        sensors_gdf.to_file(FILE_PATH)
        return sensors_gdf


def get_bbox_transformed(config=None):
    """
    Create a bounding box polygon for the area of interest and transform it to the desired CRS.

    Returns:
    --------
    GeoDataFrame: Transformed bounding box polygon
    """

    # Get configuration
    if config is None:
        config = get_config()

    bbox_coords = config.data.general.bbox_coords
    bbox_crs = config.data.general.bbox_crs
    road_network_crs = config.data.general.road_network_crs

    print(f"get_bbox_transformed: bbox coords: {bbox_coords}")
    print(f"get_bbox_transformed: bbox crs: {bbox_crs}")
    print(f"get_bbox_transformed: road network crs: {road_network_crs}")

    polygon_bbox = Polygon(bbox_coords)

    # Create a GeoDataFrame from the bounding box polygon
    bbox_gdf = gpd.GeoDataFrame(geometry=[polygon_bbox], crs=bbox_crs)

    # Assuming your road data is in British National Grid (EPSG:27700)
    # Transform the bbox to match the road data's CRS
    bbox_transformed = bbox_gdf.to_crs(road_network_crs)

    print(
        f"get_bbox_transformed: bbox transformed from {bbox_crs} to {road_network_crs}"
    )

    return bbox_transformed


def get_street_network_gdfs(
    config=None,
):
    """
    Extract the walkable network for a specified area as GeoDataFrames.

    Parameters:
    place_name (str): Name of the place (e.g., 'Newcastle upon Tyne, UK')
    to_crs (str): Target coordinate reference system (default: 'EPSG:27700' for British National Grid)

    Returns:
    GeoDataFrame: Network edges as linestrings
    """
    # Get configuration
    if config is None:
        config = get_config()

    # Use parameters from the config
    to_crs = config.data.general.road_network_crs
    network_type = config.data.general.network_type
    custom_filter = config.data.general.custom_filter
    place_name = config.data.general.place_name

    print(f"get_street_network_gdfs: {place_name} with network type: {network_type}")
    print(f"get_street_network_gdfs: custom filter: {custom_filter}")
    print(f"get_street_network_gdfs: to_crs: {to_crs}")

    # Configure OSMnx settings
    ox.settings.use_cache = True
    ox.settings.log_console = True

    try:
        print(f"\nDownloading network for: {place_name}")
        # Download and project the network
        G = ox.graph_from_place(
            place_name,
            network_type=network_type,
            custom_filter=custom_filter,
            simplify=True,
        )
        G = ox.project_graph(G, to_crs=to_crs)

        # Convert to GeoDataFrames and return only edges
        _, edges_gdf = ox.graph_to_gdfs(G)
        print(f"Network downloaded and projected to: {to_crs}")
        print(f"Number of edges: {len(edges_gdf)}")

        return edges_gdf

    except Exception as e:
        print(f"Error downloading network: {str(e)}")
        raise


def save_graph_data(adj_matrix, node_ids, prefix="graph"):
    """
    Save adjacency matrix and node IDs with proper metadata.

    Parameters:
    -----------
    adj_matrix : np.ndarray
        The adjacency matrix
    node_ids : list or np.ndarray
        List of node IDs corresponding to matrix rows/columns
    output_dir : str or Path
        Directory to save the files
    prefix : str
        Prefix for the saved files
    """
    output_dir = Path(PREPROCESSED_GRAPH_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Save the adjacency matrix
    np.save(output_dir / f"{prefix}_adj_matrix.npy", adj_matrix)

    # Save node IDs with metadata
    node_metadata = {
        "node_ids": list(
            map(str, node_ids)
        ),  # Convert to strings for JSON compatibility
        "matrix_shape": adj_matrix.shape,
        "creation_metadata": {
            "num_nodes": len(node_ids),
            "matrix_is_symmetric": np.allclose(adj_matrix, adj_matrix.T),
        },
    }

    with open(output_dir / f"{prefix}_metadata.json", "w", encoding="utf-8") as f:
        json.dump(node_metadata, f, indent=2)


def load_graph_data(prefix="graph", return_df=False):
    """
    Load adjacency matrix with associated node IDs.

    Parameters:
    -----------
    input_dir : str or Path
        Directory containing the saved files
    prefix : str
        Prefix of the saved files
    return_df : bool
        If True, returns a pandas DataFrame instead of numpy array

    Returns:
    --------
    tuple : (adj_matrix, node_ids, metadata)
        - adj_matrix: numpy array or DataFrame of the adjacency matrix
        - node_ids: list of node IDs
        - metadata: dict containing additional graph information
    """
    input_dir = Path(PREPROCESSED_GRAPH_DIR)

    # Load the adjacency matrix
    adj_matrix = np.load(input_dir / f"{prefix}_adj_matrix.npy")

    # Load metadata
    with open(input_dir / f"{prefix}_metadata.json", "r", encoding="utf-8") as f:
        metadata = json.load(f)

    node_ids = metadata["node_ids"]

    # Verify matrix shape matches metadata
    assert adj_matrix.shape == tuple(metadata["matrix_shape"]), "Matrix shape mismatch!"

    # Optionally convert to DataFrame
    if return_df:
        adj_matrix = pd.DataFrame(adj_matrix, index=node_ids, columns=node_ids)

    return adj_matrix, node_ids, metadata


def graph_to_adjacency_matrix_and_nodes(G) -> tuple:
    """
    Convert a NetworkX graph to an adjacency matrix.

    Parameters
    ----------
    G : nx.Graph
        The input graph.

    Returns
    -------
    np.ndarray
        The adjacency matrix as a dense numpy array.
    list
        The list of node IDs in the same order as the rows/columns of the matrix.
    """
    # Get a sorted list of node IDs to ensure consistent ordering
    node_ids = sorted(list(G.nodes()))

    # Create the adjacency matrix using NetworkX's built-in function
    adj_matrix = nx.adjacency_matrix(G, nodelist=node_ids, weight="weight")

    # Convert to dense numpy array for easier viewing
    adj_matrix_dense = adj_matrix.todense()

    return adj_matrix_dense, node_ids


def create_networkx_graph_from_adj_matrix(adj_matrix, node_ids, names_dict=None):
    """
    Create a NetworkX graph from adjacency matrix and node IDs.

    Parameters:
    -----------
    adj_matrix : np.ndarray
        The adjacency matrix
    node_ids : list
        List of node IDs
    names_dict : dict, optional
        Dictionary mapping node IDs to names

    Returns:
    --------
    networkx.Graph
        The reconstructed graph with all metadata
    """
    G = nx.Graph()

    # Add nodes with names if provided
    for i, node_id in enumerate(node_ids):
        node_attrs = {"id": node_id}
        if names_dict and str(node_id) in names_dict:
            node_attrs["name"] = names_dict[str(node_id)]
        G.add_node(node_id, **node_attrs)

    # Add edges with weights
    for i in range(len(node_ids)):
        for j in range(i + 1, len(node_ids)):
            weight = adj_matrix[i, j]
            if weight > 0:
                G.add_edge(node_ids[i], node_ids[j], weight=weight)

    return G



================================================
File: src/preprocessing/graph_visualization.py
================================================
#  gnn_package/src/preprocessing/graph_visualisation.py

import networkx as nx
import matplotlib.pyplot as plt
import numpy as np
from shapely.geometry import LineString, MultiLineString


def visualize_network_components(road_network_gdf):
    """
    Visualize all components of the road network in different colors.

    Parameters:
    road_network_gdf (GeoDataFrame): Network edges as GeoDataFrame

    Returns:
    tuple: (figure, axis, GeoDataFrame with component information)
    """
    # Find connected components using spatial operations
    components_gdf = road_network_gdf.copy()
    components_gdf["component"] = -1

    merged_lines = components_gdf.geometry.unary_union

    # If it's a single geometry, convert to list
    if isinstance(merged_lines, LineString):
        merged_lines = [merged_lines]
    elif isinstance(merged_lines, MultiLineString):
        merged_lines = list(merged_lines.geoms)

    # Assign component IDs
    for i, merged_line in enumerate(merged_lines):
        # Find all linestrings that intersect with this component
        mask = components_gdf.geometry.intersects(merged_line)
        components_gdf.loc[mask, "component"] = i

    # Count segments in each component
    component_sizes = components_gdf.component.value_counts()
    n_components = len(component_sizes)

    # Create the plot
    fig, ax = plt.subplots(figsize=(15, 15))

    # Create color map
    colors = plt.cm.rainbow(np.linspace(0, 1, n_components))

    # Plot each component
    for i, color in enumerate(colors):
        mask = components_gdf["component"] == i
        subset = components_gdf[mask]
        size = len(subset)

        # Only label larger components
        if (
            size > len(road_network_gdf) * 0.05
        ):  # Label components with >5% of total segments
            label = f"Component {i} ({size} segments)"
        else:
            label = None

        subset.plot(ax=ax, color=color, linewidth=1, alpha=0.7, label=label)

    # Add legend and title
    if ax.get_legend():  # Only add legend if there are labels
        plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")

    plt.title(f"Road Network Components (Total: {n_components} components)")
    plt.tight_layout()

    # Print summary
    print("\nComponent Summary:")
    print(f"Total components: {n_components}")
    print("\nLargest components:")
    print(component_sizes.head())

    return fig, ax, components_gdf


def visualize_sensor_graph(G, points_gdf):
    """
    Visualize the sensor graph with edge weights.
    """
    fig, ax = plt.subplots(figsize=(10, 10))

    # Create position dictionary from points GeoDataFrame
    pos = {
        row.original_id: (row.geometry.x, row.geometry.y)
        for idx, row in points_gdf.iterrows()
    }

    # Draw edges with width proportional to weight
    weights = [G[u][v]["weight"] for u, v in G.edges()]
    max_weight = max(weights)
    normalized_weights = [w / max_weight for w in weights]

    # Draw the graph
    nx.draw_networkx_edges(G, pos, width=normalized_weights, alpha=0.5)
    nx.draw_networkx_nodes(G, pos, node_size=50, node_color="red")

    plt.title("Fully Connected Sensor Network Graph")
    plt.axis("on")
    ax.set_aspect("equal")

    return fig, ax



================================================
File: src/preprocessing/timeseries_preprocessor.py
================================================
from typing import Dict, List, Tuple
from datetime import timedelta
from dataclasses import dataclass
import numpy as np
import pandas as pd
from gnn_package.config import get_config


@dataclass
class TimeWindow:
    start_idx: int
    end_idx: int
    node_id: str
    mask: np.ndarray  # 1 for valid data, 0 for missing


class TimeSeriesPreprocessor:
    def __init__(
        self,
        config=None,
    ):
        """
        Initialize the preprocessor for handling time series with gaps.

        Parameters:
        -----------
        window_size : int, optional
            Size of the sliding window, overrides config if provided
        stride : int, optional
            Number of steps to move the window, overrides config if provided
        gap_threshold : pd.Timedelta, optional
            Maximum allowed time difference between consecutive points, overrides config if provided
        missing_value : float, optional
            Value to use for marking missing data, overrides config if provided
        config : ExperimentConfig, optional
            Centralized configuration object. If not provided, will use global config.
        """
        # Get configuration
        if config is None:
            print("TimeSeriesPreprocessor: No config provided, using global config")
            config = get_config()

        self.window_size = config.data.general.window_size
        self.stride = config.data.general.stride
        self.gap_threshold = pd.Timedelta(
            minutes=config.data.general.gap_threshold_minutes
        )
        self.missing_value = config.data.general.missing_value

        # Store full config for other methods
        self.config = config

    def create_windows_from_grid(
        self,
        time_series_dict,
        config=None,
    ):
        """
        Create windowed data with common time boundaries across all sensors.

        Parameters:
        -----------
        time_series_dict : Dict[str, pd.Series]
            Dictionary mapping node IDs to their time series
        config : ExperimentConfig, optional
            Centralized configuration object. If not provided, will use global config.

        Returns:
        --------
        X_by_sensor : Dict[str, np.ndarray]
            Dictionary mapping node IDs to their windowed arrays
                Array of shape (n_windows, window_size)
        masks_by_sensor : Dict[str, np.ndarray]
            Dictionary mapping node IDs to their binary masks
        metadata : Dict[str,List[TimeWindow]
            Metadata for each window
        """

        # Get configuration
        if config is None:
            config = self.config

        # Note: We removed standardize parameter since it's now handled earlier in the pipeline

        @dataclass
        class TimeWindow:
            start_idx: int
            end_idx: int
            node_id: str
            mask: np.ndarray  # 1 for valid data, 0 for missing

        # Find global time range
        all_timestamps = set()
        for series in time_series_dict.values():
            all_timestamps.update(series.index)

        all_timestamps = sorted(all_timestamps)

        # Create a common grid of window start points
        window_starts = range(
            0, len(all_timestamps) - self.window_size + 1, self.stride
        )

        X_by_sensor = {}
        masks_by_sensor = {}
        metadata_by_sensor = {}

        # Process each sensor using the common time grid
        for node_id, series in time_series_dict.items():
            sensor_windows = []
            sensor_masks = []
            sensor_metadata = []

            # Reindex the series to the common time grid, filling NaNs
            common_series = pd.Series(index=all_timestamps)
            common_series.loc[series.index] = series.values

            # Create windows at each common start point
            for start_idx in window_starts:
                end_idx = start_idx + self.window_size
                window = common_series.iloc[start_idx:end_idx].values

                # Check for unexpected NaN values
                if np.any(np.isnan(window)):
                    raise ValueError(
                        "Found NaN values in input data that should have been replaced already"
                    )

                # Create mask based ONLY on missing value
                mask = window != self.missing_value

                # Note: Standardization is now removed from here since it's done globally

                # Add feature dimension if needed
                if len(window.shape) == 1:
                    window = window.reshape(-1, 1)
                if len(mask.shape) == 1:
                    mask = mask.reshape(-1, 1)

                sensor_windows.append(window)
                sensor_masks.append(mask)

                sensor_metadata.append(
                    TimeWindow(
                        start_idx=start_idx,
                        end_idx=end_idx,
                        node_id=node_id,
                        mask=mask,
                    )
                )

            X_by_sensor[node_id] = np.array(sensor_windows)
            masks_by_sensor[node_id] = np.array(sensor_masks)
            metadata_by_sensor[node_id] = sensor_metadata

        return X_by_sensor, masks_by_sensor, metadata_by_sensor

    def create_rolling_window_splits(
        self,
        time_series_dict,
        config=None,
    ):
        """
        Create multiple time-based splits using a rolling window approach.

        Parameters:
        -----------
        time_series_dict : Dict[str, pd.Series]
            Dictionary mapping node IDs to their time series data

        Returns:
        --------
        List[Dict[str, Dict[str, pd.Series]]]
            List of dictionaries, each containing a train/validation split
        """
        # Get configuration
        if config is None:
            config = self.config

        train_ratio = config.data.training.train_ratio
        n_splits = config.data.training.n_splits

        # Get the window size from config to use as buffer
        window_size_timedelta = pd.Timedelta(
            minutes=15 * self.window_size * config.data.general.buffer_factor
        )  # Assuming 15-min frequency

        # Find global min and max dates
        all_dates = []
        for series in time_series_dict.values():
            if len(series) > 0:
                all_dates.extend(series.index)

        min_date = min(all_dates)
        max_date = max(all_dates)
        total_days = (max_date - min_date).days

        splits = []

        # Create splits based on training ratio
        for i in range(n_splits):
            # Calculate step size for this approach
            step_size = (
                (total_days * (1 - train_ratio)) / (n_splits - 1) if n_splits > 1 else 0
            )

            # Calculate cutoff point (end of training data)
            train_days = total_days * train_ratio + (i * step_size)

            # Training data cutoff
            train_cutoff = min_date + timedelta(days=train_days)

            # Add buffer period between training and validation
            buffer_cutoff = train_cutoff + window_size_timedelta

            # Validation end
            val_end = max_date  # Use all available data after buffer

            # Skip if buffer would go beyond available data
            if buffer_cutoff >= max_date:
                continue

            train_dict = {}
            val_dict = {}

            for node_id, series in time_series_dict.items():
                # Get training data (everything before train cutoff)
                train_series = series[series.index < train_cutoff]

                # Get validation data (everything after buffer cutoff)
                val_series = series[series.index >= buffer_cutoff]

                # Only include if both parts have data
                if len(train_series) > 0 and len(val_series) > 0:
                    train_dict[node_id] = train_series
                    val_dict[node_id] = val_series

            splits.append({"train": train_dict, "val": val_dict})

        return splits

    def create_time_based_split(
        self,
        time_series_dict,
        config=None,
    ):
        """
        Split data based on time, either using a ratio or a specific cutoff date (simple solution).

        Parameters:
        -----------
        time_series_dict : Dict[str, pd.Series]
            Dictionary mapping node IDs to their time series data
        train_ratio : float, optional
            Ratio of data to use for training (by time, not by sample count)
        cutoff_date : datetime, optional
            Specific date to use as the split point (overrides train_ratio)

        Returns:
        --------
        Dict[str, Dict[str, pd.Series]]
            Dictionary containing train and validation series for each node
        """
        # Get configuration
        if config is None:
            config = self.config

        train_ratio = config.data.training.train_ratio
        cutoff_date = config.data.training.cutoff_date

        # Get the window size from config to use as buffer
        window_size_timedelta = pd.Timedelta(
            minutes=15 * self.window_size * config.data.general.buffer_factor
        )  # Assuming 15-min frequency

        train_dict = {}
        val_dict = {}

        # Find global min and max dates if we need to calculate cutoff
        if cutoff_date is None:
            all_dates = []
            for series in time_series_dict.values():
                if len(series) > 0:
                    all_dates.extend(series.index)

            min_date = min(all_dates)
            max_date = max(all_dates)
            total_days = (max_date - min_date).days

            # Calculate cutoff date based on train_ratio
            cutoff_date = min_date + timedelta(days=int(total_days * train_ratio))

        # Calculate buffer end date
        buffer_end = cutoff_date + window_size_timedelta

        # Split each sensor's data
        for node_id, series in time_series_dict.items():
            # Split based on date
            train_series = series[series.index < cutoff_date]

            # Validation data starts after the buffer period
            val_series = series[series.index >= buffer_end]

            # Only include if both parts have data
            if len(train_series) > 0 and len(val_series) > 0:
                train_dict[node_id] = train_series
                val_dict[node_id] = val_series

        return [{"train": train_dict, "val": val_dict}]


def resample_sensor_data(time_series_dict, freq=None, fill_value=None, config=None):
    """
    Resample all sensor time series to a consistent frequency and fill gaps.

    Parameters:
    -----------
    time_series_dict : dict
        Dictionary mapping sensor IDs to their time series data
    freq : str, optional
        Pandas frequency string (e.g., '15min', '1H'), overrides config if provided
    fill_value : float, optional
        Value to use for filling gaps, overrides config if provided
    config : ExperimentConfig, optional
        Centralized configuration object. If not provided, will use global config.

    Returns:
    --------
    dict
        Dictionary with resampled time series
    """
    # Get configuration
    if config is None:
        print("resample_sensor_data: No config provided, using global config")
        config = get_config()

    # Use parameters or config values
    if freq is None:
        freq = config.data.general.resampling_frequency
    if fill_value is None:
        fill_value = config.data.general.missing_value

    # Find global min and max dates
    all_dates = []
    for series in time_series_dict.values():
        if len(series) > 0:
            all_dates.extend(series.index)

    global_min = min(all_dates)
    global_max = max(all_dates)

    # Create common date range
    date_range = pd.date_range(start=global_min, end=global_max, freq=freq)

    # Resample each sensor's data
    resampled_dict = {}

    for sensor_id, series in time_series_dict.items():
        # Skip empty series
        if series is None or len(series) == 0:
            continue

        # Create a Series with the full date range
        resampled = pd.Series(index=date_range, dtype=float)

        # Use original values where available (handle duplicates by taking the mean)
        grouper = series.groupby(series.index)
        non_duplicate_series = grouper.mean()

        # Align with the resampled index
        resampled[non_duplicate_series.index] = non_duplicate_series

        # Fill gaps with fill_value
        resampled = resampled.fillna(fill_value)

        resampled_dict[sensor_id] = resampled

    print(f"Resampled {len(resampled_dict)} sensors to frequency {freq}")
    print(f"Each sensor now has {len(date_range)} data points")

    # Create a new dictionary to return
    result_dict = resampled_dict.copy()

    # Apply standardization if enabled in config
    if config.data.general.standardize:
        print("Applying global standardization to sensor data")
        resampled_dict, stats = standardize_sensor_data(resampled_dict, config)

        print(
            f"Standardization complete: mean={stats['mean']:.2f}, std={stats['std']:.2f}"
        )

        # Store stats in a special key that won't interfere with sensor data
        result_dict = resampled_dict.copy()  # Update with standardized data
        result_dict["__stats__"] = (
            stats  # Use a special key unlikely to conflict with sensor IDs
        )

    return result_dict


def standardize_sensor_data(time_series_dict, config=None):
    """
    Standardize sensor data globally across all sensors while preserving missing values.

    Parameters:
    -----------
    time_series_dict : Dict[str, pd.Series]
        Dictionary mapping sensor IDs to their time series data
    config : ExperimentConfig, optional
        Configuration object. If not provided, will use global config.

    Returns:
    --------
    Tuple[Dict[str, pd.Series], Dict[str, float]]
        Tuple containing:
        - Dictionary with standardized time series
        - Dictionary with statistics (mean, std) for inverse transformation
    """
    # Get configuration
    if config is None:
        config = get_config()

    # Get missing value from config
    missing_value = config.data.general.missing_value

    # Step 1: Collect all valid values across all sensors
    all_valid_values = []
    for series in time_series_dict.values():
        # Skip completely empty series
        if len(series) == 0:
            continue

        valid_mask = series.values != missing_value
        all_valid_values.append(series.values[valid_mask])

    # If we have no valid values, return the original data
    if not all_valid_values:
        print("Warning: No valid values found for standardization")
        return time_series_dict, {"mean": 0.0, "std": 1.0}

    all_valid_values = np.concatenate(all_valid_values)

    # Step 2: Calculate global statistics from all valid values
    global_mean = np.mean(all_valid_values)
    global_std = np.std(all_valid_values)

    # Add a small epsilon to avoid division by zero
    if global_std < 1e-8:
        print("Warning: Very small standard deviation detected, using default value")
        global_std = 1.0

    # Store these for later inverse transformation
    stats = {"mean": global_mean, "std": global_std}

    print(f"Global standardization: mean={global_mean:.2f}, std={global_std:.2f}")

    # Step 3: Apply standardization while preserving missing values
    standardized_dict = {}
    for sensor_id, series in time_series_dict.items():
        standardized_series = series.copy()
        valid_mask = series.values != missing_value

        # Only standardize valid values
        standardized_series.values[valid_mask] = (
            series.values[valid_mask] - global_mean
        ) / global_std

        standardized_dict[sensor_id] = standardized_series

    return standardized_dict, stats




================================================
File: src/training/__init__.py
================================================
from .stgnn_training import preprocess_data, train_model
from .stgnn_prediction import (
    load_model,
    fetch_recent_data_for_validation,
    plot_predictions_with_validation,
    predict_all_sensors_with_validation,
    predict_with_model,
    format_predictions_with_validation,
)

__all__ = [
    "preprocess_data",
    "train_model",
    "load_model",
    "fetch_recent_data_for_validation",
    "plot_predictions_with_validation",
    "predict_all_sensors_with_validation",
    "predict_with_model",
    "format_predictions_with_validation",
]



================================================
File: src/training/stgnn_prediction.py
================================================
# gnn_package/src/training/stgnn_prediction.py


import logging
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from gnn_package.src.utils.sensor_utils import get_sensor_name_id_map
from gnn_package.src.models.stgnn import create_stgnn_model
from gnn_package.src.utils.config_utils import (
    load_model_for_prediction,
    create_prediction_config,
)
from gnn_package.src.data.processors import DataProcessorFactory, ProcessorMode
from gnn_package.src.data.data_sources import APIDataSource

from gnn_package.src.utils.data_utils import validate_data_package

logger = logging.getLogger(__name__)


def load_model(model_path, config):
    """
    Load a trained STGNN model using parameters from config.

    Parameters:
    -----------
    model_path : str
        Path to the saved model
    config : ExperimentConfig
        Configuration object to use for model parameters

    Returns:
    --------
    Loaded model
    """
    logger.info(f"Loading model using configuration from {config.config_path}")

    # Create model with parameters from config
    model = create_stgnn_model(config=config)

    # Load state dict and set to eval mode
    model.load_state_dict(torch.load(model_path, map_location=torch.device("cpu")))
    model.eval()

    # Log model configuration
    logger.info(f"Loaded model with parameters:")
    logger.info(f"  hidden_dim: {config.model.hidden_dim}")
    logger.info(f"  num_layers: {config.model.num_layers}")
    logger.info(f"  num_gc_layers: {getattr(config.model, 'num_gc_layers', 2)}")
    logger.info(f"  horizon: {config.data.general.horizon}")

    return model


async def fetch_recent_data_for_validation(config):
    """
    Fetch recent data for validation using the prediction data processor.

    Parameters:
    -----------
    config : ExperimentConfig
        Configuration object containing all parameters

    Returns:
    --------
    dict
        Dictionary containing processed data for validation
    """

    # Create an API data source for the real-time data
    data_source = APIDataSource()

    # Create processor explicitly for prediction mode
    processor = DataProcessorFactory.create_processor(
        mode=ProcessorMode.PREDICTION,
        config=config,
        data_source=data_source,
    )

    # Process data using prediction-specific logic
    return await processor.process_data()


def plot_predictions_with_validation(predictions_dict, data, node_ids, config):
    """
    Plot predictions alongside actual data for validation

    Parameters:
    -----------
    predictions_dict : dict
        Dictionary returned by predict_with_model
    data : dict
        Dict containing time series data from fetch_recent_data
    node_ids : list
        List of node IDs
    config : ExperimentConfig
        Configuration object

    Returns:
    --------
    matplotlib figure
    """
    # Get validation window from config
    validation_window = config.data.general.horizon

    # Get name-to-id mapping
    name_id_map = get_sensor_name_id_map(config=config)
    name_id_map = {v: k for k, v in name_id_map.items()}

    # Get predictions array and node indices
    pred_array = predictions_dict["predictions"]
    node_indices = predictions_dict["node_indices"]
    valid_nodes = [node_ids[idx] for idx in node_indices]

    # Create a figure
    n_nodes = min(len(valid_nodes), 6)  # Limit to 6 nodes
    fig, axes = plt.subplots(n_nodes, 1, figsize=(12, 3 * n_nodes))
    if n_nodes == 1:
        axes = [axes]

    # Get time series dictionary from data
    time_series_dict = data["time_series"]

    for i, node_id in enumerate(valid_nodes[:n_nodes]):
        ax = axes[i]

        # Get full historical data
        if node_id not in time_series_dict:
            logger.warning(f"No historical data found for node {node_id}")
            continue

        historical = time_series_dict[node_id]

        # Split historical data into 'input' and 'validation' parts
        input_data = historical[:-validation_window]
        validation_data = historical[-validation_window:]

        # Get prediction for this node
        node_position = np.where(node_indices == node_ids.index(node_id))[0]
        if len(node_position) == 0:
            logger.warning(f"Cannot find node {node_id} in prediction data")
            continue

        node_idx = node_position[0]
        pred = pred_array[0, node_idx, :, 0]  # [batch=0, node, time, feature=0]

        # Get the last timestamp from input data
        last_input_time = input_data.index[-1]

        # Create time indices for prediction that align with validation data
        pred_times = validation_data.index

        # Plot
        ax.plot(
            input_data.index,
            input_data.values,
            label="Input Data",
            color="blue",
            linewidth=1.5,
        )
        ax.plot(
            validation_data.index,
            validation_data.values,
            label="Actual Values",
            color="green",
            linewidth=2,
        )
        ax.plot(pred_times, pred, "r--", label="Predictions", linewidth=2)

        # Highlight the last input point for visual clarity
        ax.scatter(
            [last_input_time],
            [input_data.values[-1]],
            color="darkblue",
            s=50,
            zorder=5,
            label="Last Input Point",
        )

        # Add sensor name to title if available
        sensor_name = name_id_map.get(node_id, node_id)
        ax.set_title(f"Model Validation: {sensor_name} (ID: {node_id})")
        ax.set_ylabel("Traffic Count")

        # Add a grid for better readability
        ax.grid(True, linestyle="--", alpha=0.7)

        # Add a legend
        ax.legend(loc="best", framealpha=0.9)

        # Format x-axis as time
        ax.tick_params(axis="x", rotation=45)

        # Add a vertical line to separate input data and validation period
        ax.axvline(x=last_input_time, color="gray", linestyle="--", alpha=0.8)

        # Calculate and display metrics if validation data exists
        if len(validation_data) > 0:
            mse = ((pred - validation_data.values) ** 2).mean()
            mae = abs(pred - validation_data.values).mean()
            ax.text(
                0.02,
                0.95,
                f"MSE: {mse:.4f}\nMAE: {mae:.4f}",
                transform=ax.transAxes,
                bbox=dict(facecolor="white", alpha=0.8),
            )

    plt.tight_layout()
    return fig


async def predict_all_sensors_with_validation(
    model_path, config=None, output_file=None, plot=True
):
    """
    Make predictions for all available sensors and validate against actual data.

    Parameters:
    -----------
    model_path : str or Path
        Path to the saved model file
    config : ExperimentConfig, optional
        Configuration object. If None, attempts to find config in model directory
        or falls back to global config.
    output_file : str, optional
        Path to save the prediction results
    plot : bool
        Whether to create and show validation plots

    Returns:
    --------
    dict
        Dictionary containing predictions, actual values, and evaluation metrics
    """
    # Load model with appropriate configuration
    prediction_config = create_prediction_config()
    model, config = load_model_for_prediction(model_path, prediction_config)

    logger.info(f"Using configuration from {config.config_path}")
    logger.info(f"Loading model from: {model_path}")
    # Load model with the config
    model = load_model(model_path=model_path, config=config)
    logger.info(f"Model loaded successfully")

    # Fetch and preprocess recent data for validation
    logger.info(f"Fetching and preprocessing recent data for validation")
    data_package = await fetch_recent_data_for_validation(config=config)

    # Extract components from the standardized structure
    node_ids = data_package["graph_data"]["node_ids"]
    time_series = data_package["time_series"]["validation"]

    logger.info(f"Preprocessed data for {len(node_ids)} nodes")

    # Make predictions using the validation dataloader
    predictions_dict = predict_with_model(model, data_package, config=config)

    # Format results
    results_df = format_predictions_with_validation(
        predictions_dict=predictions_dict,
        data_package=data_package,
        node_ids=node_ids,
        config=config,
    )

    # Save to file if requested
    if output_file and not results_df.empty:
        results_df.to_csv(output_file, index=False)
        logger.info(f"Predictions saved to {output_file}")

    # Plot if requested
    if plot and not results_df.empty:
        # Create a data structure compatible with plot_predictions
        plot_data = {"time_series": time_series}
        fig = plot_predictions_with_validation(
            predictions_dict, plot_data, node_ids, config=config
        )
        plt.show()

        # Save the plot if output file is specified
        if output_file:
            plot_filename = (
                output_file.replace(".csv", ".png")
                if output_file.endswith(".csv")
                else f"{output_file}_plot.png"
            )
            fig.savefig(plot_filename, dpi=300, bbox_inches="tight")
            logger.info(f"Plot saved to {plot_filename}")

    return {
        "predictions": predictions_dict,
        "dataframe": results_df,
        "data": data_package,
        "node_ids": node_ids,
    }


def predict_with_model(model, data_package, config):
    """
    Make predictions using a trained model and a dataloader.

    Parameters:
    -----------
    model : STGNN
        The trained model
    data_package : dict
        Complete data package containing data loaders, graph data, and metadata
    config : ExperimentConfig
        Configuration object

    Returns:
    --------
    dict
        Dictionary containing predictions and metadata
    """
    # Validate data package
    validate_data_package(
        data_package, required_components=["val_loader"], mode="prediction"
    )

    # Extract components for use
    val_loader = data_package["data_loaders"]["val_loader"]
    # Get device from config if specified, otherwise use best available
    device_name = getattr(config.training, "device", None)
    if device_name:
        device = torch.device(device_name)
    else:
        if torch.backends.mps.is_available():
            device = torch.device("mps")
        elif torch.cuda.is_available():
            device = torch.device("cuda")
        else:
            device = torch.device("cpu")

    logger.info(f"Using device: {device}")
    model.to(device)
    model.eval()

    # Get a single batch from the dataloader
    batch = next(iter(val_loader))

    # Move data to device
    x = batch["x"].to(device)
    x_mask = batch["x_mask"].to(device)
    adj = batch["adj"].to(device)

    logger.info(
        f"Input shapes - x: {x.shape}, x_mask: {x_mask.shape}, adj: {adj.shape}"
    )

    # Make prediction
    with torch.no_grad():
        predictions = model(x, adj, x_mask)

    # Convert to numpy for easier handling
    predictions_np = predictions.cpu().numpy()

    return {
        "predictions": predictions_np,
        "input_data": {
            "x": x.cpu().numpy(),
            "x_mask": x_mask.cpu().numpy(),
        },
        "node_indices": batch["node_indices"].numpy(),
    }


def format_predictions_with_validation(
    predictions_dict, data_package, node_ids, config
):
    """
    Format model predictions into a pandas DataFrame and include actual values for comparison.

    Parameters:
    -----------
    predictions_dict : dict
        Dictionary returned by predict_with_model
    data_package : dict
        Complete data package containing time series data

    node_ids : list
        List of node IDs in the order they appear in the predictions
    config : ExperimentConfig
        Configuration object

    Returns:
    --------
    pandas.DataFrame
        DataFrame containing formatted predictions and actual values
    """
    # Validate data package
    validate_data_package(data_package, required_components=["time_series"])

    # Extract time series data
    time_series_dict = data_package["time_series"]["validation"]

    # Get prediction array and node indices
    predictions = predictions_dict["predictions"]
    node_indices = predictions_dict["node_indices"]
    horizon = config.data.general.horizon

    # Get name-to-ID mapping for sensor names
    name_id_map = get_sensor_name_id_map(config=config)
    id_to_name_map = {v: k for k, v in name_id_map.items()}

    # Create rows for the DataFrame
    rows = []

    # For each node that has predictions
    for i, node_idx in enumerate(node_indices):
        node_id = node_ids[node_idx]

        # Skip if this node doesn't have time series data
        if node_id not in time_series_dict:
            continue

        # Get the time series for this node
        series = time_series_dict[node_id]

        # Skip if not enough data
        if len(series) <= horizon:
            logger.warning(f"Not enough data for node {node_id} to validate")
            continue

        # Get the validation data
        validation_data = series.iloc[-horizon:]

        # Extract predictions for this node
        node_preds = predictions[0, i, :, 0]

        # Create a row for each prediction horizon
        for h, pred_value in enumerate(node_preds):
            if h < len(validation_data):
                # Get the corresponding actual value and timestamp
                actual_time = validation_data.index[h]
                actual_value = validation_data.iloc[h]

                # Create the row
                row = {
                    "node_id": node_id,
                    "sensor_name": id_to_name_map.get(node_id, "Unknown"),
                    "timestamp": actual_time,
                    "prediction": float(pred_value),
                    "actual": float(actual_value),
                    "error": float(pred_value - actual_value),
                    "abs_error": float(abs(pred_value - actual_value)),
                    "horizon": h + 1,  # 1-based horizon index
                }
                rows.append(row)

    # Create DataFrame
    if rows:
        df = pd.DataFrame(rows)
        # Calculate overall metrics
        mse = (df["error"] ** 2).mean()
        mae = df["abs_error"].mean()
        logger.info(f"Overall MSE: {mse:.4f}, MAE: {mae:.4f}")
        return df
    else:
        logger.warning("No predictions could be validated against actual data")
        return pd.DataFrame(
            columns=[
                "node_id",
                "sensor_name",
                "timestamp",
                "prediction",
                "actual",
                "error",
                "abs_error",
                "horizon",
            ]
        )



================================================
File: src/training/stgnn_training.py
================================================
# gnn_package/src/training/stgnn_training.py

import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm, trange

from gnn_package.src import preprocessing
from gnn_package.src.models.stgnn import create_stgnn_model, STGNNTrainer
from gnn_package.config import get_config
from gnn_package.src.data.processors import DataProcessorFactory, ProcessorMode
from gnn_package.src.data.data_sources import FileDataSource
from gnn_package.src.utils.data_utils import validate_data_package
from gnn_package.src.utils.config_utils import save_model_with_config

# gnn_package/src/training/stgnn_training.py


async def preprocess_data(
    data=None, data_file=None, config=None, mode=None, verbose=True
):
    """Load and preprocess graph and sensor data for training."""
    print("Training.preprocess_data: Starting preprocessing")

    # Get configuration
    if config is None:
        if verbose:
            print("No configuration explicitly provided, using global config...")
        config = get_config(verbose=verbose)

    # Determine mode
    if mode is None:
        # Default to training
        processor_mode = ProcessorMode.TRAINING
    else:
        processor_mode = ProcessorMode(mode)

    print(f"Using processor mode: {processor_mode}")

    # Create data source based on inputs
    if data is not None:
        print("Using provided data")

        # Custom in-memory data source using the provided data
        class CustomDataSource(FileDataSource):
            async def get_data(self, config):
                return data

        data_source = CustomDataSource(None)
    elif data_file is not None:
        print(f"Using data file: {data_file}")
        data_source = FileDataSource(data_file)
    else:
        print("WARNING: No data or data_file provided")
        data_source = None  # Will be created by factory if needed

    # Create processor using factory
    print("Creating data processor...")
    processor = DataProcessorFactory.create_processor(
        mode=processor_mode, config=config, data_source=data_source
    )

    print(f"Processor created: {type(processor).__name__}")

    # Process data according to mode
    try:
        print("Calling processor.process_data()...")
        result = await processor.process_data()
        print(f"Processor returned: {type(result)}")
        if result is None:
            print("WARNING: processor.process_data() returned None!")
        return result
    except Exception as e:
        print(f"ERROR in processor.process_data(): {e}")
        import traceback

        traceback.print_exc()
        raise


class TqdmSTGNNTrainer(STGNNTrainer):
    """
    Extension of STGNNTrainer that adds progress bars using tqdm
    """

    def __init__(self, model, config):
        """
        Initialize the trainer with model and configuration.

        Parameters:
        -----------
        model : STGNN
            The model to train
        config : ExperimentConfig
            Configuration object
        """
        # Call the parent class constructor to handle device, optimizer, and criterion
        super().__init__(model, config)

        # Additional tqdm-specific initialization can go here if needed
        self.log_interval = getattr(config.training, "log_interval", 10)

    def train_epoch(self, dataloader):
        """Train for one epoch with progress bar"""
        self.model.train()
        total_loss = 0
        num_batches = 0

        # Create progress bar for batches
        pbar = tqdm(dataloader, desc="Training batches", leave=False)

        for batch in pbar:
            # Move data to device
            x = batch["x"].to(self.device)
            x_mask = batch["x_mask"].to(self.device)
            y = batch["y"].to(self.device)
            y_mask = batch["y_mask"].to(self.device)
            adj = batch["adj"].to(self.device)

            # Forward pass
            self.optimizer.zero_grad()
            y_pred = self.model(x, adj, x_mask)

            # Compute loss on valid points only
            loss = self.criterion(y_pred, y)
            if y_mask is not None:
                # Count non-zero elements in mask
                mask_sum = y_mask.sum()
                if mask_sum > 0:
                    loss = (loss * y_mask).sum() / mask_sum
                else:
                    loss = torch.tensor(0.0, device=self.device)

            # Backward pass
            loss.backward()
            self.optimizer.step()

            batch_loss = loss.item()
            total_loss += batch_loss
            num_batches += 1

            # Update progress bar with current batch loss
            pbar.set_postfix({"batch_loss": f"{batch_loss:.6f}"})

        return total_loss / max(1, num_batches)

    def evaluate(self, dataloader):
        """Evaluate the model on a validation or test set with progress bar"""
        self.model.eval()
        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            # Create progress bar for validation batches
            pbar = tqdm(dataloader, desc="Validation batches", leave=False)

            for batch in pbar:
                # Move data to device
                x = batch["x"].to(self.device)
                x_mask = batch["x_mask"].to(self.device)
                y = batch["y"].to(self.device)
                y_mask = batch["y_mask"].to(self.device)
                adj = batch["adj"].to(self.device)

                # Forward pass
                y_pred = self.model(x, adj, x_mask)

                # Compute loss on valid points only
                loss = self.criterion(y_pred, y)
                if y_mask is not None:
                    # Count non-zero elements in mask
                    mask_sum = y_mask.sum()
                    if mask_sum > 0:
                        loss = (loss * y_mask).sum() / mask_sum
                    else:
                        loss = torch.tensor(0.0, device=self.device)

                batch_loss = loss.item()
                total_loss += batch_loss
                num_batches += 1

                # Update progress bar with current batch loss
                pbar.set_postfix({"batch_loss": f"{batch_loss:.6f}"})

        return total_loss / max(1, num_batches)


def train_model(
    data_package,
    config=None,
):
    """
    Train the STGNN model with progress bars

    Parameters:
    -----------
    data_loaders : dict
        Dict containing train_loader and val_loader
    config : ExperimentConfig, optional
        Centralized configuration object. If not provided, will use global config.
    **kwargs : dict
        Additional parameters to override config settings

    Returns:
    --------
    dict
        Dictionary containing trained model and training metrics
    """

    # Get configuration
    if config is None:
        config = get_config()

        # Validate data package
    validate_data_package(
        data_package,
        required_components=["train_loader", "val_loader"],
        mode="training",
    )

    # Extract components for use
    train_loader = data_package["data_loaders"]["train_loader"]
    val_loader = data_package["data_loaders"]["val_loader"]

    num_epochs = config.training.num_epochs
    patience = config.training.patience

    # Determine device (use config or auto-detect)
    if config.training.device:
        device = torch.device(config.training.device)
    else:
        device = torch.device(
            "mps"
            if torch.backends.mps.is_available()
            else ("cuda" if torch.cuda.is_available() else "cpu")
        )
    print(f"Using device: {device}")

    # Create model
    model = create_stgnn_model(config)

    # Create trainer with tqdm support
    trainer = TqdmSTGNNTrainer(model, config)

    # Training loop with early stopping and overall progress bar
    train_losses = []
    val_losses = []
    best_val_loss = float("inf")
    best_model = None
    no_improve_count = 0

    # Use trange for overall epoch progress
    epochs_pbar = trange(num_epochs, desc="Training progress")

    for epoch in epochs_pbar:
        # Train
        train_loss = trainer.train_epoch(train_loader)
        train_losses.append(train_loss)

        # Validate
        val_loss = trainer.evaluate(val_loader)
        val_losses.append(val_loss)

        # Update progress bar with current metrics
        epochs_pbar.set_postfix(
            {
                "train_loss": f"{train_loss:.6f}",
                "val_loss": f"{val_loss:.6f}",
                "no_improve": no_improve_count,
            }
        )

        # Check for improvement
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model = model.state_dict().copy()
            no_improve_count = 0
        else:
            no_improve_count += 1

        # Early stopping
        if no_improve_count >= patience:
            print(f"Early stopping after {epoch+1} epochs")
            break

    # Load best model
    if best_model is not None:
        model.load_state_dict(best_model)

    # Plot training curve
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label="Training Loss")
    plt.plot(val_losses, label="Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Training and Validation Loss")
    plt.tight_layout()

    # Save model and configuration together
    if hasattr(config.paths, "model_save_path") and config.paths.model_save_path:

        model_dir = (
            config.paths.model_save_path / f"{config.experiment.name.replace(' ', '_')}"
        )
        save_model_with_config(model, config, model_dir)
        print(f"Model and configuration saved to {model_dir}")

    return {
        "model": model,
        "train_losses": train_losses,
        "val_losses": val_losses,
        "best_val_loss": best_val_loss,
    }


def predict_and_evaluate(model, dataloader, device=None):
    """
    Make predictions with the trained model and evaluate performance

    Parameters:
    -----------
    model : STGNN
        Trained model
    dataloader : DataLoader
        Dataloader containing test data
    device : torch.device
        Device to use for inference

    Returns:
    --------
    Dict containing predictions and evaluation metrics
    """
    if device is None:
        device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

    model.to(device)
    model.eval()

    all_preds = []
    all_targets = []
    all_masks = []

    with torch.no_grad():
        # Add progress bar for evaluation
        for batch in tqdm(dataloader, desc="Evaluating model"):
            # Move data to device
            x = batch["x"].to(device)
            x_mask = batch["x_mask"].to(device)
            y = batch["y"].to(device)
            y_mask = batch["y_mask"].to(device)
            adj = batch["adj"].to(device)

            # Forward pass
            y_pred = model(x, adj, x_mask)

            # Move predictions and targets to CPU
            all_preds.append(y_pred.cpu().numpy())
            all_targets.append(y.cpu().numpy())
            all_masks.append(y_mask.cpu().numpy())

    # Concatenate batches
    predictions = np.concatenate(all_preds, axis=0)
    targets = np.concatenate(all_targets, axis=0)
    masks = np.concatenate(all_masks, axis=0)

    # Compute metrics on valid points only
    mse = np.mean(((predictions - targets) ** 2) * masks) / np.mean(masks)
    mae = np.mean(np.abs(predictions - targets) * masks) / np.mean(masks)

    print(f"Test MSE: {mse:.6f}")
    print(f"Test MAE: {mae:.6f}")

    return {
        "predictions": predictions,
        "targets": targets,
        "masks": masks,
        "mse": mse,
        "mae": mae,
    }


def cross_validate_model(data=None, data_file=None, config=None):
    """
    Train and evaluate model using time-based cross-validation.

    Parameters:
    -----------
    data : dict, optional
        Dictionary mapping sensor IDs to their time series data
    data_file : str, optional
        Path to a pickled file containing sensor data
    config : ExperimentConfig, optional
        Centralized configuration object
    Returns:
    --------
    dict
        Dictionary containing cross-validation results
    """
    # Get configuration
    if config is None:
        config = get_config()

    # Allow override of config parameters with kwargs
    n_splits = config.data.training.n_splits

    # Load sensor data if not provided
    if data is None:
        if data_file is None:
            raise ValueError("Either data or data_file must be provided")
        data = preprocessing.load_sensor_data(data_file)

    # Resample data to consistent frequency
    resampled_data = preprocessing.resample_sensor_data(
        data,
    )

    # Create processor with appropriate settings
    processor = preprocessing.TimeSeriesPreprocessor()

    # Create rolling window splits
    print(f"Creating {n_splits} time-based cross-validation splits...")
    splits = processor.create_rolling_window_splits(
        resampled_data,
        config=config,
    )

    print(f"Generated {len(splits)} valid split(s)")

    # Train and evaluate on each split
    results = []
    for i, split in enumerate(tqdm(splits, desc="Training on CV splits")):
        print(f"\nTraining on split {i+1}/{len(splits)}")

        # Process this split into windows
        split_data_loaders = preprocess_data(
            data=split,  # Pass the split directly
            config=config,
        )

        # Train on this split
        split_results = train_model(
            data_package=split_data_loaders,
            config=config,
        )

        # Save results for this split
        results.append(
            {
                "split": i,
                "train_losses": split_results["train_losses"],
                "val_losses": split_results["val_losses"],
                "best_val_loss": split_results["best_val_loss"],
            }
        )

    # Calculate overall metrics
    best_val_losses = [r["best_val_loss"] for r in results]

    return {
        "results": results,
        "mean_val_loss": np.mean(best_val_losses),
        "std_val_loss": np.std(best_val_losses),
        "min_val_loss": np.min(best_val_losses),
        "max_val_loss": np.max(best_val_losses),
    }


def save_model(model, file_path):
    """Save the trained model"""
    torch.save(model.state_dict(), file_path)
    print(f"Model saved to {file_path}")


def train_model_with_cv(data_loaders, config=None):
    """Train model with cross-validation support."""

    if "train_loaders" in data_loaders:  # Cross-validation mode
        cv_results = []

        for i, (train_loader, val_loader) in enumerate(
            zip(data_loaders["train_loaders"], data_loaders["val_loaders"])
        ):

            print(f"Training on split {i+1}/{len(data_loaders['train_loaders'])}")

            # Create a new model for each split
            model = create_stgnn_model(config=config)

            # Train on this split
            split_results = train_model(
                {
                    "data_loader": {
                        "train_loader": train_loader,
                        "val_loader": val_loader,
                    }
                },
                config=config,
            )

            cv_results.append(split_results)

        # Aggregate results across splits
        avg_val_loss = sum(r["best_val_loss"] for r in cv_results) / len(cv_results)

        return {
            "cv_results": cv_results,
            "avg_val_loss": avg_val_loss,
            "best_model_index": np.argmin([r["best_val_loss"] for r in cv_results]),
        }

    else:  # Standard single split mode
        return train_model(data_loaders, config=config)




================================================
File: src/tuning/__init__.py
================================================
# gnn_package/src/tuning/__init__.py

from .tuning_utils import (
    tune_hyperparameters,
    get_best_params,
    load_tuning_results,
    run_multi_stage_tuning,
)
from .parameter_space import (
    get_default_param_space,
    get_focused_param_space,
)
from .experiment_manager import (
    setup_mlflow_experiment,
    log_best_trial_details,
    save_config_from_params,
)

__all__ = [
    "tune_hyperparameters",
    "get_best_params",
    "load_tuning_results",
    "get_default_param_space",
    "get_focused_param_space",
    "setup_mlflow_experiment",
    "log_best_trial_details",
    "save_config_from_params",
    "run_multi_stage_tuning",
]



================================================
File: src/tuning/experiment_manager.py
================================================
# gnn_package/src/tuning/experiment_manager.py

import os
import json
import yaml
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List, Union

import mlflow
import mlflow.pytorch
import optuna
import pandas as pd
from tabulate import tabulate

from gnn_package.config import get_config

from gnn_package.config import ExperimentConfig

logger = logging.getLogger(__name__)


def setup_mlflow_experiment(
    experiment_name: str, output_dir: Optional[Union[str, Path]] = None
) -> str:
    """
    Set up MLflow experiment and configure tracking.

    Parameters:
    -----------
    experiment_name : str
        Name of the MLflow experiment
    output_dir : str or Path, optional
        Directory to store MLflow data

    Returns:
    --------
    str
        MLflow experiment ID
    """
    # Configure MLflow tracking - use local directory if not specified
    if output_dir is None:
        output_dir = Path("mlruns")
    else:
        output_dir = Path(output_dir) / "mlruns"

    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Set MLflow tracking URI to a local directory
    tracking_uri = f"file://{output_dir.absolute()}"
    mlflow.set_tracking_uri(tracking_uri)

    # Create or get the experiment
    try:
        experiment = mlflow.get_experiment_by_name(experiment_name)
        if experiment is None:
            experiment_id = mlflow.create_experiment(experiment_name)
            logger.info(
                f"Created new experiment '{experiment_name}' with ID: {experiment_id}"
            )
        else:
            experiment_id = experiment.experiment_id
            logger.info(
                f"Using existing experiment '{experiment_name}' with ID: {experiment_id}"
            )
    except Exception as e:
        logger.error(f"Error setting up MLflow experiment: {str(e)}")
        # Fallback - create a new experiment with a timestamp to avoid conflicts
        import time

        experiment_name = f"{experiment_name}_{int(time.time())}"
        experiment_id = mlflow.create_experiment(experiment_name)
        logger.info(
            f"Created fallback experiment '{experiment_name}' with ID: {experiment_id}"
        )

    # Set the active experiment
    mlflow.set_experiment(experiment_name)

    return experiment_id


def log_trial_metrics(
    metrics: Dict[str, Any], standardization_stats: Dict[str, Any]
) -> None:
    """
    Log metrics from a trial to MLflow.

    Parameters:
    -----------
    metrics : Dict[str, Any]
        Dictionary of metrics to log
    standardization_stats : Dict[str, Any]
        Dictionary of standardization statistics from preprocessing
    """
    for name, value in metrics.items():
        if isinstance(value, (int, float)):
            mlflow.log_metric(name, value)

    # Log standardization statistics separately
    if standardization_stats:
        mlflow.log_param("standardization_mean", standardization_stats.get("mean", 0))
        mlflow.log_param("standardization_std", standardization_stats.get("std", 1))


def save_config_from_params(
    params: Dict[str, Any],
    output_path: Union[str, Path],
    base_config: Optional[ExperimentConfig] = None,
) -> None:
    """
    Create and save a configuration file from tuned parameters.

    Parameters:
    -----------
    params : Dict[str, Any]
        Dictionary of tuned parameters
    output_path : str or Path
        Path to save the configuration file
    base_config : ExperimentConfig, optional
        Base configuration to update with params
    """
    # Get base config
    if base_config is None:
        config = get_config()
    else:
        config = base_config

    # Update config with tuned parameters
    for param_name, value in params.items():
        parts = param_name.split(".")
        if len(parts) == 2:
            section, attribute = parts
            config_dict = config._config_dict
            if section in config_dict and attribute in config_dict[section]:
                config_dict[section][attribute] = value

    # Save updated config
    with open(output_path, "w") as f:
        yaml.dump(config._config_dict, f, default_flow_style=False)


def log_best_trial_details(
    study: optuna.study.Study,
    experiment_name: str,
    output_dir: Union[str, Path],
    include_time_series: bool = True,
) -> None:
    """
    Log details about the best trial and generate summary reports.

    Parameters:
    -----------
    study : optuna.study.Study
        Completed Optuna study
    experiment_name : str
        Name of the experiment
    output_dir : str or Path
        Directory to save results
    include_time_series : bool
        Whether to include training curves in the report
    """
    output_dir = Path(output_dir)
    os.makedirs(output_dir, exist_ok=True)

    # Get best trial
    best_trial = study.best_trial

    # Save best parameters
    best_params = best_trial.params
    best_params_path = output_dir / "best_params.json"
    with open(best_params_path, "w") as f:
        json.dump(best_params, f, indent=2)

    # Generate detailed report for the best trial
    best_trial_report = {
        "trial_number": best_trial.number,
        "value": best_trial.value,
        "params": best_trial.params,
        "datetime_start": best_trial.datetime_start.isoformat(),
        "datetime_complete": best_trial.datetime_complete.isoformat(),
        "duration_seconds": (
            best_trial.datetime_complete - best_trial.datetime_start
        ).total_seconds(),
    }

    # Add user attributes (if any)
    for key, value in best_trial.user_attrs.items():
        best_trial_report[key] = value

    # Save best trial report
    best_trial_report_path = output_dir / "best_trial_report.json"
    with open(best_trial_report_path, "w") as f:
        json.dump(best_trial_report, f, indent=2)

    # Generate summary of all trials
    trial_data = []
    for trial in study.trials:
        if trial.state == optuna.trial.TrialState.COMPLETE:
            row = {
                "number": trial.number,
                "value": trial.value,
                **trial.params,
                "duration_seconds": (
                    trial.datetime_complete - trial.datetime_start
                ).total_seconds(),
            }
            trial_data.append(row)

    # Create DataFrame and sort by performance
    if trial_data:
        trials_df = pd.DataFrame(trial_data)
        trials_df = trials_df.sort_values("value")

        # Save as CSV
        trials_csv_path = output_dir / "all_trials.csv"
        trials_df.to_csv(trials_csv_path, index=False)

        # Create a text report for better readability
        trials_report_path = output_dir / "trials_summary.txt"
        with open(trials_report_path, "w") as f:
            f.write(f"# Hyperparameter Tuning Report: {experiment_name}\n\n")
            f.write(f"Total trials: {len(study.trials)}\n")
            f.write(f"Completed trials: {len(trials_df)}\n")
            f.write(f"Best trial: #{best_trial.number}\n")
            f.write(f"Best value: {best_trial.value}\n\n")

            f.write("## Best Parameters\n\n")
            best_params_table = [[param, value] for param, value in best_params.items()]
            f.write(
                tabulate(
                    best_params_table, headers=["Parameter", "Value"], tablefmt="grid"
                )
            )
            f.write("\n\n")

            f.write("## Top 10 Trials\n\n")
            top_10_trials = trials_df.head(10)
            # Select relevant columns for the report
            cols_to_show = (
                ["number", "value"] + list(best_params.keys()) + ["duration_seconds"]
            )
            cols_to_show = [col for col in cols_to_show if col in top_10_trials.columns]
            f.write(
                tabulate(
                    top_10_trials[cols_to_show].values,
                    headers=cols_to_show,
                    tablefmt="grid",
                )
            )
            f.write("\n\n")

            f.write("## Parameter Importance\n\n")
            try:
                importances = optuna.importance.get_param_importances(study)
                importance_table = [
                    [param, importance] for param, importance in importances.items()
                ]
                f.write(
                    tabulate(
                        importance_table,
                        headers=["Parameter", "Importance"],
                        tablefmt="grid",
                    )
                )
            except Exception as e:
                f.write(f"Could not compute parameter importance: {str(e)}\n")

    logger.info(f"Trial reports saved to {output_dir}")

    # Save a copy of the best configuration
    try:
        from gnn_package.config import get_config

        config = get_config()
        best_config_path = output_dir / "best_config.yml"
        save_config_from_params(best_params, best_config_path, config)
        logger.info(f"Best configuration saved to {best_config_path}")
    except Exception as e:
        logger.error(f"Error saving best configuration: {str(e)}")



================================================
File: src/tuning/objective.py
================================================
# gnn_package/src/tuning/objective.py

import os
import copy
import pickle
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Union, Tuple

import mlflow
import optuna
import torch
import numpy as np

from gnn_package.config import ExperimentConfig, get_config
from gnn_package import training
from .parameter_space import get_param_space_with_suggestions
from .experiment_manager import log_trial_metrics

logger = logging.getLogger(__name__)


def update_config_with_params(
    config: ExperimentConfig, params: Dict[str, Any]
) -> ExperimentConfig:
    """
    Update configuration object with parameters from the tuning process.

    Parameters:
    -----------
    config : ExperimentConfig
        Original configuration object
    params : Dict[str, Any]
        Parameters to update in the configuration

    Returns:
    --------
    ExperimentConfig
        Updated configuration object
    """
    # Create a deep copy to avoid modifying the original
    updated_config = copy.deepcopy(config)

    # Update each parameter
    for param_name, value in params.items():
        # Split parameter name into sections
        parts = param_name.split(".")

        # Handle nested attributes
        if len(parts) == 2:
            section, attribute = parts
            if hasattr(updated_config, section) and hasattr(
                getattr(updated_config, section), attribute
            ):
                setattr(getattr(updated_config, section), attribute, value)
            else:
                logger.warning(
                    f"Could not update {param_name}: attribute not found in config"
                )
        elif len(parts) == 1:
            # Handle top-level attributes
            if hasattr(updated_config, param_name):
                setattr(updated_config, param_name, value)
            else:
                logger.warning(
                    f"Could not update {param_name}: attribute not found in config"
                )
        else:
            logger.warning(f"Unexpected parameter format: {param_name}")

    return updated_config


def create_objective_function(
    data_file: Union[str, Path],
    param_space: Dict[str, Any],
    experiment_name: str,
    config: Optional[ExperimentConfig] = None,
    n_epochs: Optional[int] = None,
):
    """
    Create an objective function for Optuna to optimize.

    Parameters:
    -----------
    data_file : str or Path
        Path to the data file
    param_space : Dict[str, Any]
        Parameter space definition
    experiment_name : str
        Name of the MLflow experiment
    config : ExperimentConfig, optional
        Base configuration to use (falls back to global config)
    n_epochs : int, optional
        Number of epochs to train (overrides config)

    Returns:
    --------
    Callable
        Objective function that takes an Optuna trial
    """
    if config is None:
        config = get_config()

    # Function to be optimized
    def objective(trial: optuna.trial.Trial) -> float:
        # Get parameter suggestions for this trial
        params = get_param_space_with_suggestions(trial, param_space)

        # Create a new MLflow run for each trial
        with mlflow.start_run(run_name=f"trial_{trial.number}", nested=True):
            # Log parameters to MLflow
            mlflow.log_params(params)

        # Update configuration with sampled parameters
        trial_config = update_config_with_params(config, params)

        # Override number of epochs if specified
        if n_epochs is not None:
            trial_config.training.num_epochs = n_epochs

        # Set a shorter patience for faster hyperparameter tuning
        # This reduces training time while still identifying promising configurations
        trial_config.training.patience = min(
            trial_config.training.patience,
            max(3, int(trial_config.training.num_epochs * 0.2)),
        )

        try:
            # Preprocess data with the current configuration
            data_package = training.preprocess_data(
                data_file=data_file,
                config=trial_config,
            )

            # Extract standardization stats if available
            standardization_stats = {}
            if hasattr(data_package["metadata"], "preprocessing_stats"):
                standardization_stats = data_package.preprocessing_stats.get(
                    "standardization", {}
                )

            # Add to trial attributes for later analysis
            trial.set_user_attr("standardization_stats", standardization_stats)

            # Train model with updated config
            results = training.train_model(
                data_package=data_package,
                config=trial_config,
            )

            # Get validation loss as the optimization target
            best_val_loss = results["best_val_loss"]

            # Calculate additional metrics
            metrics = {
                "best_val_loss": best_val_loss,
                "final_train_loss": results["train_losses"][-1],
                "final_val_loss": results["val_losses"][-1],
                "num_epochs_trained": len(results["train_losses"]),
                "stopped_early": len(results["train_losses"])
                < trial_config.training.num_epochs,
            }

            # Log metrics to MLflow
            log_trial_metrics(metrics)

            # Report to the trial
            trial.set_user_attr("metrics", metrics)

            # Free up memory
            del data_package
            del results
            torch.mps.empty_cache() if torch.backends.mps.is_available() else None

            return best_val_loss

        except Exception as e:
            # Log failed trial
            logger.error(f"Trial {trial.number} failed: {str(e)}")
            mlflow.log_param("error", str(e))
            raise optuna.exceptions.TrialPruned(f"Trial failed: {str(e)}")

    return objective


async def train_with_best_params(
    data_file: Union[str, Path],
    best_params: Dict[str, Any],
    output_dir: Union[str, Path],
    config: Optional[ExperimentConfig] = None,
) -> Tuple[ExperimentConfig, Dict[str, Any]]:
    """
    Train the model with the best parameters found during tuning.

    Parameters:
    -----------
    data_file : str or Path
        Path to the data file
    best_params : Dict[str, Any]
        Best parameters found during tuning
    output_dir : str or Path
        Directory to save results
    config : ExperimentConfig, optional
        Base configuration to use (falls back to global config)

    Returns:
    --------
    Tuple[ExperimentConfig, Dict[str, Any]]
        Updated configuration and training results
    """
    if config is None:
        config = get_config()

    # Update configuration with best parameters
    best_config = update_config_with_params(config, best_params)

    # Save the best configuration
    output_dir = Path(output_dir)
    os.makedirs(output_dir, exist_ok=True)
    best_config_path = output_dir / "best_config.yml"
    best_config.save(str(best_config_path))

    # Preprocess data with best configuration
    with mlflow.start_run(run_name="best_params_training"):
        mlflow.log_params(best_params)

        data_package = training.preprocess_data(
            data_file=data_file,
            config=best_config,
        )

        # Extract standardization stats
        standardization_stats = {}
        if hasattr(data_package["metadata"], "preprocessing_stats"):
            standardization_stats = data_package.metadata.preprocessing_stats.get(
                "standardization", {}
            )

        mlflow.log_params(
            {
                "standardization_mean": standardization_stats.get("mean", 0),
                "standardization_std": standardization_stats.get("std", 1),
            }
        )

        # Train model with best configuration
        results = training.train_model(
            data_package=data_package,
            config=best_config,
        )

        # Log metrics
        metrics = {
            "best_val_loss": results["best_val_loss"],
            "final_train_loss": results["train_losses"][-1],
            "final_val_loss": results["val_losses"][-1],
            "num_epochs_trained": len(results["train_losses"]),
        }
        mlflow.log_metrics(metrics)

        # Save the best model
        model_path = output_dir / "best_model.pth"
        torch.save(results["model"].state_dict(), str(model_path))
        mlflow.log_artifact(str(model_path))

        # Save loss curve data
        loss_data = {
            "train_losses": results["train_losses"],
            "val_losses": results["val_losses"],
        }
        loss_path = output_dir / "training_curves.pkl"
        with open(loss_path, "wb") as f:
            pickle.dump(loss_data, f)
        mlflow.log_artifact(str(loss_path))

    return best_config, results



================================================
File: src/tuning/parameter_space.py
================================================
# gnn_package/src/tuning/parameter_space.py

from typing import Dict, Any, Callable, Optional
import optuna


def get_default_param_space() -> Dict[str, Callable[[optuna.trial.Trial], Any]]:
    """
    Define the default hyperparameter search space.
    Focuses on most impactful parameters for initial tuning.

    Returns:
    --------
    Dict[str, Callable]
        Dictionary mapping parameter names to trial suggest functions
    """
    param_space = {
        # Model architecture parameters
        "model.hidden_dim": lambda trial: trial.suggest_categorical(
            "model.hidden_dim", [32, 64, 128, 256]
        ),
        "model.num_layers": lambda trial: trial.suggest_int("model.num_layers", 1, 3),
        "model.num_gc_layers": lambda trial: trial.suggest_int(
            "model.num_gc_layers", 1, 3
        ),
        "model.dropout": lambda trial: trial.suggest_float("model.dropout", 0.1, 0.5),
        # Training parameters
        "training.learning_rate": lambda trial: trial.suggest_float(
            "training.learning_rate", 1e-4, 1e-2, log=True
        ),
        "training.weight_decay": lambda trial: trial.suggest_float(
            "training.weight_decay", 1e-6, 1e-3, log=True
        ),
    }

    return param_space


def get_focused_param_space(
    previous_best_params: Optional[Dict[str, Any]] = None,
) -> Dict[str, Callable[[optuna.trial.Trial], Any]]:
    """
    Define a more focused hyperparameter search space,
    optionally centered around previous best parameters.

    Parameters:
    -----------
    previous_best_params : Dict[str, Any], optional
        Best parameters from a previous tuning run

    Returns:
    --------
    Dict[str, Callable]
        Dictionary mapping parameter names to trial suggest functions
    """
    if previous_best_params is None:
        return get_default_param_space()

    # Create a more focused search around previous best values
    param_space = {}

    # Focus hidden_dim search
    if "model.hidden_dim" in previous_best_params:
        best_hidden = previous_best_params["model.hidden_dim"]
        # Get neighboring values, ensuring we stay within reasonable ranges
        hidden_options = [
            max(16, best_hidden // 2),
            best_hidden,
            min(512, best_hidden * 2),
        ]
        # Remove duplicates and sort
        hidden_options = sorted(list(set(hidden_options)))
        param_space["model.hidden_dim"] = lambda trial: trial.suggest_categorical(
            "model.hidden_dim", hidden_options
        )
    else:
        param_space["model.hidden_dim"] = lambda trial: trial.suggest_categorical(
            "model.hidden_dim", [64, 128, 256]
        )

    # Focus num_layers search
    if "model.num_layers" in previous_best_params:
        best_layers = previous_best_params["model.num_layers"]
        param_space["model.num_layers"] = lambda trial: trial.suggest_int(
            "model.num_layers", max(1, best_layers - 1), min(4, best_layers + 1)
        )
    else:
        param_space["model.num_layers"] = lambda trial: trial.suggest_int(
            "model.num_layers", 1, 3
        )

    # Focus num_gc_layers search
    if "model.num_gc_layers" in previous_best_params:
        best_gc_layers = previous_best_params["model.num_gc_layers"]
        param_space["model.num_gc_layers"] = lambda trial: trial.suggest_int(
            "model.num_gc_layers",
            max(1, best_gc_layers - 1),
            min(4, best_gc_layers + 1),
        )
    else:
        param_space["model.num_gc_layers"] = lambda trial: trial.suggest_int(
            "model.num_gc_layers", 1, 3
        )

    # Focus dropout search
    if "model.dropout" in previous_best_params:
        best_dropout = previous_best_params["model.dropout"]
        param_space["model.dropout"] = lambda trial: trial.suggest_float(
            "model.dropout", max(0.05, best_dropout - 0.1), min(0.6, best_dropout + 0.1)
        )
    else:
        param_space["model.dropout"] = lambda trial: trial.suggest_float(
            "model.dropout", 0.1, 0.5
        )

    # Focus learning rate search
    if "training.learning_rate" in previous_best_params:
        best_lr = previous_best_params["training.learning_rate"]
        param_space["training.learning_rate"] = lambda trial: trial.suggest_float(
            "training.learning_rate", best_lr / 3, best_lr * 3, log=True
        )
    else:
        param_space["training.learning_rate"] = lambda trial: trial.suggest_float(
            "training.learning_rate", 1e-4, 1e-2, log=True
        )

    # Focus weight decay search
    if "training.weight_decay" in previous_best_params:
        best_wd = previous_best_params["training.weight_decay"]
        param_space["training.weight_decay"] = lambda trial: trial.suggest_float(
            "training.weight_decay", best_wd / 5, best_wd * 5, log=True
        )
    else:
        param_space["training.weight_decay"] = lambda trial: trial.suggest_float(
            "training.weight_decay", 1e-6, 1e-3, log=True
        )

    return param_space


def get_param_space_with_suggestions(
    trial: optuna.trial.Trial,
    param_space: Dict[str, Callable[[optuna.trial.Trial], Any]],
) -> Dict[str, Any]:
    """
    Generate parameter values from the search space for a specific trial.

    Parameters:
    -----------
    trial : optuna.trial.Trial
        Current Optuna trial object
    param_space : Dict[str, Callable]
        Parameter space definition

    Returns:
    --------
    Dict[str, Any]
        Dictionary of parameter names and suggested values
    """
    params = {}
    for param_name, suggest_func in param_space.items():
        params[param_name] = suggest_func(trial)

    return params



================================================
File: src/tuning/tuning_utils.py
================================================
# gnn_package/src/tuning/tuning_utils.py

import os
import json
import logging
import pickle
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple

import mlflow
import optuna
import yaml
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from gnn_package.config import ExperimentConfig, get_config
from .parameter_space import get_default_param_space, get_focused_param_space
from .experiment_manager import (
    setup_mlflow_experiment,
    log_best_trial_details,
    save_config_from_params,
)
from .objective import create_objective_function, train_with_best_params

logger = logging.getLogger(__name__)


def tune_hyperparameters(
    data_file: Union[str, Path],
    experiment_name: str,
    n_trials: int = 20,
    n_epochs: Optional[int] = None,
    output_dir: Optional[Union[str, Path]] = None,
    config: Optional[ExperimentConfig] = None,
    param_space: Optional[Dict[str, Any]] = None,
    previous_best_params: Optional[Dict[str, Any]] = None,
    retrain_best: bool = True,
    study_name: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Run hyperparameter tuning with Optuna and MLflow.

    Parameters:
    -----------
    data_file : str or Path
        Path to the data file
    experiment_name : str
        Name of the MLflow experiment
    n_trials : int
        Number of trials to run
    n_epochs : int, optional
        Number of epochs to train (overrides config)
    output_dir : str or Path, optional
        Directory to save results (uses experiment_name if not provided)
    config : ExperimentConfig, optional
        Base configuration to use (falls back to global config)
    param_space : Dict[str, Any], optional
        Parameter space definition (uses default if not provided)
    previous_best_params : Dict[str, Any], optional
        Best parameters from a previous tuning run for focused search
    retrain_best : bool
        Whether to retrain with the best parameters after tuning
    study_name : str, optional
        Name for the Optuna study (uses experiment_name if not provided)

    Returns:
    --------
    Dict[str, Any]
        Best parameters and study information
    """
    # Setup MLflow experiment
    if output_dir is None:
        output_dir = Path(f"results/tuning/{experiment_name}")

    output_dir = Path(output_dir)
    os.makedirs(output_dir, exist_ok=True)

    # Set up MLflow experiment
    experiment_id = setup_mlflow_experiment(experiment_name, output_dir)

    # Set default config if not provided
    if config is None:
        config = get_config()

    # Set parameter space if not provided
    if param_space is None:
        if previous_best_params is not None:
            param_space = get_focused_param_space(previous_best_params)
        else:
            param_space = get_default_param_space()

    # Set study name if not provided
    if study_name is None:
        study_name = experiment_name

    # Create objective function
    objective = create_objective_function(
        data_file=data_file,
        param_space=param_space,
        experiment_name=experiment_name,
        config=config,
        n_epochs=n_epochs,
    )

    # Create Optuna storage and study
    storage_path = output_dir / f"{study_name}.db"
    storage = optuna.storages.RDBStorage(
        url=f"sqlite:///{storage_path}", engine_kwargs={"connect_args": {"timeout": 30}}
    )

    # Create or load existing study
    study = optuna.create_study(
        study_name=study_name,
        storage=storage,
        load_if_exists=True,
        direction="minimize",  # Minimize validation loss
        pruner=optuna.pruners.MedianPruner(
            n_startup_trials=5, n_warmup_steps=10, interval_steps=1
        ),
        sampler=optuna.samplers.TPESampler(seed=42),
    )

    # Check if study already has trials
    existing_trials = len(study.trials)
    if existing_trials > 0:
        logger.info(f"Loaded existing study with {existing_trials} trials")
        logger.info(f"Best value so far: {study.best_value}")

    # Run the optimization
    with mlflow.start_run(run_name=f"{study_name}_optimization"):
        logger.info(f"Starting hyperparameter optimization with {n_trials} trials")
        study.optimize(
            objective, n_trials=n_trials, timeout=None, show_progress_bar=True
        )

    # Get best parameters
    best_params = study.best_params
    best_value = study.best_value

    logger.info(f"Best trial: #{study.best_trial.number}")
    logger.info(f"Best validation loss: {best_value}")
    logger.info(f"Best parameters: {best_params}")

    # Save best parameters to file
    best_params_path = output_dir / "best_params.json"
    with open(best_params_path, "w") as f:
        json.dump(best_params, f, indent=2)

    # Generate reports
    log_best_trial_details(study, experiment_name, output_dir)

    # Create visualization plots
    try:
        # Optimization history
        fig = optuna.visualization.plot_optimization_history(study)
        fig.write_image(str(output_dir / "optimization_history.png"))

        # Parameter importance
        fig = optuna.visualization.plot_param_importances(study)
        fig.write_image(str(output_dir / "param_importances.png"))

        # Parallel coordinate plot for parameters
        fig = optuna.visualization.plot_parallel_coordinate(study)
        fig.write_image(str(output_dir / "parallel_coordinate.png"))

        # Slice plot for selected parameters
        for param in study.best_params.keys():
            if len(study.trials) > 10:  # Only if we have enough trials
                fig = optuna.visualization.plot_slice(study, params=[param])
                fig.write_image(str(output_dir / f"slice_{param}.png"))

        logger.info(f"Visualization plots saved to {output_dir}")
    except Exception as e:
        logger.warning(f"Error creating visualization plots: {str(e)}")

    # Retrain with best parameters if requested
    if retrain_best:
        logger.info("Retraining with best parameters...")
        best_model_dir = output_dir / "best_model"
        os.makedirs(best_model_dir, exist_ok=True)

        best_config, best_results = train_with_best_params(
            data_file=data_file,
            best_params=best_params,
            output_dir=best_model_dir,
            config=config,
        )

        # Plot loss curves
        plt.figure(figsize=(10, 6))
        plt.plot(best_results["train_losses"], label="Training Loss")
        plt.plot(best_results["val_losses"], label="Validation Loss")
        plt.axhline(
            y=best_results["best_val_loss"],
            color="r",
            linestyle="--",
            label=f"Best Val Loss: {best_results['best_val_loss']:.4f}",
        )
        plt.title(f"Training Results with Best Parameters")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig(
            str(best_model_dir / "loss_curves.png"), dpi=300, bbox_inches="tight"
        )

        logger.info(f"Best model results saved to {best_model_dir}")

    return {
        "best_params": best_params,
        "best_value": best_value,
        "study": study,
        "output_dir": str(output_dir),
    }


def get_best_params(
    output_dir: Union[str, Path],
    study_name: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Load the best parameters from a previous tuning run.

    Parameters:
    -----------
    output_dir : str or Path
        Directory with tuning results
    study_name : str, optional
        Name of the Optuna study (uses output_dir.name if not provided)

    Returns:
    --------
    Dict[str, Any]
        Best parameters
    """
    output_dir = Path(output_dir)

    # First try to load from best_params.json
    best_params_path = output_dir / "best_params.json"
    if best_params_path.exists():
        with open(best_params_path, "r") as f:
            return json.load(f)

    # If not found, try to load from Optuna storage
    if study_name is None:
        study_name = output_dir.name

    storage_path = output_dir / f"{study_name}.db"
    if storage_path.exists():
        storage = optuna.storages.RDBStorage(
            url=f"sqlite:///{storage_path}",
            engine_kwargs={"connect_args": {"timeout": 30}},
        )
        study = optuna.load_study(study_name=study_name, storage=storage)
        return study.best_params

    raise FileNotFoundError(f"Could not find best parameters in {output_dir}")


def load_tuning_results(
    output_dir: Union[str, Path],
    study_name: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Load results from a previous tuning run.

    Parameters:
    -----------
    output_dir : str or Path
        Directory with tuning results
    study_name : str, optional
        Name of the Optuna study (uses output_dir.name if not provided)

    Returns:
    --------
    Dict[str, Any]
        Dictionary containing study information and results
    """
    output_dir = Path(output_dir)

    if study_name is None:
        study_name = output_dir.name

    # Load Optuna study
    storage_path = output_dir / f"{study_name}.db"
    if not storage_path.exists():
        raise FileNotFoundError(f"Optuna storage not found: {storage_path}")

    storage = optuna.storages.RDBStorage(
        url=f"sqlite:///{storage_path}", engine_kwargs={"connect_args": {"timeout": 30}}
    )
    study = optuna.load_study(study_name=study_name, storage=storage)

    # Load trial data
    all_trials_path = output_dir / "all_trials.csv"
    if all_trials_path.exists():
        trials_df = pd.read_csv(all_trials_path)
    else:
        # Create from study
        trials_data = []
        for trial in study.trials:
            if trial.state == optuna.trial.TrialState.COMPLETE:
                row = {
                    "number": trial.number,
                    "value": trial.value,
                    **trial.params,
                    "duration_seconds": (
                        trial.datetime_complete - trial.datetime_start
                    ).total_seconds(),
                }
                trials_data.append(row)

        trials_df = pd.DataFrame(trials_data) if trials_data else None

    # Load best model results if available
    best_model_dir = output_dir / "best_model"
    best_model_results = None

    if best_model_dir.exists():
        training_curves_path = best_model_dir / "training_curves.pkl"
        if training_curves_path.exists():
            with open(training_curves_path, "rb") as f:
                best_model_results = pickle.load(f)

        best_model_path = best_model_dir / "best_model.pth"
        if best_model_path.exists():
            best_model_results = best_model_results or {}
            best_model_results["model_path"] = str(best_model_path)

    return {
        "study": study,
        "best_params": study.best_params,
        "best_value": study.best_value,
        "trials_df": trials_df,
        "best_model_results": best_model_results,
        "output_dir": str(output_dir),
    }


def run_multi_stage_tuning(
    data_file: Union[str, Path],
    experiment_name: str,
    output_dir: Optional[Union[str, Path]] = None,
    config: Optional[ExperimentConfig] = None,
    n_trials_stages: List[int] = [20, 10, 5],
    n_epochs_stages: List[Optional[int]] = [10, 20, None],
    data_fraction_stages: List[Optional[float]] = [0.25, 0.5, 1.0],
) -> Dict[str, Any]:
    """
    Run multi-stage hyperparameter tuning with progressively more data and epochs.

    Parameters:
    -----------
    data_file : str or Path
        Path to the data file
    experiment_name : str
        Base name for the MLflow experiment
    output_dir : str or Path, optional
        Directory to save results
    config : ExperimentConfig, optional
        Base configuration to use
    n_trials_stages : List[int]
        Number of trials to run in each stage
    n_epochs_stages : List[int]
        Number of epochs to train in each stage
    data_fraction_stages : List[float]
        Fraction of data to use in each stage

    Returns:
    --------
    Dict[str, Any]
        Best parameters and study information from the final stage
    """
    if output_dir is None:
        output_dir = Path(f"results/tuning/{experiment_name}")

    output_dir = Path(output_dir)
    os.makedirs(output_dir, exist_ok=True)

    # Ensure all stage parameter lists have the same length
    n_stages = len(n_trials_stages)
    if len(n_epochs_stages) != n_stages:
        raise ValueError("n_epochs_stages must have the same length as n_trials_stages")
    if len(data_fraction_stages) != n_stages:
        raise ValueError(
            "data_fraction_stages must have the same length as n_trials_stages"
        )

    # Initialize best params for the first stage
    previous_best_params = None
    final_results = None

    # Create a list to store results from each stage
    stage_results = []

    # Run each stage
    for i in range(n_stages):
        stage_name = f"stage_{i+1}"
        stage_experiment_name = f"{experiment_name}_{stage_name}"
        stage_output_dir = output_dir / stage_name

        # Prepare data for this stage - this is just a placeholder
        # In a real implementation, you'd sample/preprocess data according to data_fraction_stages[i]
        stage_data_file = data_file  # For now, we use the same data file

        logger.info(f"Starting tuning stage {i+1}/{n_stages}")
        logger.info(f"  Trials: {n_trials_stages[i]}")
        logger.info(f"  Epochs: {n_epochs_stages[i]}")
        logger.info(f"  Data fraction: {data_fraction_stages[i]}")

        # Run this stage of tuning
        results = tune_hyperparameters(
            data_file=stage_data_file,
            experiment_name=stage_experiment_name,
            n_trials=n_trials_stages[i],
            n_epochs=n_epochs_stages[i],
            output_dir=stage_output_dir,
            config=config,
            previous_best_params=previous_best_params,
            retrain_best=(i == n_stages - 1),  # Only retrain on final stage
            study_name=stage_name,
        )

        # Store results for this stage
        stage_results.append(
            {
                "stage": i + 1,
                "experiment_name": stage_experiment_name,
                "best_params": results["best_params"],
                "best_value": results["best_value"],
            }
        )

        # Update best params for next stage
        previous_best_params = results["best_params"]
        final_results = results

    # Save summary of all stages
    stages_summary_path = output_dir / "stages_summary.json"
    with open(stages_summary_path, "w") as f:
        json.dump(stage_results, f, indent=2)

    # Create comparison plot of stages
    plt.figure(figsize=(10, 6))
    stages = [f"Stage {i+1}" for i in range(n_stages)]
    best_values = [stage["best_value"] for stage in stage_results]

    plt.bar(stages, best_values, color="skyblue")
    plt.title("Best Validation Loss by Tuning Stage")
    plt.xlabel("Stage")
    plt.ylabel("Validation Loss")
    plt.xticks(rotation=0)

    for i, value in enumerate(best_values):
        plt.text(i, value, f"{value:.4f}", ha="center", va="bottom")

    plt.savefig(str(output_dir / "stages_comparison.png"), dpi=300, bbox_inches="tight")

    return {
        "stage_results": stage_results,
        "final_results": final_results,
        "output_dir": str(output_dir),
        "best_params": previous_best_params,
    }




================================================
File: src/utils/__init__.py
================================================




================================================
File: src/utils/config_utils.py
================================================
import copy
import yaml
from pathlib import Path
from typing import Dict, Any, Optional
import tempfile
import torch

from gnn_package.src.models.stgnn import create_stgnn_model
from gnn_package.config import ExperimentConfig, get_config


def create_prediction_config_from_training(
    training_config: ExperimentConfig,
    override_params: Optional[Dict[str, Any]] = None,
) -> ExperimentConfig:
    """
    Create a prediction-focused configuration from a training configuration.
    Preserves model architecture and general data parameters but applies prediction-specific settings.

    Parameters:
    -----------
    training_config : ExperimentConfig
        The configuration used for training
    override_params : Dict[str, Any], optional
        Additional parameters to override in the configuration

    Returns:
    --------
    ExperimentConfig
        A new configuration optimized for prediction
    """
    # Create a deep copy of the configuration dictionary
    config_dict = copy.deepcopy(training_config._config_dict)

    # Update training settings to be more suitable for prediction
    if "data" in config_dict and "training" in config_dict["data"]:
        config_dict["data"]["training"]["use_cross_validation"] = False
        config_dict["data"]["training"]["cv_split_index"] = 0

    # Apply any override parameters
    if override_params:
        for key, value in override_params.items():
            parts = key.split(".")
            if (
                len(parts) == 3 and parts[0] == "data"
            ):  # e.g. "data.prediction.days_back"
                _, section, param = parts
                config_dict["data"][section][param] = value
            elif len(parts) == 2:  # e.g. "model.dropout"
                section, param = parts
                config_dict[section][param] = value

    # Create a temporary config file

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yml", delete=False) as temp:
        yaml.dump(config_dict, temp, default_flow_style=False)
        temp_path = temp.name

    # Load the new configuration
    new_config = ExperimentConfig(temp_path)

    # Clean up temporary file
    Path(temp_path).unlink()

    return new_config


def save_model_with_config(model, config, path):
    """
    Save model and its configuration together.

    Parameters:
    -----------
    model : torch.nn.Module
        The model to save
    config : ExperimentConfig
        The configuration used to create the model
    path : str or Path
        Directory path where to save the model and config
    """
    path = Path(path)
    path.mkdir(parents=True, exist_ok=True)

    # Save model
    torch.save(model.state_dict(), path / "model.pth")

    # Save configuration
    config.save(path / "config.yml")


def load_model_for_prediction(
    model_path, config=None, override_params=None, model_creator_func=None
):
    """
    Load a model with the appropriate configuration for prediction.

    Parameters:
    -----------
    model_path : str or Path
        Path to the saved model file
    config : ExperimentConfig, optional
        Configuration object. If None, attempts to find config in model directory
        or falls back to global config.
    override_params : Dict[str, Any], optional
        Parameters to override in the loaded configuration
    model_creator_func : Callable, optional
        Function to create model from config. If not provided, uses default creator

    Returns:
    --------
    tuple(torch.nn.Module, ExperimentConfig)
        The loaded model and its configuration
    """
    model_path = Path(model_path)

    # Configuration handling
    if config is None:
        # Try to find config in the model directory
        potential_config_path = model_path.parent / "config.yml"
        if potential_config_path.exists():
            config = ExperimentConfig(potential_config_path)
        else:
            # Fall back to global config
            config = get_config()

    # Convert training config to prediction config if needed
    config = create_prediction_config_from_training(config, override_params)

    # Create model with correct architecture
    if model_creator_func is None:
        model_creator_func = create_stgnn_model

    model = model_creator_func(config)

    # Load saved weights
    model.load_state_dict(torch.load(model_path))

    return model, config


def create_prediction_config(
    training_config_path: Optional[Path] = None,
) -> ExperimentConfig:
    """
    Create a prediction-specific configuration.

    Parameters:
    -----------
    training_config_path : Path, optional
        Path to training configuration. If None, uses default config.

    Returns:
    --------
    ExperimentConfig
        Configuration optimized for prediction
    """
    from gnn_package.config import ExperimentConfig, get_config

    if training_config_path is None:
        # Use default config
        config = get_config()
    else:
        # Load from provided path
        config = ExperimentConfig(str(training_config_path))

    # Create new config with prediction flag set
    prediction_config = ExperimentConfig(
        config_path=str(config.config_path), is_prediction_mode=True
    )

    return prediction_config



================================================
File: src/utils/data_utils.py
================================================
# gnn_package/src/utils/data_utils.py

import pandas as pd
import numpy as np


def read_pickled_gdf(dir_path, file_name):
    """
    Read a pickled GeoDataFrame from a specified directory.
    Parameters:
    ----------
    dir_path : str
        Directory path where the GeoDataFrame is stored
    file_name : str
        Name of the pickled GeoDataFrame file
    Returns:
    -------
    GeoDataFrame
        The loaded GeoDataFrame
    Raises:
    -------
    FileNotFoundError
        If the file doesn't exist
    """
    cropped_gdf = pd.read_pickle(dir_path + file_name)
    return cropped_gdf


def convert_numpy_types(obj):
    """
    Recursively convert numpy types to Python native types for JSON serialization.
    """
    if isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return convert_numpy_types(obj.tolist())
    elif isinstance(obj, np.bool_):
        return bool(obj)
    else:
        return obj


def validate_data_package(data_package, required_components=None, mode=None):
    """
    Validate that a data package has the required structure and components.

    Parameters:
    -----------
    data_package : dict
        The data package to validate
    required_components : list, optional
        List of required components, can include:
        - 'train_loader': Training data loader
        - 'val_loader': Validation data loader
        - 'adj_matrix': Adjacency matrix
        - 'node_ids': Node IDs
        - 'time_series': Time series data
    mode : str, optional
        Expected mode ('training' or 'prediction')

    Returns:
    --------
    dict
        The validated data package

    Raises:
    -------
    ValueError
        If the data package is invalid or missing required components
    """
    # Default required components if not specified
    if required_components is None:
        required_components = []

    # Basic validation
    if not isinstance(data_package, dict):
        raise ValueError("data_package must be a dictionary")

    # Check top-level keys
    expected_keys = ["data_loaders", "graph_data", "time_series", "metadata"]
    missing_keys = [key for key in expected_keys if key not in data_package]
    if missing_keys:
        raise ValueError(
            f"data_package is missing required keys: {', '.join(missing_keys)}"
        )

    # Check data_loaders structure
    data_loaders = data_package.get("data_loaders", {})
    if not isinstance(data_loaders, dict):
        raise ValueError("data_loaders must be a dictionary")

    # Check required components
    if "train_loader" in required_components and "train_loader" not in data_loaders:
        raise ValueError("data_loaders must contain 'train_loader'")

    if "val_loader" in required_components and "val_loader" not in data_loaders:
        raise ValueError("data_loaders must contain 'val_loader'")

    # Check graph_data structure
    graph_data = data_package.get("graph_data", {})
    if not isinstance(graph_data, dict):
        raise ValueError("graph_data must be a dictionary")

    if "adj_matrix" in required_components and "adj_matrix" not in graph_data:
        raise ValueError("graph_data must contain 'adj_matrix'")

    if "node_ids" in required_components and "node_ids" not in graph_data:
        raise ValueError("graph_data must contain 'node_ids'")

    # Check metadata
    metadata = data_package.get("metadata", {})
    if not isinstance(metadata, dict):
        raise ValueError("metadata must be a dictionary")

    # Check mode if specified
    if mode is not None and metadata.get("mode") != mode:
        raise ValueError(f"Expected mode '{mode}' but found '{metadata.get('mode')}'")

    # All validation passed
    return data_package



================================================
File: src/utils/sensor_utils.py
================================================
# gnn_package/src/utils/sensor_utils.py
import os
import json
from pathlib import Path
import pandas as pd
from gnn_package.config.paths import SENSORS_DATA_DIR  # Import from paths module

from private_uoapi import LSConfig, LSAuth, LightsailWrapper

from gnn_package.config import get_config


def get_sensor_name_id_map(config=None):
    """
    Create unique IDs for each sensor from the private UOAPI.

    location: id

    For the private API, where no IDs are provided, we generate
    unique IDs of the form '1XXXX' where XXXX is a zero-padded
    index (e.g. i=1 > 10001 and i=100 > 10100).

    Returns:
    dict: Mapping between sensor names (keys) and IDs (values)
    """

    # Get configuration
    if config is None:
        config = get_config()

    sensor_id_prefix = config.data.general.sensor_id_prefix

    # Check if the mapping file already exists
    if not os.path.exists(SENSORS_DATA_DIR / "sensor_name_id_map.json"):

        config = LSConfig()
        auth = LSAuth(config)
        client = LightsailWrapper(config, auth)
        sensors = client.get_traffic_sensors()

        sensors = pd.DataFrame(sensors)

        # Create mapping using configured format
        mapping = {
            location: f"{sensor_id_prefix}{str(i).zfill(4)}"
            for i, location in enumerate(sensors["location"])
        }

        # Save the mapping to a JSON file
        print("Saving sensor name to ID mapping to file.")
        with open(
            SENSORS_DATA_DIR / "sensor_name_id_map.json",
            "w",
            encoding="utf-8",
        ) as f:
            json.dump(mapping, f, indent=4)
    else:
        # Load the mapping from the JSON file
        print("Loading sensor name to ID mapping from file.")
        with open(
            SENSORS_DATA_DIR / "sensor_name_id_map.json",
            "r",
            encoding="utf-8",
        ) as f:
            mapping = json.load(f)

    return mapping




================================================
File: src/visualization/dashboard.py
================================================
# gnn_package/src/visualization/dashboard.py

from pathlib import Path
import os
import pandas as pd
from datetime import datetime
import base64
import io
from typing import List, Dict, Any, Optional, Union


def encode_image_to_base64(image_path):
    """Convert an image to a base64 string for embedding in HTML"""
    with open(image_path, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode("utf-8")
    return encoded_string


def generate_dashboard(
    results_path: Union[str, Path],
    prediction_results: Dict[str, Any],
    include_images: bool = True,
) -> str:
    """
    Generate an HTML dashboard for prediction results.

    Parameters:
    -----------
    results_path : str or Path
        Directory containing prediction results
    prediction_results : dict
        Dictionary with prediction information
    include_images : bool
        Whether to embed images in the HTML

    Returns:
    --------
    str
        HTML content of the dashboard
    """
    results_path = Path(results_path)

    # Get data from prediction results
    metrics = prediction_results.get("metrics", {})
    viz_paths = prediction_results.get("visualizations", {})
    predictions_file = prediction_results.get("predictions_file")
    summary_file = prediction_results.get("summary_file")

    # Load summary text if available
    summary_text = ""
    if summary_file and os.path.exists(summary_file):
        with open(summary_file, "r") as f:
            summary_text = f.read()

    # Create HTML content
    html_content = f"""
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Traffic Prediction Dashboard</title>
        <style>
            body {{
                font-family: Arial, sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 20px;
                color: #333;
            }}
            .container {{
                max-width: 1200px;
                margin: 0 auto;
            }}
            header {{
                background-color: #4472C4;
                color: white;
                padding: 10px 20px;
                margin-bottom: 20px;
                border-radius: 5px;
            }}
            h1, h2, h3 {{
                color: #4472C4;
            }}
            .summary-box {{
                background-color: #f9f9f9;
                border: 1px solid #ddd;
                padding: 15px;
                border-radius: 5px;
                margin-bottom: 20px;
                white-space: pre-wrap;
                font-family: monospace;
            }}
            .metrics {{
                display: flex;
                justify-content: space-around;
                margin-bottom: 20px;
            }}
            .metric-card {{
                background-color: #f2f2f2;
                border-radius: 5px;
                padding: 15px;
                text-align: center;
                width: 150px;
            }}
            .metric-value {{
                font-size: 24px;
                font-weight: bold;
                color: #4472C4;
            }}
            .metric-label {{
                font-size: 14px;
                color: #666;
            }}
            .viz-container {{
                margin-bottom: 30px;
            }}
            img {{
                max-width: 100%;
                height: auto;
                border: 1px solid #ddd;
                border-radius: 5px;
            }}
            footer {{
                margin-top: 30px;
                border-top: 1px solid #ddd;
                padding-top: 10px;
                color: #666;
                font-size: 12px;
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <header>
                <h1>Traffic Prediction Dashboard</h1>
                <p>Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </header>

            <section>
                <h2>Summary</h2>
                <div class="summary-box">
                    {summary_text if summary_text else "No summary available"}
                </div>

                <div class="metrics">
                    <div class="metric-card">
                        <div class="metric-value">{metrics.get('mse', 'N/A'):.4f}</div>
                        <div class="metric-label">MSE</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{metrics.get('mae', 'N/A'):.4f}</div>
                        <div class="metric-label">MAE</div>
                    </div>
                </div>
            </section>
    """

    # Add visualization sections if available
    if viz_paths and include_images:
        html_content += """
            <section>
                <h2>Visualizations</h2>
        """

        if "grid_plot" in viz_paths and os.path.exists(viz_paths["grid_plot"]):
            encoded_img = encode_image_to_base64(viz_paths["grid_plot"])
            html_content += f"""
                <div class="viz-container">
                    <h3>Sensor Predictions Grid</h3>
                    <img src="data:image/png;base64,{encoded_img}" alt="Sensor Grid Plot">
                </div>
            """

        if "error_analysis" in viz_paths and os.path.exists(
            viz_paths["error_analysis"]
        ):
            encoded_img = encode_image_to_base64(viz_paths["error_analysis"])
            html_content += f"""
                <div class="viz-container">
                    <h3>Error Analysis</h3>
                    <img src="data:image/png;base64,{encoded_img}" alt="Error Analysis">
                </div>
            """

        if "validation_plot" in viz_paths and os.path.exists(
            viz_paths["validation_plot"]
        ):
            encoded_img = encode_image_to_base64(viz_paths["validation_plot"])
            html_content += f"""
                <div class="viz-container">
                    <h3>Validation Plot</h3>
                    <img src="data:image/png;base64,{encoded_img}" alt="Validation Plot">
                </div>
            """

        html_content += """
            </section>
        """

    # Close HTML
    html_content += f"""
            <footer>
                <p>GNN Traffic Prediction Model | Model path: {prediction_results.get('model_path', 'Unknown')}</p>
                <p>Output files directory: {results_path}</p>
            </footer>
        </div>
    </body>
    </html>
    """

    return html_content


def save_dashboard(
    results_path: Union[str, Path],
    prediction_results: Dict[str, Any],
    output_file: Optional[Union[str, Path]] = None,
) -> str:
    """
    Generate and save an HTML dashboard for prediction results.

    Parameters:
    -----------
    results_path : str or Path
        Directory containing prediction results
    prediction_results : dict
        Dictionary with prediction information
    output_file : str or Path, optional
        File path to save the dashboard (defaults to dashboard.html in results_path)

    Returns:
    --------
    str
        Path to the saved dashboard
    """
    results_path = Path(results_path)

    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = results_path / f"dashboard_{timestamp}.html"
    else:
        output_file = Path(output_file)

    # Generate dashboard HTML
    html_content = generate_dashboard(results_path, prediction_results)

    # Save to file
    with open(output_file, "w") as f:
        f.write(html_content)

    return str(output_file)



================================================
File: src/visualization/prediction_plots.py
================================================
# gnn_package/src/visualization/prediction_plots.py

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from datetime import datetime
from matplotlib.dates import DateFormatter
from typing import Optional, Dict, Any, List, Tuple, Union
from pathlib import Path
import os
from gnn_package.src.utils.sensor_utils import get_sensor_name_id_map


def plot_predictions_with_validation(
    predictions_dict: Dict[str, Any],
    data: Dict[str, Any],
    node_ids: List[str],
    config: Any,
    max_plots: int = 6,
) -> plt.Figure:
    """
    Plot predictions alongside actual data for validation.

    Parameters:
    -----------
    predictions_dict : dict
        Dictionary returned by predict_with_model
    data : dict
        Dict containing time series data
    node_ids : list
        List of node IDs
    config : ExperimentConfig
        Configuration object
    max_plots : int
        Maximum number of sensors to plot

    Returns:
    --------
    matplotlib.figure.Figure
        The generated figure
    """
    # Get validation window from config
    validation_window = config.data.general.horizon

    # Get name-to-id mapping
    name_id_map = get_sensor_name_id_map(config=config)
    name_id_map = {v: k for k, v in name_id_map.items()}

    # Get predictions array and node indices
    pred_array = predictions_dict["predictions"]
    node_indices = predictions_dict["node_indices"]
    valid_nodes = [node_ids[idx] for idx in node_indices]

    # Create a figure
    n_nodes = min(len(valid_nodes), max_plots)  # Limit to max_plots nodes
    fig, axes = plt.subplots(n_nodes, 1, figsize=(12, 3 * n_nodes))
    if n_nodes == 1:
        axes = [axes]

    # Get time series dictionary from data
    time_series_dict = data["time_series"]["validation"]

    for i, node_id in enumerate(valid_nodes[:n_nodes]):
        ax = axes[i]

        # Get full historical data
        if node_id not in time_series_dict:
            print(f"No historical data found for node {node_id}")
            continue

        historical = time_series_dict[node_id]

        # Split historical data into 'input' and 'validation' parts
        input_data = historical[:-validation_window]
        validation_data = historical[-validation_window:]

        # Get prediction for this node
        node_position = np.where(node_indices == node_ids.index(node_id))[0]
        if len(node_position) == 0:
            print(f"Cannot find node {node_id} in prediction data")
            continue

        node_idx = node_position[0]
        pred = pred_array[0, node_idx, :, 0]  # [batch=0, node, time, feature=0]

        # Get the last timestamp from input data
        last_input_time = input_data.index[-1]

        # Create time indices for prediction that align with validation data
        pred_times = validation_data.index

        # Plot
        ax.plot(
            input_data.index,
            input_data.values,
            label="Input Data",
            color="blue",
            linewidth=1.5,
        )
        ax.plot(
            validation_data.index,
            validation_data.values,
            label="Actual Values",
            color="green",
            linewidth=2,
        )
        ax.plot(pred_times, pred, "r--", label="Predictions", linewidth=2)

        # Highlight the last input point for visual clarity
        ax.scatter(
            [last_input_time],
            [input_data.values[-1]],
            color="darkblue",
            s=50,
            zorder=5,
            label="Last Input Point",
        )

        # Add sensor name to title if available
        sensor_name = name_id_map.get(node_id, node_id)
        ax.set_title(f"Model Validation: {sensor_name} (ID: {node_id})")
        ax.set_ylabel("Traffic Count")

        # Add a grid for better readability
        ax.grid(True, linestyle="--", alpha=0.7)

        # Add a legend
        ax.legend(loc="best", framealpha=0.9)

        # Format x-axis as time
        ax.tick_params(axis="x", rotation=45)

        # Add a vertical line to separate input data and validation period
        ax.axvline(x=last_input_time, color="gray", linestyle="--", alpha=0.8)

        # Calculate and display metrics if validation data exists
        if len(validation_data) > 0:
            mse = ((pred - validation_data.values) ** 2).mean()
            mae = abs(pred - validation_data.values).mean()
            ax.text(
                0.02,
                0.95,
                f"MSE: {mse:.4f}\nMAE: {mae:.4f}",
                transform=ax.transAxes,
                bbox=dict(facecolor="white", alpha=0.8),
            )

    plt.tight_layout()
    return fig


def plot_sensors_grid(
    predictions_df: pd.DataFrame,
    plots_per_row: int = 5,
    figsize: Tuple[int, int] = (20, 25),
) -> plt.Figure:
    """
    Create a grid of plots showing prediction vs actual values for all sensors.

    Parameters:
    -----------
    predictions_df : pandas DataFrame
        DataFrame containing the prediction results with columns:
        'node_id', 'sensor_name', 'timestamp', 'prediction', 'actual', 'horizon'
    plots_per_row : int
        Number of plots to show in each row
    figsize : tuple
        Size of the overall figure (width, height)

    Returns:
    --------
    matplotlib.figure.Figure
        The figure containing the grid of plots
    """
    # Get unique sensors
    unique_sensors = predictions_df["node_id"].unique()
    num_sensors = len(unique_sensors)

    # Calculate grid dimensions
    num_rows = int(np.ceil(num_sensors / plots_per_row))

    # Create figure and axes
    fig, axes = plt.subplots(num_rows, plots_per_row, figsize=figsize)
    axes = axes.flatten()  # Flatten to make indexing easier

    # Set overall title
    fig.suptitle(f"Predictions vs Actual Values for {num_sensors} Sensors", fontsize=16)

    # Format for dates
    date_formatter = DateFormatter("%H:%M")

    # First pass: determine global min and max values for consistent y-axis scaling
    global_min = float("inf")
    global_max = float("-inf")

    for sensor_id in unique_sensors:
        # Get data for this sensor
        sensor_data = predictions_df[predictions_df["node_id"] == sensor_id]

        # Check if we have data
        if len(sensor_data) > 0:
            # Get min and max values for both predictions and actuals
            predictions_min = sensor_data["prediction"].min()
            predictions_max = sensor_data["prediction"].max()
            actuals_min = sensor_data["actual"].min()
            actuals_max = sensor_data["actual"].max()

            # Update global min and max
            global_min = min(global_min, predictions_min)
            global_max = max(global_max, predictions_max)

    # Add a small buffer to the limits (5% padding)
    y_range = global_max - global_min
    global_min = global_min - 0.05 * y_range if y_range > 0 else global_min - 1
    global_max = global_max + 0.05 * y_range if y_range > 0 else global_max + 1

    # Loop through each sensor and create a plot
    for i, sensor_id in enumerate(unique_sensors):
        if i >= len(axes):  # Safety check
            break

        # Get data for this sensor
        sensor_data = predictions_df[predictions_df["node_id"] == sensor_id]

        # Check if we have data
        if len(sensor_data) > 0:
            # Get sensor name
            sensor_name = sensor_data["sensor_name"].iloc[0]

            # Sort by timestamp to ensure correct plot order
            sensor_data = sensor_data.sort_values("timestamp")

            # Get x and y values
            timestamps = sensor_data["timestamp"]
            predictions = sensor_data["prediction"]
            actuals = sensor_data["actual"]

            # Plot
            ax = axes[i]
            ax.plot(timestamps, predictions, "r-", label="Prediction", linewidth=2)
            ax.plot(timestamps, actuals, "b-", label="Actual", linewidth=2)

            # Format plot
            ax.set_title(f"{sensor_name.split('Ncl')[-1]}", fontsize=10)
            ax.tick_params(axis="x", rotation=45, labelsize=8)
            ax.tick_params(axis="y", labelsize=8)
            ax.xaxis.set_major_formatter(date_formatter)

            # Apply consistent y-axis limits to all plots
            ax.set_ylim(global_min, global_max)

            # Only show legend for the first plot
            if i == 0:
                ax.legend(loc="upper right", fontsize=8)

            # Add grid for better readability
            ax.grid(True, linestyle="--", alpha=0.6)

            # Calculate and show error metrics
            mse = ((predictions - actuals) ** 2).mean()
            mae = (predictions - actuals).abs().mean()
            ax.text(
                0.02,
                0.95,
                f"MAE: {mae:.1f}",
                transform=ax.transAxes,
                fontsize=7,
                bbox=dict(facecolor="white", alpha=0.7),
            )
        else:
            # No data case
            ax.text(
                0.5,
                0.5,
                f"No data for {sensor_id}",
                ha="center",
                va="center",
                transform=ax.transAxes,
            )
            ax.axis("off")

    # Turn off unused subplots
    for j in range(i + 1, len(axes)):
        axes[j].axis("off")

    # Adjust spacing
    plt.tight_layout(rect=[0, 0, 1, 0.97])  # Make room for suptitle

    return fig


def plot_error_distribution(
    predictions_df: pd.DataFrame, figsize: Tuple[int, int] = (15, 10)
) -> plt.Figure:
    """
    Create plots showing the error distribution and patterns.

    Parameters:
    -----------
    predictions_df : pandas DataFrame
        DataFrame containing prediction results
    figsize : tuple
        Size of the figure

    Returns:
    --------
    matplotlib.figure.Figure
        The figure with error distribution plots
    """
    fig, axes = plt.subplots(2, 2, figsize=figsize)

    # Plot 1: Error histogram
    axes[0, 0].hist(
        predictions_df["error"], bins=30, color="skyblue", edgecolor="black"
    )
    axes[0, 0].set_title("Prediction Error Distribution")
    axes[0, 0].set_xlabel("Error")
    axes[0, 0].set_ylabel("Frequency")
    axes[0, 0].grid(True, linestyle="--", alpha=0.7)

    # Plot 2: Error by prediction horizon
    horizon_errors = predictions_df.groupby("horizon")["abs_error"].mean()
    axes[0, 1].bar(horizon_errors.index, horizon_errors.values, color="lightgreen")
    axes[0, 1].set_title("Mean Absolute Error by Prediction Horizon")
    axes[0, 1].set_xlabel("Horizon (steps ahead)")
    axes[0, 1].set_ylabel("Mean Absolute Error")
    axes[0, 1].grid(True, linestyle="--", alpha=0.7)

    # Plot 3: Scatter plot of predicted vs actual
    axes[1, 0].scatter(
        predictions_df["actual"],
        predictions_df["prediction"],
        alpha=0.5,
        s=10,
        color="blue",
    )

    # Add perfect prediction line
    max_val = max(predictions_df["actual"].max(), predictions_df["prediction"].max())
    min_val = min(predictions_df["actual"].min(), predictions_df["prediction"].min())
    axes[1, 0].plot([min_val, max_val], [min_val, max_val], "r--")

    axes[1, 0].set_title("Predicted vs Actual Values")
    axes[1, 0].set_xlabel("Actual Values")
    axes[1, 0].set_ylabel("Predicted Values")
    axes[1, 0].grid(True, linestyle="--", alpha=0.7)

    # Plot 4: Top 10 sensors by error
    sensor_errors = (
        predictions_df.groupby(["sensor_name"])["abs_error"]
        .mean()
        .sort_values(ascending=False)
        .head(10)
    )
    sensor_errors.plot(kind="barh", ax=axes[1, 1], color="salmon")
    axes[1, 1].set_title("Top 10 Sensors by Mean Absolute Error")
    axes[1, 1].set_xlabel("Mean Absolute Error")
    axes[1, 1].set_ylabel("Sensor Name")

    plt.tight_layout()
    return fig


def save_visualization_pack(
    predictions_df: pd.DataFrame,
    results_dict: Dict[str, Any],
    output_dir: Union[str, Path],
    timestamp: Optional[str] = None,
) -> Dict[str, str]:
    """
    Generate and save a comprehensive set of visualizations.

    Parameters:
    -----------
    predictions_df : pandas DataFrame
        DataFrame with prediction results
    results_dict : dict
        Dictionary with prediction results and data
    output_dir : str or Path
        Directory to save visualizations
    timestamp : str, optional
        Timestamp string for filenames

    Returns:
    --------
    dict
        Dictionary with paths to all saved visualizations
    """
    # Create output directory if it doesn't exist
    output_dir = Path(output_dir)
    os.makedirs(output_dir, exist_ok=True)

    # Use current timestamp if not provided
    if timestamp is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Dictionary to store all visualization paths
    viz_paths = {}

    # 1. Create and save sensors grid plot
    try:
        grid_fig = plot_sensors_grid(predictions_df)
        grid_path = output_dir / f"sensors_grid_{timestamp}.png"
        grid_fig.savefig(grid_path, dpi=150, bbox_inches="tight")
        plt.close(grid_fig)
        viz_paths["grid_plot"] = str(grid_path)
    except Exception as e:
        print(f"Error creating sensors grid plot: {e}")

    # 2. Create and save error distribution plots
    try:
        error_fig = plot_error_distribution(predictions_df)
        error_path = output_dir / f"error_analysis_{timestamp}.png"
        error_fig.savefig(error_path, dpi=150, bbox_inches="tight")
        plt.close(error_fig)
        viz_paths["error_analysis"] = str(error_path)
    except Exception as e:
        print(f"Error creating error distribution plot: {e}")

    # 3. Create and save detailed validation plot
    try:
        if all(k in results_dict for k in ["predictions", "data", "node_ids"]):
            validation_fig = plot_predictions_with_validation(
                results_dict["predictions"],
                results_dict["data"],
                results_dict["node_ids"],
                results_dict.get("config"),
            )
            validation_path = output_dir / f"validation_plot_{timestamp}.png"
            validation_fig.savefig(validation_path, dpi=150, bbox_inches="tight")
            plt.close(validation_fig)
            viz_paths["validation_plot"] = str(validation_path)
    except Exception as e:
        print(f"Error creating validation plot: {e}")

    return viz_paths



