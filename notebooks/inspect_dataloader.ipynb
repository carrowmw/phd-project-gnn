{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from gnn_package import training\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get processed data\n",
    "if os.path.exists(\"data_loaders_test_1wk.pkl\"):\n",
    "    with open(\"data_loaders_test_1wk.pkl\", \"rb\") as f:\n",
    "        data_loaders = pickle.load(f)\n",
    "else:\n",
    "    print(\"Data loaders not found. Please run the data processing script first.\")\n",
    "\n",
    "\n",
    "if os.path.exists(\"test_data_1wk.pkl\"):\n",
    "    with open(\"test_data_1wk.pkl\", \"rb\") as f:\n",
    "        results_containing_data = pickle.load(f)\n",
    "else:\n",
    "    print(\"Test data not found. Please run the data processing script first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataloader(dataloader):\n",
    "    \"\"\"Inspect the structure of a PyTorch DataLoader and its batches.\"\"\"\n",
    "    print(f\"DataLoader type: {type(dataloader)}\")\n",
    "    print(f\"DataLoader object: {dataloader}\")\n",
    "\n",
    "    # Get information about batch size and other dataloader properties\n",
    "    print(f\"\\nDataLoader Properties:\")\n",
    "    print(f\"Batch size: {dataloader.batch_size}\")\n",
    "    print(f\"Number of workers: {dataloader.num_workers}\")\n",
    "    print(\n",
    "        f\"Collate function: {dataloader.collate_fn.__name__ if hasattr(dataloader.collate_fn, '__name__') else dataloader.collate_fn}\"\n",
    "    )\n",
    "\n",
    "    # Examine the dataset\n",
    "    print(f\"\\nDataset:\")\n",
    "    print(f\"Dataset type: {type(dataloader.dataset)}\")\n",
    "    print(f\"Dataset length: {len(dataloader.dataset)}\")\n",
    "\n",
    "    # Try to inspect one sample from the dataset\n",
    "    try:\n",
    "        sample = dataloader.dataset[10000]\n",
    "        print(f\"\\nSample from dataset:\")\n",
    "        print(f\"Sample type: {type(sample)}\")\n",
    "        print(\n",
    "            f\"Sample keys: {sample.keys() if isinstance(sample, dict) else 'Not a dictionary'}\"\n",
    "        )\n",
    "        for key, value in sample.items() if isinstance(sample, dict) else []:\n",
    "            print(\n",
    "                f\"  {key}: {type(value)}, Shape: {value.shape if hasattr(value, 'shape') else 'No shape attribute'}\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Could not inspect dataset sample: {e}\")\n",
    "\n",
    "    # Try to examine one batch\n",
    "    print(f\"\\nBatch inspection:\")\n",
    "    try:\n",
    "        batch = next(iter(dataloader))\n",
    "        print(f\"Batch type: {type(batch)}\")\n",
    "        print(\n",
    "            f\"Batch keys: {batch.keys() if isinstance(batch, dict) else 'Not a dictionary'}\"\n",
    "        )\n",
    "\n",
    "        for key, value in batch.items() if isinstance(batch, dict) else []:\n",
    "            if hasattr(value, \"shape\"):\n",
    "                print(f\"  {key}: Shape {value.shape}, Type {value.dtype}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {type(value)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not inspect batch: {e}\")\n",
    "\n",
    "    # Calculate total number of batches\n",
    "    try:\n",
    "        num_batches = len(dataloader)\n",
    "        print(f\"\\nTotal number of batches: {num_batches}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not determine number of batches: {e}\")\n",
    "\n",
    "    return batch if \"batch\" in locals() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the train dataloader\n",
    "batch = inspect_dataloader(data_loaders[\"train_loader\"])\n",
    "\n",
    "# If a batch was successfully retrieved, perform a deeper inspection\n",
    "if batch is not None:\n",
    "    print(\"\\nDetailed batch analysis:\")\n",
    "\n",
    "    # For the x tensor (input features)\n",
    "    if \"x\" in batch:\n",
    "        x = batch[\"x\"]\n",
    "        print(f\"\\nx tensor analysis:\")\n",
    "        print(f\"  Shape: {x.shape}\")\n",
    "        print(\n",
    "            f\"  Interpretation: [batch_size={x.shape[0]}, num_nodes={x.shape[1]}, seq_len={x.shape[2]}, features={x.shape[3]}]\"\n",
    "        )\n",
    "        print(f\"  Min value: {x.min().item():.4f}, Max value: {x.max().item():.4f}\")\n",
    "        print(f\"  Contains -1 (missing data): {(x == -1).any().item()}\")\n",
    "        print(\n",
    "            f\"  Percentage of missing data: {((x == -1).sum() / x.numel()) * 100:.2f}%\"\n",
    "        )\n",
    "\n",
    "    # For the adjacency matrix\n",
    "    if \"adj\" in batch:\n",
    "        adj = batch[\"adj\"]\n",
    "        print(f\"\\nadj tensor analysis:\")\n",
    "        print(f\"  Shape: {adj.shape}\")\n",
    "        print(f\"  Interpretation: [num_nodes={adj.shape[0]}, num_nodes={adj.shape[1]}]\")\n",
    "        print(f\"  Min value: {adj.min().item():.4f}, Max value: {adj.max().item():.4f}\")\n",
    "        print(f\"  Is symmetric: {torch.allclose(adj, adj.transpose(0, 1))}\")\n",
    "\n",
    "    # For the masks\n",
    "    if \"x_mask\" in batch:\n",
    "        x_mask = batch[\"x_mask\"]\n",
    "        print(f\"\\nx_mask tensor analysis:\")\n",
    "        print(f\"  Shape: {x_mask.shape}\")\n",
    "        print(f\"  Values are binary: {torch.all((x_mask == 0) | (x_mask == 1)).item()}\")\n",
    "        print(\n",
    "            f\"  Percentage of valid (non-masked) values: {x_mask.mean().item()*100:.2f}%\"\n",
    "        )\n",
    "\n",
    "    # For the target values\n",
    "    if \"y\" in batch:\n",
    "        y = batch[\"y\"]\n",
    "        print(f\"\\ny tensor analysis (targets):\")\n",
    "        print(f\"  Shape: {y.shape}\")\n",
    "        print(\n",
    "            f\"  Interpretation: [batch_size={y.shape[0]}, num_nodes={y.shape[1]}, horizon={y.shape[2]}, features={y.shape[3]}]\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataloader_missing_values(dataloader):\n",
    "    \"\"\"\n",
    "    Analyze the presence of -1 values (missing data) in a dataloader and check window completeness.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataloader : torch.utils.data.DataLoader\n",
    "        The dataloader to analyze\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    print(\"Analyzing missing values in dataloader...\")\n",
    "\n",
    "    # Initialize counters and storage\n",
    "    total_batches = 0\n",
    "    total_windows = 0\n",
    "    windows_with_missing = 0\n",
    "    missing_values_count = 0\n",
    "    total_values_count = 0\n",
    "\n",
    "    # Store window counts per sensor\n",
    "    node_window_counts = {}\n",
    "    node_missing_counts = {}\n",
    "\n",
    "    # Process some batches\n",
    "    max_batches = 10  # Limit to 10 batches for efficiency\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        if batch_idx >= max_batches:\n",
    "            break\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "        # Extract data tensors\n",
    "        x = batch[\"x\"]  # [batch_size, num_nodes, seq_len, features]\n",
    "        x_mask = batch[\"x_mask\"]  # [batch_size, num_nodes, seq_len, features]\n",
    "        node_indices = batch[\"node_indices\"]  # Sensor indices in this batch\n",
    "\n",
    "        batch_size, num_nodes, seq_len, _ = x.shape\n",
    "\n",
    "        # Convert node indices to list if it's a tensor\n",
    "        if torch.is_tensor(node_indices):\n",
    "            node_indices = node_indices.cpu().numpy().tolist()\n",
    "\n",
    "        # Count windows and missing values\n",
    "        total_windows += batch_size * num_nodes\n",
    "\n",
    "        # Check for -1 values (missing data indicators)\n",
    "        missing_mask = x == -1\n",
    "        batch_missing = missing_mask.sum().item()\n",
    "        missing_values_count += batch_missing\n",
    "        total_values_count += x.numel()\n",
    "\n",
    "        # Count windows with any missing values\n",
    "        for b in range(batch_size):\n",
    "            for n in range(num_nodes):\n",
    "                # Get node ID (sensor ID)\n",
    "                node_id = node_indices[n] if n < len(node_indices) else f\"unknown_{n}\"\n",
    "\n",
    "                # Increment window count for this node\n",
    "                if node_id not in node_window_counts:\n",
    "                    node_window_counts[node_id] = 0\n",
    "                    node_missing_counts[node_id] = 0\n",
    "\n",
    "                node_window_counts[node_id] += 1\n",
    "\n",
    "                # Check if this window has any missing values\n",
    "                window_missing = missing_mask[b, n].any().item()\n",
    "                if window_missing:\n",
    "                    windows_with_missing += 1\n",
    "                    node_missing_counts[node_id] += 1\n",
    "\n",
    "        # Check if mask matches -1 values\n",
    "        mask_matches_missing = ((x_mask == 0) == (x == -1)).all().item()\n",
    "        if not mask_matches_missing:\n",
    "            print(\n",
    "                f\"WARNING: Batch {batch_idx} has mismatches between mask and -1 values!\"\n",
    "            )\n",
    "\n",
    "    # Create a DataFrame for window counts by sensor\n",
    "    window_counts_df = pd.DataFrame(\n",
    "        {\n",
    "            \"sensor_id\": list(node_window_counts.keys()),\n",
    "            \"total_windows\": [node_window_counts[nid] for nid in node_window_counts],\n",
    "            \"windows_with_missing\": [\n",
    "                node_missing_counts[nid] for nid in node_window_counts\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Calculate percentage of windows with missing values\n",
    "    window_counts_df[\"pct_windows_with_missing\"] = (\n",
    "        window_counts_df[\"windows_with_missing\"]\n",
    "        / window_counts_df[\"total_windows\"]\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    # Sort by number of windows\n",
    "    window_counts_df = window_counts_df.sort_values(\"total_windows\", ascending=False)\n",
    "\n",
    "    # Check if all sensors have the same number of windows\n",
    "    equal_window_counts = window_counts_df[\"total_windows\"].nunique() == 1\n",
    "\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        \"total_batches\": total_batches,\n",
    "        \"total_windows\": total_windows,\n",
    "        \"windows_with_missing\": windows_with_missing,\n",
    "        \"pct_windows_with_missing\": (\n",
    "            windows_with_missing / total_windows * 100 if total_windows > 0 else 0\n",
    "        ),\n",
    "        \"missing_values_count\": missing_values_count,\n",
    "        \"total_values_count\": total_values_count,\n",
    "        \"pct_missing_values\": (\n",
    "            missing_values_count / total_values_count * 100\n",
    "            if total_values_count > 0\n",
    "            else 0\n",
    "        ),\n",
    "        \"window_counts_by_sensor\": window_counts_df,\n",
    "        \"equal_window_counts\": equal_window_counts,\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n=== DataLoader Missing Value Analysis ===\")\n",
    "    print(f\"Total batches analyzed: {total_batches}\")\n",
    "    print(f\"Total windows: {total_windows}\")\n",
    "    print(\n",
    "        f\"Windows with missing values: {windows_with_missing} ({results['pct_windows_with_missing']:.2f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Total missing values: {missing_values_count} out of {total_values_count} ({results['pct_missing_values']:.2f}%)\"\n",
    "    )\n",
    "    print(f\"All sensors have equal window counts: {equal_window_counts}\")\n",
    "    print(f\"Number of sensors in analysis: {len(node_window_counts)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def visualize_dataloader_completeness(dataloader, max_sensors=10):\n",
    "    \"\"\"\n",
    "    Visualize the completeness of data in the dataloader by sensor.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataloader : torch.utils.data.DataLoader\n",
    "        The dataloader to analyze\n",
    "    max_sensors : int\n",
    "        Maximum number of sensors to display\n",
    "    \"\"\"\n",
    "    # Get the analysis results\n",
    "    results = analyze_dataloader_missing_values(dataloader)\n",
    "\n",
    "    # Extract the window counts DataFrame\n",
    "    window_df = results[\"window_counts_by_sensor\"]\n",
    "\n",
    "    # Limit to top sensors\n",
    "    if len(window_df) > max_sensors:\n",
    "        window_df = window_df.head(max_sensors)\n",
    "\n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot 1: Window counts by sensor\n",
    "    bar_colors = plt.cm.viridis(np.linspace(0, 1, len(window_df)))\n",
    "    ax1.bar(window_df[\"sensor_id\"], window_df[\"total_windows\"], color=bar_colors)\n",
    "    ax1.set_title(\"Number of Windows by Sensor\")\n",
    "    ax1.set_xlabel(\"Sensor ID\")\n",
    "    ax1.set_ylabel(\"Window Count\")\n",
    "    ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Add text on bars if not too many sensors\n",
    "    if len(window_df) <= max_sensors:\n",
    "        for i, v in enumerate(window_df[\"total_windows\"]):\n",
    "            ax1.text(i, v + 0.1, str(v), ha=\"center\")\n",
    "\n",
    "    # Plot 2: Percentage of windows with missing values\n",
    "    ax2.bar(\n",
    "        window_df[\"sensor_id\"], window_df[\"pct_windows_with_missing\"], color=bar_colors\n",
    "    )\n",
    "    ax2.set_title(\"Percentage of Windows with Missing Values\")\n",
    "    ax2.set_xlabel(\"Sensor ID\")\n",
    "    ax2.set_ylabel(\"Percentage\")\n",
    "    ax2.tick_params(axis=\"x\", rotation=45)\n",
    "    ax2.set_ylim(0, 100)\n",
    "\n",
    "    # Add text on bars\n",
    "    if len(window_df) <= max_sensors:\n",
    "        for i, v in enumerate(window_df[\"pct_windows_with_missing\"]):\n",
    "            ax2.text(i, v + 1, f\"{v:.1f}%\", ha=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Create a second figure to show a heatmap of missing values\n",
    "    # Extract a sample batch to examine\n",
    "    batch = next(iter(dataloader))\n",
    "    x = batch[\"x\"]  # [batch_size, num_nodes, seq_len, features]\n",
    "    node_indices = batch[\"node_indices\"]\n",
    "\n",
    "    # Convert node indices to list if it's a tensor\n",
    "    if torch.is_tensor(node_indices):\n",
    "        node_indices = node_indices.cpu().numpy().tolist()\n",
    "\n",
    "    # Create a mask of missing values (1 = missing, 0 = present)\n",
    "    missing_mask = (x == -1).float().cpu().numpy()\n",
    "\n",
    "    # Plot up to 2 batches\n",
    "    batch_size, num_nodes, seq_len, _ = x.shape\n",
    "    plot_batches = min(2, batch_size)\n",
    "\n",
    "    fig, axs = plt.subplots(plot_batches, 1, figsize=(12, 4 * plot_batches))\n",
    "    if plot_batches == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for b in range(plot_batches):\n",
    "        # Create mask matrix for this batch\n",
    "        mask_matrix = missing_mask[b, :, :, 0]\n",
    "\n",
    "        # Plot heatmap\n",
    "        im = axs[b].imshow(mask_matrix, aspect=\"auto\", cmap=\"Blues_r\")\n",
    "        axs[b].set_title(f\"Missing Values Pattern in Batch {b}\")\n",
    "        axs[b].set_xlabel(\"Time Step\")\n",
    "        axs[b].set_ylabel(\"Node (Sensor) Index\")\n",
    "\n",
    "        # Add sensor labels\n",
    "        node_labels = [f\"{i}:{node_indices[i]}\" for i in range(num_nodes)]\n",
    "        axs[b].set_yticks(range(num_nodes))\n",
    "        axs[b].set_yticklabels(node_labels)\n",
    "\n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=axs[b], label=\"Missing (1) vs Present (0)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataloader_missing_values(data_loaders[\"train_loader\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_collate_function(dataloader, time_series_dict, window_size=12, stride=1):\n",
    "    \"\"\"\n",
    "    Inspect the collate function used by the dataloader, which might be introducing extra -1 values.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataloader : torch.utils.data.DataLoader\n",
    "        The dataloader to inspect\n",
    "    time_series_dict : dict\n",
    "        Original time series data for comparison\n",
    "    window_size : int\n",
    "        Size of sliding windows\n",
    "    stride : int\n",
    "        Step size for sliding windows\n",
    "    \"\"\"\n",
    "    from gnn_package.src.preprocessing import TimeSeriesPreprocessor\n",
    "    from gnn_package.src.dataloaders import collate_fn\n",
    "\n",
    "    print(\"Inspecting dataloader collate function...\")\n",
    "\n",
    "    # First create windows using the processor\n",
    "    processor = TimeSeriesPreprocessor(\n",
    "        window_size=window_size,\n",
    "        stride=stride,\n",
    "        gap_threshold=pd.Timedelta(minutes=15),\n",
    "        missing_value=-1.0,\n",
    "    )\n",
    "\n",
    "    X_by_sensor, masks_by_sensor, metadata_by_sensor = processor.create_windows(\n",
    "        time_series_dict, standardize=True\n",
    "    )\n",
    "\n",
    "    # Get the collate function from the dataloader\n",
    "    actual_collate_fn = dataloader.collate_fn\n",
    "\n",
    "    # Get a few samples from the dataset to test the collate function directly\n",
    "    try:\n",
    "        # Get the dataset\n",
    "        dataset = dataloader.dataset\n",
    "\n",
    "        # Take a small batch of samples\n",
    "        batch_size = min(8, len(dataset))\n",
    "        samples = [dataset[i] for i in range(batch_size)]\n",
    "\n",
    "        # Run the collate function directly\n",
    "        print(\"\\nRunning collate function directly on samples...\")\n",
    "        batch = actual_collate_fn(samples)\n",
    "\n",
    "        # Analyze the batch\n",
    "        print(\"\\nBatch structure after collate_fn:\")\n",
    "        for key, value in batch.items():\n",
    "            if torch.is_tensor(value):\n",
    "                print(f\"  {key}: Shape {value.shape}, Type {value.dtype}\")\n",
    "\n",
    "                # Check for -1 values\n",
    "                if key in [\"x\", \"y\"]:\n",
    "                    missing_count = (value == -1).sum().item()\n",
    "                    total_count = value.numel()\n",
    "                    print(f\"    Contains -1 values: {missing_count > 0}\")\n",
    "                    print(\n",
    "                        f\"    Missing value percentage: {missing_count/total_count*100:.2f}%\"\n",
    "                    )\n",
    "            else:\n",
    "                print(f\"  {key}: {type(value)}\")\n",
    "\n",
    "        # Compare with original sample values\n",
    "        print(\"\\nComparing -1 values between original samples and batch:\")\n",
    "\n",
    "        # Count missing values in original samples\n",
    "        original_missing = 0\n",
    "        original_total = 0\n",
    "\n",
    "        for sample in samples:\n",
    "            if \"x\" in sample and torch.is_tensor(sample[\"x\"]):\n",
    "                original_missing += (sample[\"x\"] == -1).sum().item()\n",
    "                original_total += sample[\"x\"].numel()\n",
    "\n",
    "        # Count missing values in batch\n",
    "        batch_missing = (batch[\"x\"] == -1).sum().item()\n",
    "        batch_total = batch[\"x\"].numel()\n",
    "\n",
    "        # Print comparison\n",
    "        print(\n",
    "            f\"  Original samples: {original_missing} missing out of {original_total} ({original_missing/original_total*100:.2f}%)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  After collate_fn: {batch_missing} missing out of {batch_total} ({batch_missing/batch_total*100:.2f}%)\"\n",
    "        )\n",
    "\n",
    "        if batch_missing > original_missing:\n",
    "            print(\n",
    "                f\"  WARNING: Collate function is adding {batch_missing - original_missing} extra missing values!\"\n",
    "            )\n",
    "            print(\n",
    "                \"  This suggests the collate_fn is introducing -1 values when forming batches.\"\n",
    "            )\n",
    "\n",
    "            # Let's print the collate_fn code for inspection\n",
    "            import inspect\n",
    "\n",
    "            print(\"\\nCollate function source code:\")\n",
    "            print(inspect.getsource(actual_collate_fn))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in collate function inspection: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"X_by_sensor\": X_by_sensor,\n",
    "        \"masks_by_sensor\": masks_by_sensor,\n",
    "        \"batch\": batch if \"batch\" in locals() else None,\n",
    "    }\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "collate_results = inspect_collate_function(\n",
    "    data_loaders[\"train_loader\"], results_containing_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataset_implementation(data_loaders):\n",
    "    \"\"\"Inspect the dataset implementation to understand how it's creating batches\"\"\"\n",
    "    from gnn_package.src.dataloaders import SpatioTemporalDataset\n",
    "\n",
    "    # First check the dataset type\n",
    "    dataset = data_loaders[\"train_loader\"].dataset\n",
    "    print(f\"Dataset type: {type(dataset)}\")\n",
    "\n",
    "    # Check if it's our SpatioTemporalDataset\n",
    "    if isinstance(dataset, SpatioTemporalDataset):\n",
    "        print(\"\\nFound SpatioTemporalDataset instance:\")\n",
    "        print(f\"Number of node IDs: {len(dataset.node_ids)}\")\n",
    "        print(f\"Number of sample indices: {len(dataset.sample_indices)}\")\n",
    "\n",
    "        # Check a few sample indices to understand structure\n",
    "        print(\"\\nSample indices structure (first 5):\")\n",
    "        for i, (node_id, window_idx) in enumerate(dataset.sample_indices[:5]):\n",
    "            print(f\"  {i}: node_id={node_id}, window_idx={window_idx}\")\n",
    "\n",
    "        # Check __getitem__ implementation\n",
    "        import inspect\n",
    "\n",
    "        print(\"\\nExamining __getitem__ method:\")\n",
    "        print(inspect.getsource(dataset.__getitem__))\n",
    "\n",
    "        # Test __getitem__ directly\n",
    "        print(\"\\nTesting __getitem__ directly:\")\n",
    "        sample = dataset[0]\n",
    "        print(\"Sample keys:\", sample.keys())\n",
    "\n",
    "        # Check dimensions and missing value ratio\n",
    "        for key, value in sample.items():\n",
    "            if torch.is_tensor(value):\n",
    "                if value.numel() > 0:\n",
    "                    missing_ratio = (value == -1).float().mean().item() * 100\n",
    "                    print(f\"  {key}: shape={value.shape}, missing={missing_ratio:.2f}%\")\n",
    "                else:\n",
    "                    print(f\"  {key}: shape={value.shape}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "        # Now let's get a batch from the dataloader and check each node's representation\n",
    "        print(\"\\nAnalyzing a batch from the dataloader:\")\n",
    "        batch = next(iter(data_loaders[\"train_loader\"]))\n",
    "\n",
    "        # Check node representation in the batch\n",
    "        x = batch[\"x\"]  # [batch_size, num_nodes, seq_len, features]\n",
    "        batch_size, num_nodes, seq_len, _ = x.shape\n",
    "\n",
    "        print(f\"Batch shape: {x.shape}\")\n",
    "\n",
    "        # Check missing value patterns by node\n",
    "        print(\"\\nMissing value patterns by node position in batch:\")\n",
    "        for n in range(min(num_nodes, 10)):  # Limit to first 10 nodes\n",
    "            node_missing = (x[:, n, :, :] == -1).float().mean().item() * 100\n",
    "            print(f\"  Node position {n}: {node_missing:.2f}% missing values\")\n",
    "\n",
    "        # Check if all values are missing for some nodes - this would confirm our theory\n",
    "        all_missing_nodes = 0\n",
    "        for n in range(num_nodes):\n",
    "            if (x[:, n, :, :] == -1).all().item():\n",
    "                all_missing_nodes += 1\n",
    "\n",
    "        print(\n",
    "            f\"\\nNodes with all values missing: {all_missing_nodes} out of {num_nodes} ({all_missing_nodes/num_nodes*100:.2f}%)\"\n",
    "        )\n",
    "\n",
    "        if all_missing_nodes > 0:\n",
    "            print(\n",
    "                \"This confirms our theory - the dataloader is creating tensors with all nodes,\"\n",
    "            )\n",
    "            print(\n",
    "                \"but only filling in values for the nodes present in each batch, leaving others as -1\"\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        print(f\"Dataset is not SpatioTemporalDataset but {type(dataset)}\")\n",
    "        # Try to extract some info about the dataset\n",
    "        print(\"\\nAttempting to inspect dataset properties:\")\n",
    "        for attr in dir(dataset):\n",
    "            if not attr.startswith(\"_\") and not callable(getattr(dataset, attr)):\n",
    "                try:\n",
    "                    value = getattr(dataset, attr)\n",
    "                    print(f\"  {attr}: {type(value)}\")\n",
    "                except:\n",
    "                    print(f\"  {attr}: <error getting value>\")\n",
    "\n",
    "    # Try to check the collate_fn source code regardless\n",
    "    print(\"\\nCollate function source code:\")\n",
    "    import inspect\n",
    "\n",
    "    print(inspect.getsource(data_loaders[\"train_loader\"].collate_fn))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "dataset_results = inspect_dataset_implementation(data_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
