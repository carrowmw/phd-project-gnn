{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta, datetime\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from private_uoapi import (LightsailWrapper, LSAuth, LSConfig, DateRangeParams, convert_to_dataframe)\n",
    "from gnn_package.src.utils.sensor_utils import get_sensor_name_id_map\n",
    "from gnn_package.config.paths import RAW_TIMESERIES_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TIMESERIES_DIR\n",
    "VEH_CLASS = 'person'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_traffic_data(start_date, end_date, output_filename, window_size_days=7):\n",
    "    \"\"\"\n",
    "    Fetch traffic data from the API and save it in the format required by run_experiment.py.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : datetime\n",
    "        Start date for data fetching\n",
    "    end_date : datetime\n",
    "        End date for data fetching\n",
    "    output_filename : str\n",
    "        Name of the output file to save the data\n",
    "    window_size_days : int\n",
    "        Size of the time window for each API request (to avoid timeout issues)\n",
    "    \"\"\"\n",
    "    print(f\"Fetching traffic data from {start_date} to {end_date}\")\n",
    "\n",
    "    # Initialize API client\n",
    "    config = LSConfig()\n",
    "    auth = LSAuth(config)\n",
    "    client = LightsailWrapper(config, auth)\n",
    "\n",
    "    print(f\"Using base URL: {config.base_url}\")\n",
    "    print(f\"Using username: {config.username}\")\n",
    "    print(f\"Using secret key: {'*' * len(config.secret_key)}\")\n",
    "\n",
    "    # Get sensor name to ID mapping\n",
    "    name_id_map = get_sensor_name_id_map()\n",
    "\n",
    "    # Create time windows to fetch data in chunks\n",
    "    total_days = (end_date - start_date).days\n",
    "    num_windows = (total_days + window_size_days - 1) // window_size_days\n",
    "\n",
    "    all_data = {}\n",
    "    unique_veh_classes = set()\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        window_start = start_date + timedelta(days=i * window_size_days)\n",
    "        window_end = min(window_start + timedelta(days=window_size_days), end_date)\n",
    "\n",
    "        print(f\"Fetching window {i+1}/{num_windows}: {window_start} to {window_end}\")\n",
    "\n",
    "        # Create date range parameters\n",
    "        date_range_params = DateRangeParams(\n",
    "            start_date=window_start,\n",
    "            end_date=window_end,\n",
    "            max_date_range=timedelta(days=window_size_days + 1),\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Fetch data for this window\n",
    "            count_data = await client.get_traffic_data(date_range_params)\n",
    "            counts_df = convert_to_dataframe(count_data)\n",
    "\n",
    "            # filter to only include vehicle classes of interest\n",
    "            counts_df = counts_df[counts_df[\"veh_class\"] == VEH_CLASS]\n",
    "            unique_veh_classes.update(counts_df[\"veh_class\"].unique())\n",
    "\n",
    "            # Aggregate data on direction\n",
    "            counts_df = counts_df.groupby([\"dt\", \"veh_class\", \"location\"]).agg({\"value\": \"sum\"}).reset_index()\n",
    "\n",
    "            # Convert to time series dictionary format\n",
    "            for location in set(counts_df[\"location\"]):\n",
    "                location_df = counts_df[counts_df[\"location\"] == location]\n",
    "\n",
    "                if len(location_df) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Get the sensor ID from the mapping\n",
    "                if location in name_id_map:\n",
    "                    sensor_id = name_id_map[location]\n",
    "                else:\n",
    "                    print(f\"Warning: Location {location} not found in mapping, skipping\")\n",
    "                    continue\n",
    "\n",
    "                # Create the time series\n",
    "                if sensor_id not in all_data:\n",
    "                    all_data[sensor_id] = pd.Series(index=pd.DatetimeIndex([]))\n",
    "\n",
    "                # Extract values and timestamps\n",
    "                for _, row in location_df.iterrows():\n",
    "                    dt = row[\"dt\"]\n",
    "                    value = row[\"value\"]\n",
    "\n",
    "                    # Add to the existing series\n",
    "                    if dt not in all_data[sensor_id].index:\n",
    "                        all_data[sensor_id].at[dt] = value\n",
    "\n",
    "            print(f\"Processed {len(location_df)} records for window {i+1}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for window {i+1}: {str(e)}\")\n",
    "\n",
    "    # Process and clean the series\n",
    "    for sensor_id in all_data:\n",
    "        # Sort by timestamp\n",
    "        all_data[sensor_id] = all_data[sensor_id].sort_index()\n",
    "\n",
    "        # Remove duplicates\n",
    "        all_data[sensor_id] = all_data[sensor_id][~all_data[sensor_id].index.duplicated(keep='first')]\n",
    "\n",
    "    # Filter out sensors with very little data\n",
    "    MIN_DATA_POINTS = 24  # At least 24 hours of data\n",
    "    filtered_data = {k: v for k, v in all_data.items() if len(v) >= MIN_DATA_POINTS}\n",
    "\n",
    "    print(f\"Collected data for {len(filtered_data)} sensors out of {len(name_id_map)} total sensors\")\n",
    "\n",
    "    # Save the data\n",
    "    output_path = RAW_TIMESERIES_DIR / output_filename\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        pickle.dump(filtered_data, f)\n",
    "\n",
    "    print(f\"Unique vehicle classes found: {unique_veh_classes}\")\n",
    "    print(f\"Data saved to {output_path}\")\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Fetch data for 1 week\n",
    "async def fetch_data_example_1wk():\n",
    "    end_date = datetime.now() - timedelta(days=10)  # Exclude last 10 days\n",
    "    start_date = end_date - timedelta(days=17)\n",
    "\n",
    "    return await fetch_traffic_data(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        output_filename=f\"test_data_1wk_{VEH_CLASS}.pkl\",\n",
    "        window_size_days=1  # Fetch in 1-day chunks to avoid timeout\n",
    "    )\n",
    "\n",
    "# Example 2: Fetch data for 1 month\n",
    "async def fetch_data_example_1mo():\n",
    "    end_date = datetime.now() - timedelta(days=10)  # Exclude last 10 days\n",
    "    start_date = end_date - timedelta(days=40)\n",
    "\n",
    "    return await fetch_traffic_data(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        output_filename=f\"test_data_1mo_{VEH_CLASS}.pkl\",\n",
    "        window_size_days=7  # Fetch in 7-day chunks\n",
    "    )\n",
    "\n",
    "# Example 3: Fetch data for 1 3months\n",
    "async def fetch_data_example_3mo():\n",
    "    end_date = datetime.now() - timedelta(days=10)  # Exclude last 10 days\n",
    "    start_date = end_date - timedelta(days=90)\n",
    "\n",
    "    return await fetch_traffic_data(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        output_filename=f\"test_data_3mo_{VEH_CLASS}.pkl\",\n",
    "        window_size_days=7  # Fetch in 7-day chunks\n",
    "    )\n",
    "\n",
    "# Example 4: Fetch data for 1 year\n",
    "async def fetch_data_example_1yr():\n",
    "    end_date = datetime.now() - timedelta(days=10)  # Exclude last 10 days\n",
    "    start_date = end_date - timedelta(days=365)\n",
    "\n",
    "    return await fetch_traffic_data(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        output_filename=f\"test_data_1yr_{VEH_CLASS}.pkl\",\n",
    "        window_size_days=30  # Fetch in 30-day chunks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1wk = await fetch_data_example_1wk()\n",
    "data_1mo = await fetch_data_example_1mo()\n",
    "data_3mo = await fetch_data_example_3mo()\n",
    "data_1yr = await fetch_data_example_1yr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1wk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
