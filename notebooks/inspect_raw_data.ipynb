{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gnn_package import paths\n",
    "from gnn_package import preprocessing\n",
    "\n",
    "raw_file_name = \"test_data_1wk.pkl\"\n",
    "raw_dir = paths.RAW_TIMESERIES_DIR\n",
    "raw_file_path = os.path.join(paths.RAW_TIMESERIES_DIR, raw_file_name)\n",
    "print(f\"Loading raw data from {raw_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(raw_file_path):\n",
    "    with open(raw_file_path, \"rb\") as f:\n",
    "        results_containing_data = pickle.load(f)\n",
    "\n",
    "# Clean the data\n",
    "results_containing_data_cleaned = preprocessing.resample_sensor_data(\n",
    "    results_containing_data,\n",
    "    freq=\"15min\",\n",
    "    fill_value=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_containing_data[\"10000\"].isnull().sum(), results_containing_data[\n",
    "    \"10003\"\n",
    "].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example series with missing values\n",
    "example_series = results_containing_data_cleaned[\"10029\"]\n",
    "\n",
    "example_series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = {}\n",
    "\n",
    "for name, series in results_containing_data_cleaned.items():\n",
    "    temp = series.copy()\n",
    "    temp = temp.reset_index()\n",
    "    temp.columns = [\"dt\", \"value\"]\n",
    "    value_counts[name] = temp[\"value\"].value_counts(dropna=False)\n",
    "\n",
    "value_counts_df = pd.DataFrame(value_counts)\n",
    "value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sensor_statistics(time_series_dict):\n",
    "    \"\"\"\n",
    "    Generate comprehensive statistics for each sensor's time series data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_series_dict : dict\n",
    "        Dictionary mapping sensor IDs to their time series data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing summary statistics for each sensor\n",
    "    \"\"\"\n",
    "    # Define the statistics we want to calculate\n",
    "    stats_data = []\n",
    "\n",
    "    # Calculate the full date range of the dataset\n",
    "    all_dates = []\n",
    "    for series in time_series_dict.values():\n",
    "        if len(series) > 0:\n",
    "            all_dates.extend(series.index.tolist())\n",
    "\n",
    "    overall_start = min(all_dates)\n",
    "    overall_end = max(all_dates)\n",
    "    total_period_days = (overall_end - overall_start).total_seconds() / (60 * 60 * 24)\n",
    "\n",
    "    # Expected readings for 15-minute intervals\n",
    "    expected_readings = int(total_period_days * 24 * 4) + 1\n",
    "\n",
    "    # Process each sensor\n",
    "    for sensor_id, series in time_series_dict.items():\n",
    "        # Skip empty series\n",
    "        if series is None or len(series) == 0:\n",
    "            continue\n",
    "\n",
    "        # Basic count statistics\n",
    "        total_readings = len(series)\n",
    "        unique_readings = series.nunique()\n",
    "        completeness = (total_readings / expected_readings) * 100\n",
    "\n",
    "        # Create a resampled series to analyze missing data\n",
    "        # First ensure the index is sorted and duplicates are removed\n",
    "        series = series.sort_index()\n",
    "        series = series[~series.index.duplicated(keep=\"first\")]\n",
    "\n",
    "        # Time-based statistics\n",
    "        start_date = series.index.min()\n",
    "        end_date = series.index.max()\n",
    "        time_span_days = (end_date - start_date).total_seconds() / (60 * 60 * 24)\n",
    "\n",
    "        # Value statistics\n",
    "        min_value = series.min()\n",
    "        max_value = series.max()\n",
    "        mean_value = series.mean()\n",
    "        median_value = series.median()\n",
    "        std_dev = series.std()\n",
    "\n",
    "        # Resampling to find gaps\n",
    "        resampled = series.resample(\"15min\").mean()\n",
    "        missing_intervals = resampled.isna().sum()\n",
    "        actual_intervals = (~resampled.isna()).sum()\n",
    "        resampled_completeness = (actual_intervals / len(resampled)) * 100\n",
    "\n",
    "        # Convert the boolean series to numpy array\n",
    "        missing_mask = resampled.isna().values\n",
    "\n",
    "        # Find transitions from not missing to missing (gap starts)\n",
    "        gap_starts = []\n",
    "        # Find transitions from missing to not missing (gap ends)\n",
    "        gap_ends = []\n",
    "\n",
    "        # Simpler approach to find gap transitions\n",
    "        for i in range(1, len(missing_mask)):\n",
    "            # Gap starts: previous value is not missing, current value is missing\n",
    "            if missing_mask[i] and not missing_mask[i - 1]:\n",
    "                gap_starts.append(i)\n",
    "            # Gap ends: previous value is missing, current value is not missing\n",
    "            elif not missing_mask[i] and missing_mask[i - 1]:\n",
    "                gap_ends.append(i)\n",
    "\n",
    "        # Handle case where gap is at beginning\n",
    "        if missing_mask[0]:\n",
    "            gap_starts.insert(0, 0)\n",
    "\n",
    "        # Handle case where gap is at end\n",
    "        if missing_mask[-1]:\n",
    "            gap_ends.append(len(missing_mask))\n",
    "\n",
    "        # Calculate the largest gap in hours\n",
    "        largest_gap_hours = 0\n",
    "        if (\n",
    "            len(gap_starts) > 0\n",
    "            and len(gap_ends) > 0\n",
    "            and len(gap_starts) == len(gap_ends)\n",
    "        ):\n",
    "            gap_lengths = [end - start for start, end in zip(gap_starts, gap_ends)]\n",
    "            if gap_lengths:\n",
    "                largest_gap_hours = max(gap_lengths) * 0.25  # 15-min intervals to hours\n",
    "\n",
    "        # Daily pattern analysis - hour of day with most readings\n",
    "        if len(series) > 0:\n",
    "            hour_counts = series.groupby(series.index.hour).size()\n",
    "            busiest_hour = hour_counts.idxmax() if not hour_counts.empty else None\n",
    "            busiest_hour_count = hour_counts.max() if not hour_counts.empty else 0\n",
    "\n",
    "            # Day of week pattern\n",
    "            dow_counts = series.groupby(series.index.dayofweek).size()\n",
    "            busiest_day = dow_counts.idxmax() if not dow_counts.empty else None\n",
    "            busiest_day_map = {\n",
    "                0: \"Monday\",\n",
    "                1: \"Tuesday\",\n",
    "                2: \"Wednesday\",\n",
    "                3: \"Thursday\",\n",
    "                4: \"Friday\",\n",
    "                5: \"Saturday\",\n",
    "                6: \"Sunday\",\n",
    "            }\n",
    "            busiest_day_name = busiest_day_map.get(busiest_day, \"Unknown\")\n",
    "        else:\n",
    "            busiest_hour = None\n",
    "            busiest_hour_count = 0\n",
    "            busiest_day = None\n",
    "            busiest_day_name = \"Unknown\"\n",
    "\n",
    "        # Append to our statistics collection\n",
    "        stats_data.append(\n",
    "            {\n",
    "                \"sensor_id\": sensor_id,\n",
    "                \"total_readings\": total_readings,\n",
    "                \"unique_values\": unique_readings,\n",
    "                \"raw_completeness_pct\": completeness,\n",
    "                \"resampled_completeness_pct\": resampled_completeness,\n",
    "                \"start_date\": start_date,\n",
    "                \"end_date\": end_date,\n",
    "                \"time_span_days\": time_span_days,\n",
    "                \"min_value\": min_value,\n",
    "                \"max_value\": max_value,\n",
    "                \"mean_value\": mean_value,\n",
    "                \"median_value\": median_value,\n",
    "                \"std_dev\": std_dev,\n",
    "                \"missing_15min_intervals\": missing_intervals,\n",
    "                \"largest_gap_hours\": largest_gap_hours,\n",
    "                \"readings_per_day\": (\n",
    "                    total_readings / time_span_days if time_span_days > 0 else 0\n",
    "                ),\n",
    "                \"busiest_hour\": busiest_hour,\n",
    "                \"busiest_hour_count\": busiest_hour_count,\n",
    "                \"busiest_day\": busiest_day_name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Create DataFrame\n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "\n",
    "    # Sort by completeness\n",
    "    stats_df = stats_df.sort_values(\"raw_completeness_pct\", ascending=False)\n",
    "\n",
    "    # Format percentages\n",
    "    stats_df[\"raw_completeness_pct\"] = stats_df[\"raw_completeness_pct\"].round(2)\n",
    "    stats_df[\"resampled_completeness_pct\"] = stats_df[\n",
    "        \"resampled_completeness_pct\"\n",
    "    ].round(2)\n",
    "\n",
    "    # Round numerical columns\n",
    "    numeric_cols = [\n",
    "        \"mean_value\",\n",
    "        \"median_value\",\n",
    "        \"std_dev\",\n",
    "        \"readings_per_day\",\n",
    "        \"largest_gap_hours\",\n",
    "        \"time_span_days\",\n",
    "    ]\n",
    "    stats_df[numeric_cols] = stats_df[numeric_cols].round(2)\n",
    "\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = generate_sensor_statistics(results_containing_data_cleaned)\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_time_series_data(time_series_dict):\n",
    "    \"\"\"\n",
    "    Create a comprehensive analysis of the time series data with multiple dataframes\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_series_dict : dict\n",
    "        Dictionary mapping sensor IDs to their time series data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing multiple DataFrames with different analyses\n",
    "    \"\"\"\n",
    "    # Get basic statistics\n",
    "    basic_stats = generate_sensor_statistics(time_series_dict)\n",
    "\n",
    "    # Create a summary of data completeness and quality\n",
    "    summary_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Total Sensors\": len(time_series_dict),\n",
    "            \"Sensors with Data\": len(basic_stats),\n",
    "            \"Average Completeness (%)\": basic_stats[\"raw_completeness_pct\"].mean(),\n",
    "            \"Median Completeness (%)\": basic_stats[\"raw_completeness_pct\"].median(),\n",
    "            \"Date Range Start\": basic_stats[\"start_date\"].min(),\n",
    "            \"Date Range End\": basic_stats[\"end_date\"].max(),\n",
    "            \"Total Days\": (\n",
    "                basic_stats[\"end_date\"].max() - basic_stats[\"start_date\"].min()\n",
    "            ).days,\n",
    "            \"Average Readings per Sensor\": basic_stats[\"total_readings\"].mean(),\n",
    "            \"Max Readings (Sensor)\": f\"{basic_stats['total_readings'].max()} ({basic_stats.iloc[basic_stats['total_readings'].idxmax()]['sensor_id']})\",\n",
    "            \"Min Readings (Sensor)\": f\"{basic_stats['total_readings'].min()} ({basic_stats.iloc[basic_stats['total_readings'].idxmin()]['sensor_id']})\",\n",
    "        },\n",
    "        index=[\"Summary\"],\n",
    "    )\n",
    "\n",
    "    # Calculate data distribution by time of day (hourly)\n",
    "    hourly_data = {}\n",
    "    for sensor_id, series in time_series_dict.items():\n",
    "        if series is not None and len(series) > 0:\n",
    "            hourly_data[sensor_id] = series.groupby(series.index.hour).size()\n",
    "\n",
    "    hourly_df = pd.DataFrame(hourly_data).fillna(0)\n",
    "    hourly_df[\"Average\"] = hourly_df.mean(axis=1)\n",
    "\n",
    "    # Calculate data distribution by day of week\n",
    "    dow_data = {}\n",
    "    for sensor_id, series in time_series_dict.items():\n",
    "        if series is not None and len(series) > 0:\n",
    "            dow_data[sensor_id] = series.groupby(series.index.dayofweek).size()\n",
    "\n",
    "    dow_df = pd.DataFrame(dow_data).fillna(0)\n",
    "    dow_df.index = [\n",
    "        \"Monday\",\n",
    "        \"Tuesday\",\n",
    "        \"Wednesday\",\n",
    "        \"Thursday\",\n",
    "        \"Friday\",\n",
    "        \"Saturday\",\n",
    "        \"Sunday\",\n",
    "    ]\n",
    "    dow_df[\"Average\"] = dow_df.mean(axis=1)\n",
    "\n",
    "    # Calculate data distribution by day of month\n",
    "    dom_data = {}\n",
    "    for sensor_id, series in time_series_dict.items():\n",
    "        if series is not None and len(series) > 0:\n",
    "            dom_data[sensor_id] = series.groupby(series.index.day).size()\n",
    "\n",
    "    dom_df = pd.DataFrame(dom_data).fillna(0)\n",
    "    dom_df[\"Average\"] = dom_df.mean(axis=1)\n",
    "\n",
    "    # Calculate missing data patterns (consecutive missing values)\n",
    "    missing_patterns = {}\n",
    "    for sensor_id, series in time_series_dict.items():\n",
    "        if series is not None and len(series) > 0:\n",
    "            # Resample to consistent 15-minute intervals\n",
    "            resampled = series.resample(\"15min\").mean()\n",
    "\n",
    "            # Find runs of missing data\n",
    "            is_missing = resampled.isna()\n",
    "\n",
    "            # Calculate runs of True values (missing data)\n",
    "            runs = []\n",
    "            run_length = 0\n",
    "\n",
    "            for val in is_missing:\n",
    "                if val:\n",
    "                    run_length += 1\n",
    "                elif run_length > 0:\n",
    "                    runs.append(run_length)\n",
    "                    run_length = 0\n",
    "\n",
    "            # Add the last run if needed\n",
    "            if run_length > 0:\n",
    "                runs.append(run_length)\n",
    "\n",
    "            # Calculate statistics on missing data runs\n",
    "            if runs:\n",
    "                missing_patterns[sensor_id] = {\n",
    "                    \"count\": len(runs),\n",
    "                    \"mean_length\": np.mean(runs) * 15,  # convert to minutes\n",
    "                    \"max_length\": np.max(runs) * 15,  # convert to minutes\n",
    "                    \"median_length\": np.median(runs) * 15,  # convert to minutes\n",
    "                }\n",
    "            else:\n",
    "                missing_patterns[sensor_id] = {\n",
    "                    \"count\": 0,\n",
    "                    \"mean_length\": 0,\n",
    "                    \"max_length\": 0,\n",
    "                    \"median_length\": 0,\n",
    "                }\n",
    "\n",
    "    missing_df = pd.DataFrame(missing_patterns).T\n",
    "    missing_df.columns = [\n",
    "        \"Number of Gaps\",\n",
    "        \"Mean Gap Length (min)\",\n",
    "        \"Max Gap Length (min)\",\n",
    "        \"Median Gap Length (min)\",\n",
    "    ]\n",
    "    missing_df = missing_df.sort_values(\"Number of Gaps\", ascending=False)\n",
    "\n",
    "    return {\n",
    "        \"basic_stats\": basic_stats,\n",
    "        \"summary\": summary_df,\n",
    "        \"hourly_distribution\": hourly_df,\n",
    "        \"day_of_week_distribution\": dow_df,\n",
    "        \"day_of_month_distribution\": dom_df,\n",
    "        \"missing_data_patterns\": missing_df,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results = analyze_time_series_data(results_containing_data_cleaned)\n",
    "display(analysis_results[\"summary\"])\n",
    "display(analysis_results[\"basic_stats\"])\n",
    "display(analysis_results[\"missing_data_patterns\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_time_series_summary(time_series_dict, basic_stats):\n",
    "    \"\"\"\n",
    "    Create visualizations for the time series data summary\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_series_dict : dict\n",
    "        Dictionary mapping sensor IDs to their time series data\n",
    "    basic_stats : DataFrame\n",
    "        DataFrame containing the basic statistics for each sensor\n",
    "    \"\"\"\n",
    "    # Set up the plotting style\n",
    "    plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "    # Create a figure with 2x2 subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # 1. Data completeness by sensor\n",
    "    completeness = basic_stats[[\"sensor_id\", \"raw_completeness_pct\"]].set_index(\n",
    "        \"sensor_id\"\n",
    "    )\n",
    "    completeness = completeness.sort_values(\"raw_completeness_pct\", ascending=True)\n",
    "    sns.barplot(\n",
    "        x=completeness[\"raw_completeness_pct\"],\n",
    "        y=completeness.index,\n",
    "        ax=axs[0, 0],\n",
    "        palette=\"viridis\",\n",
    "    )\n",
    "    axs[0, 0].set_title(\"Data Completeness by Sensor (%)\", fontsize=12)\n",
    "    axs[0, 0].set_xlabel(\"Completeness (%)\")\n",
    "    axs[0, 0].set_ylabel(\"Sensor ID\")\n",
    "\n",
    "    # 2. Average readings by hour of day\n",
    "    hourly_data = {}\n",
    "    for sensor_id, series in time_series_dict.items():\n",
    "        if series is not None and len(series) > 0:\n",
    "            hourly_data[sensor_id] = series.groupby(series.index.hour).size()\n",
    "\n",
    "    hourly_df = pd.DataFrame(hourly_data).fillna(0)\n",
    "    hourly_avg = hourly_df.mean(axis=1)\n",
    "\n",
    "    sns.lineplot(x=hourly_avg.index, y=hourly_avg.values, marker=\"o\", ax=axs[0, 1])\n",
    "    axs[0, 1].set_title(\"Average Readings by Hour of Day\", fontsize=12)\n",
    "    axs[0, 1].set_xlabel(\"Hour of Day\")\n",
    "    axs[0, 1].set_ylabel(\"Average Number of Readings\")\n",
    "    axs[0, 1].set_xticks(range(0, 24, 2))\n",
    "\n",
    "    # 3. Average readings by day of week\n",
    "    dow_data = {}\n",
    "    for sensor_id, series in time_series_dict.items():\n",
    "        if series is not None and len(series) > 0:\n",
    "            dow_data[sensor_id] = series.groupby(series.index.dayofweek).size()\n",
    "\n",
    "    dow_df = pd.DataFrame(dow_data).fillna(0)\n",
    "    dow_avg = dow_df.mean(axis=1)\n",
    "    days = [\n",
    "        \"Monday\",\n",
    "        \"Tuesday\",\n",
    "        \"Wednesday\",\n",
    "        \"Thursday\",\n",
    "        \"Friday\",\n",
    "        \"Saturday\",\n",
    "        \"Sunday\",\n",
    "    ]\n",
    "\n",
    "    sns.barplot(x=days, y=dow_avg.values, ax=axs[1, 0], palette=\"viridis\")\n",
    "    axs[1, 0].set_title(\"Average Readings by Day of Week\", fontsize=12)\n",
    "    axs[1, 0].set_xlabel(\"Day of Week\")\n",
    "    axs[1, 0].set_ylabel(\"Average Number of Readings\")\n",
    "    plt.setp(axs[1, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # 4. Distribution of readings per day\n",
    "    if \"readings_per_day\" in basic_stats.columns:\n",
    "        sns.histplot(basic_stats[\"readings_per_day\"], kde=True, ax=axs[1, 1])\n",
    "        axs[1, 1].set_title(\"Distribution of Readings per Day\", fontsize=12)\n",
    "        axs[1, 1].set_xlabel(\"Average Readings per Day\")\n",
    "        axs[1, 1].set_ylabel(\"Number of Sensors\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return nothing\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sensor_data(time_series_dict, visualize=True):\n",
    "    \"\"\"\n",
    "    Analyze sensor time series data and return a comprehensive report\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_series_dict : dict\n",
    "        Dictionary mapping sensor IDs to their time series data\n",
    "    visualize : bool\n",
    "        Whether to generate visualizations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing DataFrames with different analyses\n",
    "    \"\"\"\n",
    "    # Run the analysis\n",
    "    analysis_results = analyze_time_series_data(time_series_dict)\n",
    "\n",
    "    # Print a summary\n",
    "    print(\"==== Traffic Sensor Data Analysis ====\")\n",
    "    print(f\"Total Sensors: {len(time_series_dict)}\")\n",
    "    print(\n",
    "        f\"Period: {analysis_results['summary'].iloc[0]['Date Range Start']} to {analysis_results['summary'].iloc[0]['Date Range End']}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average Completeness: {analysis_results['summary'].iloc[0]['Average Completeness (%)']:.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average Readings per Sensor: {analysis_results['summary'].iloc[0]['Average Readings per Sensor']:.0f}\"\n",
    "    )\n",
    "    print(\"====================================\")\n",
    "\n",
    "    # Create visualizations if requested\n",
    "    if visualize:\n",
    "        visualize_time_series_summary(time_series_dict, analysis_results[\"basic_stats\"])\n",
    "\n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full analysis\n",
    "analysis = analyze_sensor_data(results_containing_data_cleaned)\n",
    "\n",
    "# Display the main statistics DataFrame\n",
    "# display(analysis['basic_stats'])\n",
    "\n",
    "# Display the summary information\n",
    "display(analysis[\"summary\"])\n",
    "\n",
    "# Display missing data patterns\n",
    "# display(analysis['missing_data_patterns'])\n",
    "\n",
    "# Display hourly distribution\n",
    "# display(analysis['hourly_distribution'])\n",
    "\n",
    "# Display day of week distribution\n",
    "# display(analysis['day_of_week_distribution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def visualize_missing_data_masks(\n",
    "    time_series_dict, window_size=12, stride=1, num_sensors=3, num_windows=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize how missing data is represented by masks and interpreted by the model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_series_dict : dict\n",
    "        Dictionary mapping sensor IDs to time series data\n",
    "    window_size : int\n",
    "        Size of sliding windows\n",
    "    stride : int\n",
    "        Step size for sliding windows\n",
    "    num_sensors : int\n",
    "        Number of sensors to visualize\n",
    "    num_windows : int\n",
    "        Number of windows to show per sensor\n",
    "    \"\"\"\n",
    "    # Choose a few sensors with some missing data\n",
    "    sensor_ids = list(time_series_dict.keys())\n",
    "    if len(sensor_ids) > num_sensors:\n",
    "        sensor_ids = sensor_ids[:num_sensors]\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 4 * len(sensor_ids)))\n",
    "    fig.suptitle(\"Visualization of Missing Data Masks\", fontsize=16)\n",
    "\n",
    "    # Set up a 2D grid: sensors x (raw data, windows)\n",
    "    gs = fig.add_gridspec(len(sensor_ids), 2, width_ratios=[2, 3])\n",
    "\n",
    "    for i, sensor_id in enumerate(sensor_ids):\n",
    "        series = time_series_dict[sensor_id]\n",
    "\n",
    "        # Remove duplicates and sort index\n",
    "        series = series.sort_index()\n",
    "        series = series[~series.index.duplicated(keep=\"first\")]\n",
    "\n",
    "        # Create a resampled series to ensure regular intervals\n",
    "        resampled = series.resample(\"15min\").mean()\n",
    "\n",
    "        # 1. Plot raw time series with missing values\n",
    "        ax1 = fig.add_subplot(gs[i, 0])\n",
    "\n",
    "        # Plot the raw data\n",
    "        valid_mask = ~pd.isna(resampled)\n",
    "\n",
    "        # X values for plotting\n",
    "        x_dates = resampled.index\n",
    "\n",
    "        # Plot with gaps for missing values\n",
    "        ax1.plot(x_dates[valid_mask], resampled[valid_mask], \"b-\", label=\"Valid Data\")\n",
    "\n",
    "        # Mark missing values with red X\n",
    "        missing_mask = ~valid_mask\n",
    "        if any(missing_mask):\n",
    "            ax1.scatter(\n",
    "                x_dates[missing_mask],\n",
    "                [0] * sum(missing_mask),  # Put at y=0 for visibility\n",
    "                marker=\"x\",\n",
    "                color=\"red\",\n",
    "                s=50,\n",
    "                label=\"Missing Data\",\n",
    "            )\n",
    "\n",
    "        # Highlight a few windows for detailed inspection\n",
    "        window_indices = []\n",
    "        consecutive_valid = []\n",
    "\n",
    "        # Find consecutive windows of valid data\n",
    "        current_run = 0\n",
    "        for j in range(len(valid_mask)):\n",
    "            if valid_mask.iloc[j]:\n",
    "                current_run += 1\n",
    "                if current_run >= window_size:\n",
    "                    window_indices.append(j - window_size + 1)\n",
    "            else:\n",
    "                current_run = 0\n",
    "\n",
    "            # Track consecutive valid points for visualization\n",
    "            if valid_mask.iloc[j]:\n",
    "                if len(consecutive_valid) == 0 or j - consecutive_valid[-1][-1] > 1:\n",
    "                    consecutive_valid.append([j])\n",
    "                else:\n",
    "                    consecutive_valid[-1].append(j)\n",
    "\n",
    "        # If we didn't find enough windows, add some with missing data\n",
    "        if len(window_indices) < num_windows:\n",
    "            for j in range(0, len(resampled) - window_size + 1, window_size):\n",
    "                if j not in window_indices and len(window_indices) < num_windows:\n",
    "                    window_indices.append(j)\n",
    "\n",
    "        # Limit to requested number\n",
    "        window_indices = window_indices[:num_windows]\n",
    "\n",
    "        # Highlight window regions on raw data plot\n",
    "        colors = [\"green\", \"orange\", \"purple\", \"brown\"]\n",
    "        for j, start_idx in enumerate(window_indices):\n",
    "            end_idx = start_idx + window_size\n",
    "            color = colors[j % len(colors)]\n",
    "\n",
    "            # Get date range for this window\n",
    "            if start_idx < len(x_dates) and end_idx <= len(x_dates):\n",
    "                start_date = x_dates[start_idx]\n",
    "                end_date = x_dates[min(end_idx, len(x_dates) - 1)]\n",
    "\n",
    "                # Draw rectangle for window\n",
    "                ax1.axvspan(\n",
    "                    start_date,\n",
    "                    end_date,\n",
    "                    alpha=0.2,\n",
    "                    color=color,\n",
    "                    label=f\"Window {j+1}\" if j == 0 else \"\",\n",
    "                )\n",
    "\n",
    "                # Label window\n",
    "                ax1.text(\n",
    "                    start_date + (end_date - start_date) / 2,\n",
    "                    ax1.get_ylim()[1] * 0.9,\n",
    "                    f\"W{j+1}\",\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=color,\n",
    "                )\n",
    "\n",
    "        ax1.set_title(f\"Sensor {sensor_id} Raw Time Series\")\n",
    "        ax1.set_xlabel(\"Date\")\n",
    "        ax1.set_ylabel(\"Value\")\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        ax1.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n",
    "        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "        # 2. Visualize the windowed data and masks\n",
    "        ax2 = fig.add_subplot(gs[i, 1])\n",
    "\n",
    "        # Create a matrix to show windows and their masks\n",
    "        window_matrix = np.zeros((len(window_indices), window_size))\n",
    "        mask_matrix = np.zeros((len(window_indices), window_size))\n",
    "\n",
    "        # Fill the matrices\n",
    "        for j, start_idx in enumerate(window_indices):\n",
    "            end_idx = start_idx + window_size\n",
    "\n",
    "            if end_idx <= len(resampled):\n",
    "                # Get window data\n",
    "                window_data = resampled.iloc[start_idx:end_idx].values\n",
    "\n",
    "                # Create mask (1 for valid, 0 for missing)\n",
    "                window_mask = (~pd.isna(window_data)).astype(int)\n",
    "\n",
    "                # Fill matrices\n",
    "                window_matrix[j, :] = np.where(\n",
    "                    window_mask == 1, window_data, -1\n",
    "                )  # -1 for missing in the model\n",
    "                mask_matrix[j, :] = window_mask\n",
    "\n",
    "        # Create a custom colormap for the mask visualization\n",
    "        mask_cmap = ListedColormap([\"red\", \"green\"])\n",
    "\n",
    "        # Plot the mask matrix\n",
    "        im = ax2.imshow(\n",
    "            mask_matrix,\n",
    "            aspect=\"auto\",\n",
    "            cmap=mask_cmap,\n",
    "            interpolation=\"none\",\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "        )\n",
    "\n",
    "        # Overlay values on the mask visualization\n",
    "        for j in range(mask_matrix.shape[0]):\n",
    "            for k in range(mask_matrix.shape[1]):\n",
    "                if mask_matrix[j, k] == 1:  # Valid data\n",
    "                    value = window_matrix[j, k]\n",
    "                    ax2.text(\n",
    "                        k,\n",
    "                        j,\n",
    "                        f\"{value:.1f}\",\n",
    "                        ha=\"center\",\n",
    "                        va=\"center\",\n",
    "                        color=\"black\",\n",
    "                        fontsize=8,\n",
    "                    )\n",
    "                else:  # Missing data\n",
    "                    ax2.text(\n",
    "                        k,\n",
    "                        j,\n",
    "                        \"X\",\n",
    "                        ha=\"center\",\n",
    "                        va=\"center\",\n",
    "                        color=\"white\",\n",
    "                        fontsize=10,\n",
    "                        fontweight=\"bold\",\n",
    "                    )\n",
    "\n",
    "        # Add colorbar and labels\n",
    "        cbar = plt.colorbar(im, ax=ax2, ticks=[0, 1])\n",
    "        cbar.set_ticklabels([\"Missing (0)\", \"Valid (1)\"])\n",
    "\n",
    "        ax2.set_title(f\"Sensor {sensor_id} Windowed Data with Masks\")\n",
    "        ax2.set_xlabel(\"Time Step in Window\")\n",
    "        ax2.set_ylabel(\"Window Index\")\n",
    "        ax2.set_yticks(range(len(window_indices)))\n",
    "        ax2.set_yticklabels([f\"Window {j+1}\" for j in range(len(window_indices))])\n",
    "        ax2.set_xticks(range(window_size))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()\n",
    "\n",
    "    # Create a diagram explaining how the model handles missing values\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # Turn off axis\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Create flow diagram explaining the process\n",
    "    # Main title\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        0.95,\n",
    "        \"How STGNN Model Handles Missing Data\",\n",
    "        fontsize=16,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # Step 1: Raw Time Series\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((0.05, 0.75), 0.2, 0.1, fill=True, color=\"lightblue\", alpha=0.5)\n",
    "    )\n",
    "    ax.text(0.15, 0.8, \"Raw Time Series\", ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
    "    ax.text(\n",
    "        0.15,\n",
    "        0.77,\n",
    "        \"Contains gaps and missing values\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "    # Arrow 1\n",
    "    ax.arrow(\n",
    "        0.25, 0.8, 0.1, 0, head_width=0.02, head_length=0.02, fc=\"black\", ec=\"black\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Preprocessing\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((0.35, 0.75), 0.2, 0.1, fill=True, color=\"lightgreen\", alpha=0.5)\n",
    "    )\n",
    "    ax.text(\n",
    "        0.45, 0.8, \"Data Preprocessing\", ha=\"center\", va=\"center\", fontweight=\"bold\"\n",
    "    )\n",
    "    ax.text(\n",
    "        0.45, 0.77, \"Find continuous segments\", ha=\"center\", va=\"center\", fontsize=9\n",
    "    )\n",
    "\n",
    "    # Arrow 2\n",
    "    ax.arrow(\n",
    "        0.55, 0.8, 0.1, 0, head_width=0.02, head_length=0.02, fc=\"black\", ec=\"black\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Window Creation\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((0.65, 0.75), 0.2, 0.1, fill=True, color=\"lightyellow\", alpha=0.5)\n",
    "    )\n",
    "    ax.text(0.75, 0.8, \"Window Creation\", ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
    "    ax.text(\n",
    "        0.75, 0.77, \"Create fixed-size windows\", ha=\"center\", va=\"center\", fontsize=9\n",
    "    )\n",
    "\n",
    "    # Arrow 3\n",
    "    ax.arrow(\n",
    "        0.75, 0.75, 0, -0.1, head_width=0.02, head_length=0.02, fc=\"black\", ec=\"black\"\n",
    "    )\n",
    "\n",
    "    # Branch out to two parallel boxes\n",
    "    # Box 1: Window Values\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((0.55, 0.55), 0.2, 0.1, fill=True, color=\"lightcoral\", alpha=0.5)\n",
    "    )\n",
    "    ax.text(0.65, 0.6, \"Window Values\", ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
    "    ax.text(\n",
    "        0.65, 0.57, \"Missing values set to -1\", ha=\"center\", va=\"center\", fontsize=9\n",
    "    )\n",
    "\n",
    "    # Box 2: Mask Creation\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((0.75, 0.55), 0.2, 0.1, fill=True, color=\"lightblue\", alpha=0.5)\n",
    "    )\n",
    "    ax.text(0.85, 0.6, \"Mask Creation\", ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
    "    ax.text(0.85, 0.57, \"1 = valid, 0 = missing\", ha=\"center\", va=\"center\", fontsize=9)\n",
    "\n",
    "    # Arrows to next step\n",
    "    ax.arrow(\n",
    "        0.65, 0.55, 0, -0.1, head_width=0.02, head_length=0.02, fc=\"black\", ec=\"black\"\n",
    "    )\n",
    "    ax.arrow(\n",
    "        0.85, 0.55, 0, -0.1, head_width=0.02, head_length=0.02, fc=\"black\", ec=\"black\"\n",
    "    )\n",
    "\n",
    "    # Step 4: Model Input\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((0.35, 0.35), 0.6, 0.1, fill=True, color=\"lightsalmon\", alpha=0.5)\n",
    "    )\n",
    "    ax.text(\n",
    "        0.65, 0.4, \"Model Input Tensors\", ha=\"center\", va=\"center\", fontweight=\"bold\"\n",
    "    )\n",
    "    ax.text(\n",
    "        0.65,\n",
    "        0.37,\n",
    "        \"x: [batch_size, num_nodes, seq_len, 1]    x_mask: [batch_size, num_nodes, seq_len, 1]\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "    # Arrow 4\n",
    "    ax.arrow(\n",
    "        0.65, 0.35, 0, -0.1, head_width=0.02, head_length=0.02, fc=\"black\", ec=\"black\"\n",
    "    )\n",
    "\n",
    "    # Step 5: STGNN Model\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle(\n",
    "            (0.35, 0.15), 0.6, 0.1, fill=True, color=\"lightsteelblue\", alpha=0.5\n",
    "        )\n",
    "    )\n",
    "    ax.text(\n",
    "        0.65, 0.2, \"STGNN Model Processing\", ha=\"center\", va=\"center\", fontweight=\"bold\"\n",
    "    )\n",
    "    ax.text(\n",
    "        0.65,\n",
    "        0.17,\n",
    "        \"Only applies operations on valid data (where mask = 1)\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "    # Mask handling explanation (key points)\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((0.1, 0.05), 0.8, 0.08, fill=True, color=\"#e6f7ff\", alpha=0.5)\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        0.09,\n",
    "        \"Key Concepts for Mask Handling in STGNN\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        0.06,\n",
    "        \"1. Graph convolutions compute features only for valid nodes (mask=1)\\n\"\n",
    "        + \"2. Missing values remain masked through each layer\\n\"\n",
    "        + \"3. Loss function weighted by mask: only valid predictions contribute to loss\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "visualize_missing_data_masks(results_containing_data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mask_propagation(time_series_dict, window_size=12):\n",
    "    \"\"\"\n",
    "    Visualize how masks propagate through the STGNN model layers.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_series_dict : dict\n",
    "        Dictionary mapping sensor IDs to time series data\n",
    "    window_size : int\n",
    "        Size of sliding windows\n",
    "    \"\"\"\n",
    "    # Choose a sensor with some missing data\n",
    "    sensor_id = None\n",
    "    for sid, series in time_series_dict.items():\n",
    "        if series is not None and len(series) > 0:\n",
    "            # Check for missing values\n",
    "            resampled = series.resample(\"15min\").mean()\n",
    "            if resampled.isna().sum() > 0:\n",
    "                sensor_id = sid\n",
    "                break\n",
    "\n",
    "    if sensor_id is None:\n",
    "        print(\"No sensor with missing values found. Using the first sensor.\")\n",
    "        # If no sensor has missing values, just use the first one\n",
    "        sensor_id = list(time_series_dict.keys())[0]\n",
    "\n",
    "    series = time_series_dict[sensor_id]\n",
    "\n",
    "    # Remove duplicates and sort index\n",
    "    series = series.sort_index()\n",
    "    series = series[~series.index.duplicated(keep=\"first\")]\n",
    "\n",
    "    # Create a resampled series to ensure regular intervals\n",
    "    resampled = series.resample(\"15min\").mean()\n",
    "\n",
    "    # Find a window with some missing values\n",
    "    window_start = None\n",
    "    for i in range(len(resampled) - window_size + 1):\n",
    "        window = resampled.iloc[i : i + window_size]\n",
    "        # Look for windows with some but not all missing values\n",
    "        missing_count = window.isna().sum()\n",
    "        if 0 < missing_count < window_size // 2:\n",
    "            window_start = i\n",
    "            break\n",
    "\n",
    "    if window_start is None:\n",
    "        # If no suitable window found, use the first window\n",
    "        window_start = 0\n",
    "\n",
    "    # Extract the window\n",
    "    window = resampled.iloc[window_start : window_start + window_size]\n",
    "\n",
    "    # Create mask (1 for valid, 0 for missing)\n",
    "    window_mask = (~window.isna()).astype(int).values\n",
    "\n",
    "    # Replace NaN with -1 (model's representation for missing values)\n",
    "    window_values = window.values.copy()\n",
    "    window_values[np.isnan(window_values)] = -1\n",
    "\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(15, 12))\n",
    "    fig.suptitle(\n",
    "        f\"Mask Propagation Through STGNN Layers - Sensor {sensor_id}\", fontsize=16\n",
    "    )\n",
    "\n",
    "    # 1. Original window with missing values\n",
    "    axs[0].plot(range(window_size), window_values, \"o-\", label=\"Original Values\")\n",
    "    axs[0].set_title(\"1. Input Window with Missing Values\")\n",
    "    axs[0].set_xlabel(\"Time Step\")\n",
    "    axs[0].set_ylabel(\"Value\")\n",
    "\n",
    "    # Mark missing values\n",
    "    for i in range(window_size):\n",
    "        if window_mask[i] == 0:\n",
    "            axs[0].scatter(\n",
    "                i, window_values[i], color=\"red\", marker=\"x\", s=100, label=\"_nolegend_\"\n",
    "            )\n",
    "            axs[0].annotate(\n",
    "                \"Missing\",\n",
    "                (i, window_values[i]),\n",
    "                xytext=(0, 10),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                fontsize=8,\n",
    "                color=\"red\",\n",
    "            )\n",
    "\n",
    "    axs[0].set_xticks(range(window_size))\n",
    "    axs[0].set_ylim([min(window_values) - 1, max(window_values) + 1])\n",
    "\n",
    "    # 2. Mask visualization\n",
    "    mask_cmap = ListedColormap([\"red\", \"green\"])\n",
    "    im = axs[1].imshow(\n",
    "        window_mask.reshape(1, -1),\n",
    "        aspect=\"auto\",\n",
    "        cmap=mask_cmap,\n",
    "        interpolation=\"none\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "\n",
    "    axs[1].set_title(\"2. Input Mask (0=Missing, 1=Valid)\")\n",
    "    axs[1].set_xlabel(\"Time Step\")\n",
    "    axs[1].set_yticks([])\n",
    "    axs[1].set_xticks(range(window_size))\n",
    "\n",
    "    # Overlay mask values\n",
    "    for i in range(window_size):\n",
    "        axs[1].text(\n",
    "            i,\n",
    "            0,\n",
    "            str(window_mask[i]),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"white\" if window_mask[i] == 0 else \"black\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=axs[1], orientation=\"horizontal\")\n",
    "    cbar.set_ticks([0.25, 0.75])\n",
    "    cbar.set_ticklabels([\"Missing (0)\", \"Valid (1)\"])\n",
    "\n",
    "    # 3. Simulate GCN layer processing\n",
    "    axs[2].set_title(\"3. Graph Convolution: Features computed only for valid nodes\")\n",
    "\n",
    "    # Simulate GCN operation by applying a simple operation to valid points\n",
    "    gcn_values = window_values.copy()\n",
    "    # Simulate transformation - in reality this would involve neighbors\n",
    "    for i in range(window_size):\n",
    "        if window_mask[i] == 1:\n",
    "            # Valid value - transform it (here just apply a simple function)\n",
    "            gcn_values[i] = 0.8 * window_values[i] + 0.5\n",
    "\n",
    "    # Plot transformed values\n",
    "    axs[2].plot(\n",
    "        range(window_size), gcn_values, \"o-\", color=\"purple\", label=\"GCN Output\"\n",
    "    )\n",
    "\n",
    "    # Mark valid vs. missing values\n",
    "    for i in range(window_size):\n",
    "        if window_mask[i] == 0:\n",
    "            axs[2].scatter(i, gcn_values[i], color=\"red\", marker=\"x\", s=100)\n",
    "            axs[2].annotate(\n",
    "                \"Still Missing\",\n",
    "                (i, gcn_values[i]),\n",
    "                xytext=(0, 10),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                fontsize=8,\n",
    "                color=\"red\",\n",
    "            )\n",
    "        else:\n",
    "            axs[2].scatter(i, gcn_values[i], color=\"green\", marker=\"o\", s=80)\n",
    "            axs[2].annotate(\n",
    "                \"Transformed\",\n",
    "                (i, gcn_values[i]),\n",
    "                xytext=(0, -15),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                fontsize=8,\n",
    "                color=\"green\",\n",
    "            )\n",
    "\n",
    "    axs[2].set_xlabel(\"Time Step\")\n",
    "    axs[2].set_ylabel(\"Value\")\n",
    "    axs[2].set_xticks(range(window_size))\n",
    "    axs[2].set_ylim([min(gcn_values) - 1, max(gcn_values) + 1])\n",
    "\n",
    "    # 4. Loss calculation\n",
    "    axs[3].set_title(\"4. Loss Calculation: Only valid predictions contribute to loss\")\n",
    "\n",
    "    # Simulate target values and predictions\n",
    "    target_values = window_values.copy() + 1  # Arbitrary offset\n",
    "    pred_values = gcn_values.copy() + np.random.normal(0, 0.5, window_size)  # Add noise\n",
    "\n",
    "    # Calculate error for each point\n",
    "    errors = np.abs(target_values - pred_values)\n",
    "\n",
    "    # Create a bar chart for errors\n",
    "    bar_colors = [\"green\" if m == 1 else \"lightgray\" for m in window_mask]\n",
    "    bars = axs[3].bar(range(window_size), errors, color=bar_colors)\n",
    "\n",
    "    # Add labels explaining masked vs. unmasked loss\n",
    "    for i in range(window_size):\n",
    "        if window_mask[i] == 1:\n",
    "            axs[3].text(\n",
    "                i,\n",
    "                errors[i] + 0.1,\n",
    "                \"Contributes\\nto Loss\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=8,\n",
    "                color=\"green\",\n",
    "            )\n",
    "        else:\n",
    "            axs[3].text(\n",
    "                i,\n",
    "                errors[i] + 0.1,\n",
    "                \"Ignored\\nin Loss\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=8,\n",
    "                color=\"red\",\n",
    "            )\n",
    "\n",
    "    axs[3].set_xlabel(\"Time Step\")\n",
    "    axs[3].set_ylabel(\"Error\")\n",
    "    axs[3].set_xticks(range(window_size))\n",
    "\n",
    "    # Add a legend to explain color scheme\n",
    "    from matplotlib.patches import Patch\n",
    "\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=\"green\", label=\"Valid Data Points\"),\n",
    "        Patch(facecolor=\"lightgray\", label=\"Missing Data Points\"),\n",
    "    ]\n",
    "    axs[3].legend(handles=legend_elements, loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "visualize_mask_propagation(results_containing_data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_dataloader_creation(time_series_dict, window_size=12, stride=1):\n",
    "    \"\"\"\n",
    "    Debug the process of creating windows to see if there's an issue with -1 values.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_series_dict : dict\n",
    "        Dictionary mapping sensor IDs to time series data\n",
    "    window_size : int\n",
    "        Size of sliding windows\n",
    "    stride : int\n",
    "        Step size for sliding windows\n",
    "    \"\"\"\n",
    "    from gnn_package.src.preprocessing import TimeSeriesPreprocessor\n",
    "\n",
    "    print(\n",
    "        f\"Debugging window creation process with window_size={window_size}, stride={stride}\"\n",
    "    )\n",
    "\n",
    "    # Create preprocessor with same parameters as in your training code\n",
    "    processor = TimeSeriesPreprocessor(\n",
    "        window_size=window_size,\n",
    "        stride=stride,\n",
    "        gap_threshold=pd.Timedelta(minutes=15),\n",
    "        missing_value=-1.0,\n",
    "    )\n",
    "\n",
    "    # Track statistics from original data\n",
    "    original_stats = {}\n",
    "\n",
    "    # First, analyze original data completeness\n",
    "    for sensor_id, series in time_series_dict.items():\n",
    "        if series is None or len(series) == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate completeness in original data\n",
    "        # First, clean up any duplicates\n",
    "        series = series[~series.index.duplicated(keep=\"first\")]\n",
    "\n",
    "        # Resample to ensure regular intervals\n",
    "        series = series.resample(\"15min\").mean()\n",
    "\n",
    "        # Calculate original stats\n",
    "        total_values = len(series)\n",
    "        missing_values = series.isna().sum()\n",
    "        pct_missing = (missing_values / total_values) * 100 if total_values > 0 else 0\n",
    "        pct_complete = 100 - pct_missing\n",
    "\n",
    "        original_stats[sensor_id] = {\n",
    "            \"total_values\": total_values,\n",
    "            \"missing_values\": missing_values,\n",
    "            \"pct_complete\": pct_complete,\n",
    "        }\n",
    "\n",
    "    print(f\"\\nOriginal data completeness stats for {len(original_stats)} sensors:\")\n",
    "    for sensor_id, stats in list(original_stats.items())[:5]:  # Show just first 5\n",
    "        print(\n",
    "            f\"  Sensor {sensor_id}: {stats['pct_complete']:.2f}% complete ({stats['missing_values']} missing out of {stats['total_values']} values)\"\n",
    "        )\n",
    "\n",
    "    # Now create windows using the same processor logic\n",
    "    print(\"\\nCreating windows using TimeSeriesPreprocessor...\")\n",
    "    X_by_sensor, masks_by_sensor, metadata_by_sensor = processor.create_windows(\n",
    "        time_series_dict, standardize=True\n",
    "    )\n",
    "\n",
    "    # Analyze window completeness\n",
    "    window_stats = {}\n",
    "\n",
    "    for sensor_id, windows in X_by_sensor.items():\n",
    "        if sensor_id not in window_stats:\n",
    "            window_stats[sensor_id] = {\n",
    "                \"total_windows\": 0,\n",
    "                \"windows_with_missing\": 0,\n",
    "                \"total_values\": 0,\n",
    "                \"missing_values\": 0,\n",
    "            }\n",
    "\n",
    "        # Count windows and values\n",
    "        window_stats[sensor_id][\"total_windows\"] = len(windows)\n",
    "        window_stats[sensor_id][\"total_values\"] = windows.size\n",
    "\n",
    "        # Count windows with missing values\n",
    "        missing_mask = windows == -1\n",
    "        windows_with_missing = np.any(missing_mask, axis=1).sum()\n",
    "        window_stats[sensor_id][\"windows_with_missing\"] = windows_with_missing\n",
    "\n",
    "        # Count missing values\n",
    "        missing_values = missing_mask.sum()\n",
    "        window_stats[sensor_id][\"missing_values\"] = missing_values\n",
    "\n",
    "        # Check if masks correctly identify missing values\n",
    "        if sensor_id in masks_by_sensor:\n",
    "            sensor_masks = masks_by_sensor[sensor_id]\n",
    "            mask_matches = ((sensor_masks == 0) == (windows == -1)).all()\n",
    "            window_stats[sensor_id][\"mask_correct\"] = mask_matches\n",
    "\n",
    "    # Print summary stats on windows\n",
    "    print(f\"\\nWindow creation results for {len(window_stats)} sensors:\")\n",
    "    for sensor_id, stats in list(window_stats.items())[:5]:  # Show just first 5\n",
    "        pct_windows_missing = (\n",
    "            (stats[\"windows_with_missing\"] / stats[\"total_windows\"] * 100)\n",
    "            if stats[\"total_windows\"] > 0\n",
    "            else 0\n",
    "        )\n",
    "        pct_values_missing = (\n",
    "            (stats[\"missing_values\"] / stats[\"total_values\"] * 100)\n",
    "            if stats[\"total_values\"] > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        print(f\"  Sensor {sensor_id}:\")\n",
    "        print(\n",
    "            f\"    Windows: {stats['total_windows']} total, {stats['windows_with_missing']} with missing values ({pct_windows_missing:.2f}%)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"    Values: {stats['missing_values']} missing out of {stats['total_values']} ({pct_values_missing:.2f}%)\"\n",
    "        )\n",
    "\n",
    "        # Compare with original completeness\n",
    "        if sensor_id in original_stats:\n",
    "            orig_pct_missing = 100 - original_stats[sensor_id][\"pct_complete\"]\n",
    "            print(\n",
    "                f\"    Original missing: {orig_pct_missing:.2f}%, Window missing: {pct_values_missing:.2f}%\"\n",
    "            )\n",
    "\n",
    "            if pct_values_missing > orig_pct_missing + 5:  # 5% tolerance\n",
    "                print(\n",
    "                    f\"    WARNING: Missing values increased significantly after windowing!\"\n",
    "                )\n",
    "\n",
    "    # Examine a specific window for one sensor to see the pattern\n",
    "    print(\"\\nExamining specific windows for a sample sensor:\")\n",
    "    example_sensor_id = list(X_by_sensor.keys())[0]\n",
    "    example_windows = X_by_sensor[example_sensor_id]\n",
    "    example_masks = masks_by_sensor[example_sensor_id]\n",
    "\n",
    "    print(f\"Sensor {example_sensor_id} window shape: {example_windows.shape}\")\n",
    "\n",
    "    # Look at first 3 windows\n",
    "    for i in range(min(3, len(example_windows))):\n",
    "        window = example_windows[i]\n",
    "        mask = example_masks[i]\n",
    "\n",
    "        missing_count = (window == -1).sum()\n",
    "        print(\n",
    "            f\"\\n  Window {i}: {missing_count} missing values out of {len(window)} ({missing_count/len(window)*100:.2f}%)\"\n",
    "        )\n",
    "\n",
    "        # Print window values and masks side by side\n",
    "        print(\"  Index  Value   Mask\")\n",
    "        print(\"  ------------------\")\n",
    "        for j in range(len(window)):\n",
    "            value_str = f\"{window[j]:.2f}\" if window[j] != -1 else \"-1.00\"\n",
    "            print(f\"  {j:5d}  {value_str}  {int(mask[j])}\")\n",
    "\n",
    "    # Now let's visualize a few windows and their masks\n",
    "    visualize_windows_and_masks(X_by_sensor, masks_by_sensor)\n",
    "\n",
    "    return {\n",
    "        \"original_stats\": original_stats,\n",
    "        \"window_stats\": window_stats,\n",
    "        \"X_by_sensor\": X_by_sensor,\n",
    "        \"masks_by_sensor\": masks_by_sensor,\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_windows_and_masks(\n",
    "    X_by_sensor, masks_by_sensor, num_sensors=2, num_windows=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize windows and their masks to help diagnose missing data issues.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_by_sensor : dict\n",
    "        Dictionary mapping sensor IDs to window arrays\n",
    "    masks_by_sensor : dict\n",
    "        Dictionary mapping sensor IDs to mask arrays\n",
    "    num_sensors : int\n",
    "        Number of sensors to visualize\n",
    "    num_windows : int\n",
    "        Number of windows per sensor to visualize\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Get a subset of sensors\n",
    "    sensor_ids = list(X_by_sensor.keys())[:num_sensors]\n",
    "\n",
    "    # Create a figure for each sensor\n",
    "    for sensor_id in sensor_ids:\n",
    "        windows = X_by_sensor[sensor_id]\n",
    "        masks = masks_by_sensor[sensor_id]\n",
    "\n",
    "        # Limit number of windows\n",
    "        n_windows = min(num_windows, len(windows))\n",
    "\n",
    "        fig, axs = plt.subplots(n_windows, 1, figsize=(12, 3 * n_windows))\n",
    "        if n_windows == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        fig.suptitle(f\"Sensor {sensor_id} Windows and Masks\", fontsize=14)\n",
    "\n",
    "        for i in range(n_windows):\n",
    "            window = windows[i]\n",
    "            mask = masks[i]\n",
    "\n",
    "            # Plot the window values\n",
    "            axs[i].plot(range(len(window)), window, \"b-\", label=\"Window Values\")\n",
    "\n",
    "            # Mark missing values (-1)\n",
    "            missing_mask = window == -1\n",
    "            if missing_mask.any():\n",
    "                axs[i].scatter(\n",
    "                    np.where(missing_mask)[0],\n",
    "                    window[missing_mask],\n",
    "                    color=\"red\",\n",
    "                    marker=\"x\",\n",
    "                    s=100,\n",
    "                    label=\"Missing (-1)\",\n",
    "                )\n",
    "\n",
    "            # Plot mask values on same axis (scaled to match window values)\n",
    "            min_val, max_val = window[~missing_mask].min(), window[~missing_mask].max()\n",
    "            if not np.isfinite(min_val):\n",
    "                min_val, max_val = -1, 1  # Default if all values are missing\n",
    "\n",
    "            range_val = max(1, max_val - min_val)\n",
    "\n",
    "            # Scale mask to plot range\n",
    "            scaled_mask = min_val + mask * range_val\n",
    "            axs[i].plot(\n",
    "                range(len(mask)), scaled_mask, \"g--\", alpha=0.7, label=\"Mask (scaled)\"\n",
    "            )\n",
    "\n",
    "            # Add mask values as text\n",
    "            for j, m in enumerate(mask):\n",
    "                axs[i].text(\n",
    "                    j,\n",
    "                    scaled_mask[j] + 0.05 * range_val,\n",
    "                    f\"{int(m)}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    color=\"green\",\n",
    "                    fontsize=8,\n",
    "                )\n",
    "\n",
    "            axs[i].set_title(f\"Window {i}\")\n",
    "            axs[i].set_xlabel(\"Time Step\")\n",
    "            axs[i].set_ylabel(\"Value\")\n",
    "            axs[i].legend(loc=\"upper right\")\n",
    "\n",
    "            # Annotate missing value percentage\n",
    "            missing_pct = missing_mask.sum() / len(window) * 100\n",
    "            axs[i].text(\n",
    "                0.02,\n",
    "                0.95,\n",
    "                f\"{missing_pct:.1f}% missing values\",\n",
    "                transform=axs[i].transAxes,\n",
    "                fontsize=10,\n",
    "                bbox=dict(facecolor=\"white\", alpha=0.8),\n",
    "            )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "debug_results = debug_dataloader_creation(\n",
    "    results_containing_data_cleaned, window_size=12, stride=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
